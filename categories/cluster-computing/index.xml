<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Cluster-Computing on Prometheus</title><link>https://marios2019.github.io/prometheus-docs/categories/cluster-computing/</link><description>Recent content in Cluster-Computing on Prometheus</description><generator>Hugo</generator><language>en</language><atom:link href="https://marios2019.github.io/prometheus-docs/categories/cluster-computing/index.xml" rel="self" type="application/rss+xml"/><item><title>Hardware Specifications</title><link>https://marios2019.github.io/prometheus-docs/docs/hardware/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://marios2019.github.io/prometheus-docs/docs/hardware/</guid><description>&lt;h2 id="cluster-overview">Cluster Overview&lt;a class="td-heading-self-link" href="#cluster-overview" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>The Prometheus cluster features a modern architecture optimized for deep learning workloads with high-performance GPUs, abundant memory, and fast storage systems.&lt;/p>
&lt;h2 id="head-node">Head Node&lt;a class="td-heading-self-link" href="#head-node" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>&lt;strong>Management and login node for the cluster&lt;/strong>&lt;/p>
&lt;h3 id="hardware-configuration">Hardware Configuration&lt;a class="td-heading-self-link" href="#hardware-configuration" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Chassis&lt;/strong>: GIGABYTE R182-Z90-00&lt;/li>
&lt;li>&lt;strong>Motherboard&lt;/strong>: GIGABYTE MZ92-FS0-00&lt;/li>
&lt;li>&lt;strong>CPU&lt;/strong>: 2× AMD EPYC 7313 (16 cores/32 threads each)&lt;/li>
&lt;li>&lt;strong>Total CPU Cores&lt;/strong>: 32 cores / 64 threads&lt;/li>
&lt;li>&lt;strong>RAM&lt;/strong>: 16× 32GB Samsung M393A4K40EB3-CWE&lt;/li>
&lt;li>&lt;strong>Total RAM&lt;/strong>: 512GB DDR4&lt;/li>
&lt;li>&lt;strong>Storage&lt;/strong>: 2× 1.92TB Intel SSDSC2KB019T8 SSD&lt;/li>
&lt;li>&lt;strong>File System&lt;/strong>: &lt;code>/trinity/home&lt;/code> (400GB allocated)&lt;/li>
&lt;/ul>
&lt;h3 id="purpose">Purpose&lt;a class="td-heading-self-link" href="#purpose" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;ul>
&lt;li>SSH login and job submission&lt;/li>
&lt;li>File management and transfers&lt;/li>
&lt;li>SLURM job scheduling&lt;/li>
&lt;li>&lt;strong>Not for compute workloads&lt;/strong>&lt;/li>
&lt;/ul>
&lt;h2 id="compute-nodes">Compute Nodes&lt;a class="td-heading-self-link" href="#compute-nodes" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="gpu-nodes-gpu01-08-8-nodes">GPU Nodes &lt;code>gpu[01-08]&lt;/code> (8 nodes)&lt;a class="td-heading-self-link" href="#gpu-nodes-gpu01-08-8-nodes" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;p>&lt;strong>Primary compute nodes with NVIDIA A5000 GPUs&lt;/strong>&lt;/p></description></item><item><title>Prometheus Cluster Documentation</title><link>https://marios2019.github.io/prometheus-docs/docs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://marios2019.github.io/prometheus-docs/docs/</guid><description>&lt;div class="pageinfo pageinfo-primary">
&lt;p>This section contains comprehensive documentation for the Prometheus cluster - a high-performance computing environment for deep learning research at CYENS.&lt;/p>

&lt;/div>

&lt;h2 id="overview">Overview&lt;a class="td-heading-self-link" href="#overview" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>The Prometheus cluster is a state-of-the-art deep learning computing facility featuring:&lt;/p>
&lt;ul>
&lt;li>&lt;strong>64 NVIDIA A5000 GPUs&lt;/strong> (24GB each) across 8 compute nodes&lt;/li>
&lt;li>&lt;strong>4 NVIDIA A6000 Ada GPUs&lt;/strong> (48GB each) on a dedicated node&lt;/li>
&lt;li>&lt;strong>4.6TB total GPU memory&lt;/strong> for large-scale model training&lt;/li>
&lt;li>&lt;strong>High-performance Lustre storage&lt;/strong> with 305TB capacity&lt;/li>
&lt;li>&lt;strong>SLURM job scheduler&lt;/strong> for efficient resource management&lt;/li>
&lt;/ul>
&lt;p>This documentation will guide you through:&lt;/p></description></item><item><title>Job Submission</title><link>https://marios2019.github.io/prometheus-docs/docs/job-submission/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://marios2019.github.io/prometheus-docs/docs/job-submission/</guid><description>&lt;h2 id="slurm-job-scheduler">SLURM Job Scheduler&lt;a class="td-heading-self-link" href="#slurm-job-scheduler" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>The Prometheus cluster uses &lt;strong>SLURM&lt;/strong> (Simple Linux Utility for Resource Management) for job scheduling and resource allocation. SLURM ensures fair resource sharing and efficient cluster utilization.&lt;/p>


&lt;div class="alert alert-warning" role="alert">
&lt;h4 class="alert-heading">Important&lt;/h4>

 &lt;strong>NEVER run compute jobs directly on the head node!&lt;/strong> Always use SLURM to submit jobs to compute nodes.

&lt;/div>

&lt;h2 id="interactive-jobs">Interactive Jobs&lt;a class="td-heading-self-link" href="#interactive-jobs" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>Interactive jobs are perfect for development, testing, and debugging. Use &lt;code>srun&lt;/code> to request resources immediately.&lt;/p></description></item><item><title>Partitions &amp; Queues</title><link>https://marios2019.github.io/prometheus-docs/docs/partitions/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://marios2019.github.io/prometheus-docs/docs/partitions/</guid><description>&lt;h2 id="overview">Overview&lt;a class="td-heading-self-link" href="#overview" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>The Prometheus cluster has &lt;strong>two partitions&lt;/strong> with different priority queues (QoS) that control resource limits and scheduling priority. All limits are applied &lt;strong>per group&lt;/strong>, and the default time limit is &lt;strong>4 hours&lt;/strong> for all partitions.&lt;/p>
&lt;h2 id="partition-architecture">Partition Architecture&lt;a class="td-heading-self-link" href="#partition-architecture" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="defq-partition-default">&lt;code>defq&lt;/code> Partition (Default)&lt;a class="td-heading-self-link" href="#defq-partition-default" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Nodes&lt;/strong>: 8 compute nodes (&lt;code>gpu[01-08]&lt;/code>)&lt;/li>
&lt;li>&lt;strong>GPU Type&lt;/strong>: NVIDIA A5000 (24GB VRAM each)&lt;/li>
&lt;li>&lt;strong>Total GPUs&lt;/strong>: 64 (8 GPUs per node)&lt;/li>
&lt;li>&lt;strong>Default partition&lt;/strong>: Jobs submitted without specifying partition go here&lt;/li>
&lt;/ul>
&lt;h3 id="a6000-partition">&lt;code>a6000&lt;/code> Partition&lt;a class="td-heading-self-link" href="#a6000-partition" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Nodes&lt;/strong>: 1 compute node (&lt;code>gpu09&lt;/code>)&lt;/li>
&lt;li>&lt;strong>GPU Type&lt;/strong>: NVIDIA RTX A6000 Ada Generation (48GB VRAM each)&lt;/li>
&lt;li>&lt;strong>Total GPUs&lt;/strong>: 4&lt;/li>
&lt;li>&lt;strong>Use case&lt;/strong>: High-memory GPU workloads&lt;/li>
&lt;/ul>
&lt;h2 id="priority-queues-qos">Priority Queues (QoS)&lt;a class="td-heading-self-link" href="#priority-queues-qos" aria-label="Heading self-link">&lt;/a>&lt;/h2>


&lt;div class="alert alert-info" role="alert">
&lt;h4 class="alert-heading">Resource Limits&lt;/h4>

 All resource limits are applied &lt;strong>per group&lt;/strong>, not per user. Coordinate with your group members to avoid conflicts.

&lt;/div>

&lt;h3 id="defq-partition-queues">&lt;code>defq&lt;/code> Partition Queues&lt;a class="td-heading-self-link" href="#defq-partition-queues" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;table class="table table-striped">
&lt;thead>
&lt;tr>
&lt;th>Priority Queue&lt;/th>
&lt;th>Time Limit&lt;/th>
&lt;th>Max CPUs&lt;/th>
&lt;th>Max GPUs&lt;/th>
&lt;th>Max RAM&lt;/th>
&lt;th>Max Jobs&lt;/th>
&lt;th>Priority&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>normal&lt;/code>&lt;/td>
&lt;td>1 day&lt;/td>
&lt;td>384&lt;/td>
&lt;td>48&lt;/td>
&lt;td>3TB&lt;/td>
&lt;td>30&lt;/td>
&lt;td>High&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>long&lt;/code>&lt;/td>
&lt;td>7 days&lt;/td>
&lt;td>384&lt;/td>
&lt;td>48&lt;/td>
&lt;td>3TB&lt;/td>
&lt;td>20&lt;/td>
&lt;td>Medium&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>preemptive&lt;/code>&lt;/td>
&lt;td>Infinite&lt;/td>
&lt;td>All*&lt;/td>
&lt;td>All*&lt;/td>
&lt;td>All*&lt;/td>
&lt;td>10&lt;/td>
&lt;td>Low&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;h3 id="a6000-partition-queues">&lt;code>a6000&lt;/code> Partition Queues&lt;a class="td-heading-self-link" href="#a6000-partition-queues" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;table class="table table-striped">
&lt;thead>
&lt;tr>
&lt;th>Priority Queue&lt;/th>
&lt;th>Time Limit&lt;/th>
&lt;th>Max CPUs&lt;/th>
&lt;th>Max GPUs&lt;/th>
&lt;th>Max RAM&lt;/th>
&lt;th>Max Jobs&lt;/th>
&lt;th>Priority&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td>&lt;code>normal-a6000&lt;/code>&lt;/td>
&lt;td>1 day&lt;/td>
&lt;td>48&lt;/td>
&lt;td>3&lt;/td>
&lt;td>384GB&lt;/td>
&lt;td>6&lt;/td>
&lt;td>High&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>long-a6000&lt;/code>&lt;/td>
&lt;td>7 days&lt;/td>
&lt;td>48&lt;/td>
&lt;td>3&lt;/td>
&lt;td>384GB&lt;/td>
&lt;td>4&lt;/td>
&lt;td>Medium&lt;/td>
&lt;/tr>
&lt;tr>
&lt;td>&lt;code>preemptive-a6000&lt;/code>&lt;/td>
&lt;td>Infinite&lt;/td>
&lt;td>All*&lt;/td>
&lt;td>All*&lt;/td>
&lt;td>All*&lt;/td>
&lt;td>2&lt;/td>
&lt;td>Low&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>* &lt;strong>Preemptive queues&lt;/strong> can use all available resources but jobs may be automatically terminated when higher-priority jobs need resources.&lt;/p></description></item><item><title>Storage Systems</title><link>https://marios2019.github.io/prometheus-docs/docs/storage/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://marios2019.github.io/prometheus-docs/docs/storage/</guid><description>&lt;h2 id="storage-overview">Storage Overview&lt;a class="td-heading-self-link" href="#storage-overview" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;p>The Prometheus cluster provides multiple storage systems optimized for different use cases, from personal files to high-performance parallel computing workloads.&lt;/p>
&lt;h2 id="storage-architecture">Storage Architecture&lt;a class="td-heading-self-link" href="#storage-architecture" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="home-directories-trinityhome">Home Directories (&lt;code>/trinity/home/&lt;/code>)&lt;a class="td-heading-self-link" href="#home-directories-trinityhome" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Type&lt;/strong>: SSD-backed storage&lt;/li>
&lt;li>&lt;strong>Mount point&lt;/strong>: &lt;code>/trinity/home/&amp;lt;username&amp;gt;&lt;/code>&lt;/li>
&lt;li>&lt;strong>Quota&lt;/strong>: 20GB per user&lt;/li>
&lt;li>&lt;strong>Purpose&lt;/strong>: Personal configuration files, small scripts&lt;/li>
&lt;li>&lt;strong>Backup&lt;/strong>: Regular backups maintained&lt;/li>
&lt;li>&lt;strong>Performance&lt;/strong>: Fast random I/O, moderate capacity&lt;/li>
&lt;/ul>
&lt;h3 id="shared-group-storage-lustrefsdata">Shared Group Storage (&lt;code>/lustreFS/data/&lt;/code>)&lt;a class="td-heading-self-link" href="#shared-group-storage-lustrefsdata" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Type&lt;/strong>: Lustre parallel file system&lt;/li>
&lt;li>&lt;strong>Mount point&lt;/strong>: &lt;code>/lustreFS/data/&amp;lt;group-name&amp;gt;&lt;/code>&lt;/li>
&lt;li>&lt;strong>Quota&lt;/strong>: 30TB per group (or 20,971,520 files)&lt;/li>
&lt;li>&lt;strong>Purpose&lt;/strong>: Primary workspace for research data and results&lt;/li>
&lt;li>&lt;strong>Performance&lt;/strong>: High-throughput parallel I/O&lt;/li>
&lt;li>&lt;strong>Shared&lt;/strong>: All group members have access&lt;/li>
&lt;/ul>
&lt;h3 id="local-node-storage">Local Node Storage&lt;a class="td-heading-self-link" href="#local-node-storage" aria-label="Heading self-link">&lt;/a>&lt;/h3>
&lt;ul>
&lt;li>&lt;strong>Type&lt;/strong>: NVMe SSD (1TB per compute node)&lt;/li>
&lt;li>&lt;strong>Purpose&lt;/strong>: Temporary files during job execution&lt;/li>
&lt;li>&lt;strong>Access&lt;/strong>: Only available during allocated jobs&lt;/li>
&lt;li>&lt;strong>Performance&lt;/strong>: Highest IOPS for temporary data&lt;/li>
&lt;/ul>
&lt;h2 id="file-system-details">File System Details&lt;a class="td-heading-self-link" href="#file-system-details" aria-label="Heading self-link">&lt;/a>&lt;/h2>
&lt;h3 id="home-directory-usage">Home Directory Usage&lt;a class="td-heading-self-link" href="#home-directory-usage" aria-label="Heading self-link">&lt;/a>&lt;/h3>


&lt;div class="alert alert-warning" role="alert">
&lt;h4 class="alert-heading">Quota Limit&lt;/h4>

 Home directories have a strict &lt;strong>20GB limit&lt;/strong>. Use them only for configuration files, not data or models.

&lt;/div>

&lt;div class="highlight">&lt;pre tabindex="0" style="background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4;">&lt;code class="language-bash" data-lang="bash">&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"># Check your home directory quota&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>quota -us
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"># View home directory contents&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>ls -la /trinity/home/&lt;span style="color:#000">$USER&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>&lt;span style="color:#8f5902;font-style:italic"># Typical home directory structure&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>/trinity/home/username/
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── .bashrc &lt;span style="color:#8f5902;font-style:italic"># Shell configuration&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── .ssh/ &lt;span style="color:#8f5902;font-style:italic"># SSH keys and config&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── .jupyter/ &lt;span style="color:#8f5902;font-style:italic"># Jupyter configuration&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── .conda/ &lt;span style="color:#8f5902;font-style:italic"># Conda configuration&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>├── scripts/ &lt;span style="color:#8f5902;font-style:italic"># Small utility scripts&lt;/span>
&lt;/span>&lt;/span>&lt;span style="display:flex;">&lt;span>└── .local/ &lt;span style="color:#8f5902;font-style:italic"># Local Python packages&lt;/span>
&lt;/span>&lt;/span>&lt;/code>&lt;/pre>&lt;/div>&lt;p>&lt;strong>Best practices for home directories&lt;/strong>:&lt;/p></description></item><item><title>About Prometheus Cluster</title><link>https://marios2019.github.io/prometheus-docs/about/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://marios2019.github.io/prometheus-docs/about/</guid><description>&lt;link rel="preload" as="image" href="https://marios2019.github.io/prometheus-docs/about/background_hu_3acffa24aee29d0.jpg" media="(max-width: 1200px)">
&lt;link rel="preload" as="image" href="https://marios2019.github.io/prometheus-docs/about/background_hu_cbea8e4fbc198c39.jpg" media="(min-width: 1200px)">
&lt;style>
#td-cover-block-0 {
 background-image: url(/prometheus-docs/about/background_hu_3acffa24aee29d0.jpg);
}
@media only screen and (min-width: 1200px) {
 #td-cover-block-0 {
 background-image: url(/prometheus-docs/about/background_hu_cbea8e4fbc198c39.jpg);
 }
}
&lt;/style>
&lt;section id="td-cover-block-0" class="row td-cover-block td-cover-block--height-auto js-td-cover td-overlay td-overlay--dark -bg-dark">
 &lt;div class="col-12">
 &lt;div class="container td-overlay__inner">
 &lt;div class="text-center">
 
 
 &lt;div class="pt-3 lead">
 
 &lt;div class="text-center mb-4">
&lt;pre class="ascii-art" style="color: #ffffff; font-weight: bold; text-shadow: 2px 2px 8px rgba(0,0,0,0.9); background: linear-gradient(135deg, rgba(0, 123, 255, 0.6), rgba(0, 86, 179, 0.7)); padding: 20px 30px; border-radius: 10px; border: 2px solid rgba(255,255,255,0.4); display: inline-block; margin: 0; font-size: 0.8em; line-height: 1.2; font-family: 'Courier New', monospace;">
 ________ ________ ________ _____ ______ _______ _________ ___ ___ _______ ___ ___ ________ 
 |\ __ \|\ __ \|\ __ \|\ _ \ _ \|\ ___ \|\___ ___\\ \|\ \|\ ___ \ |\ \|\ \|\ ____\ 
 \ \ \|\ \ \ \|\ \ \ \|\ \ \ \\\__\ \ \ \ __/\|___ \ \_\ \ \\\ \ \ __/|\ \ \\\ \ \ \___|_ 
 \ \ ____\ \ _ _\ \ \\\ \ \ \\|__| \ \ \ \_|/__ \ \ \ \ \ __ \ \ \_|/_\ \ \\\ \ \_____ \ 
 \ \ \___|\ \ \\ \\ \ \\\ \ \ \ \ \ \ \ \_|\ \ \ \ \ \ \ \ \ \ \ \_|\ \ \ \\\ \|____|\ \ 
 \ \__\ \ \__\\ _\\ \_______\ \__\ \ \__\ \_______\ \ \__\ \ \__\ \__\ \_______\ \_______\____\_\ \ 
 \|__| \|__|\|__|\|_______|\|__| \|__|\|_______| \|__| \|__|\|__|\|_______|\|_______|\_________\ 
 \|_________| 
&lt;/pre>
&lt;div style="margin-top: 20px;">
&lt;p class="lead ascii-description" style="color: #ffffff; font-weight: bold; text-shadow: 2px 2px 8px rgba(0,0,0,0.9); background: linear-gradient(135deg, rgba(0, 123, 255, 0.8), rgba(0, 86, 179, 0.9)); padding: 20px 30px; border-radius: 10px; border: 2px solid rgba(255,255,255,0.4); display: inline-block; font-size: 1.1em;">
High-Performance GPU Computing Cluster at CYENS Centre of Excellence
&lt;/p></description></item><item><title>Prometheus Deep Learning Cluster</title><link>https://marios2019.github.io/prometheus-docs/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://marios2019.github.io/prometheus-docs/</guid><description>&lt;link rel="preload" as="image" href="https://marios2019.github.io/prometheus-docs/background_hu_44063d47ab0ba17a.jpg" media="(max-width: 1200px)">
&lt;link rel="preload" as="image" href="https://marios2019.github.io/prometheus-docs/background_hu_f7899ce16370f62b.jpg" media="(min-width: 1200px)">
&lt;style>
#td-cover-block-0 {
 background-image: url(/prometheus-docs/background_hu_44063d47ab0ba17a.jpg);
}
@media only screen and (min-width: 1200px) {
 #td-cover-block-0 {
 background-image: url(/prometheus-docs/background_hu_f7899ce16370f62b.jpg);
 }
}
&lt;/style>
&lt;section id="td-cover-block-0" class="row td-cover-block td-cover-block--height-full js-td-cover td-overlay td-overlay--dark -bg-dark">
 &lt;div class="col-12">
 &lt;div class="container td-overlay__inner">
 &lt;div class="text-center">
 
 
 &lt;div class="pt-3 lead">
 
 &lt;div class="text-center mb-4">
&lt;pre class="ascii-art" style="color: #ffffff; font-weight: bold; text-shadow: 2px 2px 8px rgba(0,0,0,0.9); background: linear-gradient(135deg, rgba(0, 123, 255, 0.6), rgba(0, 86, 179, 0.7)); padding: 20px 30px; border-radius: 10px; border: 2px solid rgba(255,255,255,0.4); display: inline-block; margin: 0; font-size: 0.8em; line-height: 1.2; font-family: 'Courier New', monospace;">
 ________ ________ ________ _____ ______ _______ _________ ___ ___ _______ ___ ___ ________ 
 |\ __ \|\ __ \|\ __ \|\ _ \ _ \|\ ___ \|\___ ___\\ \|\ \|\ ___ \ |\ \|\ \|\ ____\ 
 \ \ \|\ \ \ \|\ \ \ \|\ \ \ \\\__\ \ \ \ __/\|___ \ \_\ \ \\\ \ \ __/|\ \ \\\ \ \ \___|_ 
 \ \ ____\ \ _ _\ \ \\\ \ \ \\|__| \ \ \ \_|/__ \ \ \ \ \ __ \ \ \_|/_\ \ \\\ \ \_____ \ 
 \ \ \___|\ \ \\ \\ \ \\\ \ \ \ \ \ \ \ \_|\ \ \ \ \ \ \ \ \ \ \ \_|\ \ \ \\\ \|____|\ \ 
 \ \__\ \ \__\\ _\\ \_______\ \__\ \ \__\ \_______\ \ \__\ \ \__\ \__\ \_______\ \_______\____\_\ \ 
 \|__| \|__|\|__|\|_______|\|__| \|__|\|_______| \|__| \|__|\|__|\|_______|\|_______|\_________\ 
 \|_________| 
&lt;/pre>
&lt;div style="margin-top: 20px;">
&lt;p class="lead ascii-description" style="color: #ffffff; font-weight: bold; text-shadow: 2px 2px 8px rgba(0,0,0,0.9); background: linear-gradient(135deg, rgba(0, 123, 255, 0.8), rgba(0, 86, 179, 0.9)); padding: 20px 30px; border-radius: 10px; border: 2px solid rgba(255,255,255,0.4); display: inline-block; font-size: 1.1em;">
High-Performance GPU Computing Cluster at CYENS Centre of Excellence
&lt;/p></description></item></channel></rss>