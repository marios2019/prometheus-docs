<!doctype html><html itemscope itemtype=http://schema.org/WebPage lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=canonical type=text/html href=https://marios2019.github.io/prometheus-docs/docs/><link rel=alternate type=application/rss+xml href=https://marios2019.github.io/prometheus-docs/docs/index.xml><meta name=robots content="noindex, nofollow"><link rel="shortcut icon" href=/prometheus-docs/favicons/favicon.ico><link rel=apple-touch-icon href=/prometheus-docs/favicons/apple-touch-icon-180x180.png sizes=180x180><link rel=icon type=image/png href=/prometheus-docs/favicons/favicon-16x16.png sizes=16x16><link rel=icon type=image/png href=/prometheus-docs/favicons/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/prometheus-docs/favicons/android-36x36.png sizes=36x36><link rel=icon type=image/png href=/prometheus-docs/favicons/android-48x48.png sizes=48x48><link rel=icon type=image/png href=/prometheus-docs/favicons/android-72x72.png sizes=72x72><link rel=icon type=image/png href=/prometheus-docs/favicons/android-96x96.png sizes=96x96><link rel=icon type=image/png href=/prometheus-docs/favicons/android-144x144.png sizes=144x144><link rel=icon type=image/png href=/prometheus-docs/favicons/android-192x192.png sizes=192x192><title>Prometheus Cluster Documentation | Prometheus</title><meta name=description content="Complete guide to using the Prometheus deep learning cluster at CYENS"><meta property="og:url" content="https://marios2019.github.io/prometheus-docs/docs/"><meta property="og:site_name" content="Prometheus"><meta property="og:title" content="Prometheus Cluster Documentation"><meta property="og:description" content="Complete guide to using the Prometheus deep learning cluster at CYENS"><meta property="og:locale" content="en"><meta property="og:type" content="website"><meta property="og:image" content="https://marios2019.github.io/images/background.jpg"><meta itemprop=name content="Prometheus Cluster Documentation"><meta itemprop=description content="Complete guide to using the Prometheus deep learning cluster at CYENS"><meta itemprop=dateModified content="2025-06-10T14:25:25+03:00"><meta itemprop=wordCount content="314"><meta itemprop=image content="https://marios2019.github.io/images/background.jpg"><meta itemprop=keywords content="Cluster,Documentation,Deep-Learning,Hpc,Prometheus,Gpu-Computing"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://marios2019.github.io/images/background.jpg"><meta name=twitter:title content="Prometheus Cluster Documentation"><meta name=twitter:description content="Complete guide to using the Prometheus deep learning cluster at CYENS"><meta name=twitter:site content="@CYENS_CoE"><link rel=preload href=/prometheus-docs/scss/main.min.0b8fe9a1711dd63c53761cfd5d8fe083f710c2afc757c167b277e596c1b12c8f.css as=style integrity="sha256-C4/poXEd1jxTdhz9XY/gg/cQwq/HV8FnsnfllsGxLI8=" crossorigin=anonymous><link href=/prometheus-docs/scss/main.min.0b8fe9a1711dd63c53761cfd5d8fe083f710c2afc757c167b277e596c1b12c8f.css rel=stylesheet integrity="sha256-C4/poXEd1jxTdhz9XY/gg/cQwq/HV8FnsnfllsGxLI8=" crossorigin=anonymous><script src=https://code.jquery.com/jquery-3.7.1.min.js integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin=anonymous></script></head><body class=td-section><header><nav class="td-navbar js-navbar-scroll" data-bs-theme=dark><div class="container-fluid flex-column flex-md-row"><a class=navbar-brand href=/prometheus-docs/><span class="navbar-brand__logo navbar-logo"><svg id="Layer_1" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 500 500" style="enable-background:new 0 0 500 500"><g><path style="fill:#fff" d="M116.8525 421.9722c-5.7041.0-10.3442-4.3127-10.3442-9.6129V88.183c0-5.3002 4.6401-9.6117 10.3442-9.6117H320.858c3.0347.0 9.3959.5498 11.7506 2.6302l.3545.3442 58.905 63.2912c2.3101 2.491 2.9202 8.4928 2.9202 11.3184v256.2039c0 5.3002-4.6407 9.6129-10.3436 9.6129H116.8525z"/><g><g><g><path style="fill:#767676" d="M384.4445 423.2066H116.852c-6.3839.0-11.5786-4.8658-11.5786-10.8474V88.1831c0-5.9804 5.1947-10.8461 11.5786-10.8461h204.0062c.377.0 9.2786.0329 12.568 2.9389l.3947.3833 58.9508 63.337c3.2135 3.4652 3.2514 11.7924 3.2514 12.1593v256.2036C396.0231 418.3408 390.8284 423.2066 384.4445 423.2066zM116.5079 411.9189c.0848.0278.1999.0531.3441.0531h267.5925c.1442.0.2581-.0253.3441-.0531V156.1556c-.0076-.9033-.3593-3.7347-.7034-5.0037l-57.6527-61.9416c-1.4651-.3176-4.4533-.6389-5.5742-.6389H116.852c-.143.0-.2594.024-.3441.0531V411.9189zm267.4533-261.149zM327.0321 89.371v.0013V89.371z"/></g></g></g><g><g><path style="fill:#5b7fc0" d="M189.0874 210.1754l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4473C177.5953 212.627 183.0601 210.1742 189.0874 210.1754zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 234.1722 197.0804 232.033z"/><path style="opacity:.3;fill:#fff" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/><g><defs><path id="SVGID_1_" d="M194.7376 237.6875c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 234.2399 196.1861 236.239 194.7376 237.6875z"/></defs><clipPath id="SVGID_2_"><use xlink:href="#SVGID_1_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_2_);fill:#fff" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/><path style="opacity:.13;clip-path:url(#SVGID_2_);fill:#020202" d="M190.0704 225.0237c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 225.7247 191.9774 225.0237 190.0704 225.0237z"/></g><g><defs><path id="SVGID_3_" d="M189.0898 210.176c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 212.6276 183.0612 210.176 189.0898 210.176zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 236.239 197.0839 234.2399 197.0839 232.0372z"/></defs><clipPath id="SVGID_4_"><use xlink:href="#SVGID_3_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_4_);fill:#5b7fc0" d="M172.6595 215.6045c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8475-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 209.1953 176.6171 211.647 172.6595 215.6045z"/></g></g><rect x="198.8952" y="225.1043" style="fill:#5b7fc0" width="122.6266" height="13.8671"/></g><g><path style="fill:#d95140" d="M189.0874 155.7611l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.0249 2.454-11.4897 6.4116-15.4473C177.5953 158.2128 183.0601 155.7599 189.0874 155.7611zm7.993 21.8577c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.2508 181.7667 197.0816 179.758 197.0804 177.6188z"/><path style="opacity:.3;fill:#fff" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/><g><defs><path id="SVGID_5_" d="M194.7376 183.2733c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 179.8256 196.1861 181.8248 194.7376 183.2733z"/></defs><clipPath id="SVGID_6_"><use xlink:href="#SVGID_5_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_6_);fill:#fff" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/><path style="opacity:.13;clip-path:url(#SVGID_6_);fill:#020202" d="M190.0704 170.6095c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9546.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.663-2.8588-6.116.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C193.7885 171.3104 191.9774 170.6095 190.0704 170.6095z"/></g><g><defs><path id="SVGID_7_" d="M189.0898 155.7617c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8066-21.8612-21.8613.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 158.2134 183.0612 155.7617 189.0898 155.7617zm7.9941 21.8613c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 181.8248 197.0839 179.8256 197.0839 177.623z"/></defs><clipPath id="SVGID_8_"><use xlink:href="#SVGID_7_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_8_);fill:#d95140" d="M172.6595 161.1903c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 154.7811 176.6171 157.2327 172.6595 161.1903z"/></g><rect x="198.8952" y="170.69" style="fill:#d95140" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#56a55c" d="M189.5379 264.6147l.0012-.0012c7.7751.0012 15.0294 4.1862 18.932 10.9235 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3304-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032-5.8394.0-11.3281-2.2733-15.458-6.4032-4.13-4.13-6.4032-9.6186-6.4056-15.4628.0012-6.0249 2.454-11.4897 6.4116-15.4472C178.0458 267.0663 183.5105 264.6135 189.5379 264.6147zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6538 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403C196.7013 290.6202 197.5321 288.6115 197.5309 286.4723z"/><path style="opacity:.3;fill:#fff" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/><g><defs><path id="SVGID_9_" d="M195.1881 292.1268c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.9989 7.9942-7.9941 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.5344 288.6792 196.6366 290.6783 195.1881 292.1268z"/></defs><clipPath id="SVGID_10_"><use xlink:href="#SVGID_9_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_10_);fill:#fff" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/><path style="opacity:.13;clip-path:url(#SVGID_10_);fill:#020202" d="M190.5209 279.463c-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7446-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9941 2.3802-1e-4 4.616 1.0833 6.1218 2.8788C194.239 280.164 192.4279 279.463 190.5209 279.463z"/></g><g><defs><path id="SVGID_11_" d="M189.5403 264.6153c7.7763.0 15.0283 4.1826 18.926 10.9151 1.9201 3.3135 2.9377 7.0987 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8613-12.0547.0024-21.8636-9.8065-21.8612-21.8613.0-6.0285 2.4516-11.492 6.4116-15.452C178.0482 267.0669 183.5117 264.6153 189.5403 264.6153zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9941.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.6366 290.6783 197.5344 288.6792 197.5344 286.4765z"/></defs><clipPath id="SVGID_12_"><use xlink:href="#SVGID_11_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_12_);fill:#56a55c" d="M173.11 270.0439c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8613 12.0547.0024 21.8636-9.797 21.8613-21.8613.0024-3.8474-1.0151-7.6326-2.9353-10.9462-3.8977-6.7325-11.1497-10.9151-18.926-10.9151C182.5311 263.6346 177.0676 266.0863 173.11 270.0439z"/></g></g><rect x="199.3456" y="279.5436" style="fill:#56a55c" width="122.6266" height="13.8671"/></g><g><g><path style="fill:#f1bc42" d="M189.0874 318.7208l.0012-.0012c7.7751.0012 15.0295 4.1862 18.932 10.9234 1.9177 3.3159 2.9305 7.1011 2.9293 10.9378.0 5.8394-2.2733 11.3305-6.4032 15.4604-4.1288 4.1288-9.6186 6.4032-15.458 6.4032s-11.328-2.2733-15.458-6.4032-6.4032-9.6186-6.4056-15.4628c.0012-6.025 2.454-11.4897 6.4116-15.4472C177.5953 321.1724 183.0601 318.7196 189.0874 318.7208zm7.993 21.8576c.0012-1.4042-.3687-2.7868-1.063-3.9887-1.4293-2.4684-4.0833-3.9995-6.9299-4.0019-4.4077.0024-7.993 3.5877-7.993 7.993.0 2.1356.832 4.1431 2.3427 5.6539 1.5083 1.5083 3.5159 2.3403 5.6503 2.3415 2.1356.0 4.1443-.8308 5.6539-2.3403S197.0816 342.7176 197.0804 340.5784z"/><path style="opacity:.3;fill:#fff" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/><g><defs><path id="SVGID_13_" d="M194.7376 346.2329c-1.4461 1.4461-3.4452 2.3439-5.6479 2.3439-4.4077-.0024-7.9918-3.5865-7.9942-7.9942.0024-4.4125 3.5937-7.999 7.9942-7.9942 2.8443.0 5.497 1.5323 6.924 3.9983.6991 1.2067 1.0702 2.5881 1.0702 3.9959C197.0839 342.7853 196.1861 344.7844 194.7376 346.2329z"/></defs><clipPath id="SVGID_14_"><use xlink:href="#SVGID_13_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_14_);fill:#fff" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/><path style="opacity:.13;clip-path:url(#SVGID_14_);fill:#020202" d="M190.0704 333.5691c-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0011 1.9547.7088 3.7452 1.8782 5.1354-1.7447-1.4674-2.8575-3.6631-2.8588-6.1161.0024-4.4125 3.5936-7.999 7.9942-7.9942 2.3802-1e-4 4.616 1.0834 6.1218 2.8788C193.7885 334.2701 191.9774 333.5691 190.0704 333.5691z"/></g><g><defs><path id="SVGID_15_" d="M189.0898 318.7214c7.7763.0 15.0283 4.1826 18.926 10.915 1.9201 3.3136 2.9377 7.0988 2.9353 10.9462.0024 12.0643-9.8065 21.8636-21.8613 21.8612-12.0547.0024-21.8636-9.8065-21.8612-21.8612.0-6.0285 2.4516-11.4921 6.4116-15.452C177.5977 321.173 183.0612 318.7214 189.0898 318.7214zm7.9941 21.8612c0-1.4078-.3711-2.7892-1.0702-3.9959-1.4269-2.466-4.0797-3.9983-6.924-3.9983-4.4005-.0048-7.9918 3.5817-7.9942 7.9942.0024 4.4077 3.5865 7.9918 7.9942 7.9942 2.2027.0 4.2018-.8978 5.6479-2.3439C196.1861 344.7844 197.0839 342.7853 197.0839 340.5826z"/></defs><clipPath id="SVGID_16_"><use xlink:href="#SVGID_15_" style="overflow:visible"/></clipPath><path style="clip-path:url(#SVGID_16_);fill:#f1bc42" d="M172.6595 324.15c-3.96 3.96-6.4116 9.4235-6.4116 15.452-.0024 12.0547 9.8066 21.8636 21.8613 21.8612 12.0547.0024 21.8636-9.797 21.8613-21.8612.0024-3.8474-1.0151-7.6327-2.9353-10.9462-3.8977-6.7324-11.1497-10.9151-18.926-10.9151C182.0806 317.7407 176.6171 320.1924 172.6595 324.15z"/></g></g><rect x="198.8952" y="333.6497" style="fill:#f1bc42" width="122.6266" height="13.8671"/></g></g></svg></span><span class=navbar-brand__name>Prometheus</span></a><div class="td-navbar-nav-scroll ms-md-auto" id=main_navbar><ul class=navbar-nav><li class=nav-item><a class=nav-link href=/prometheus-docs/about/><span>About</span></a></li><li class=nav-item><a class="nav-link active" href=/prometheus-docs/docs/><span>Docs</span></a></li></ul></div><div class="d-none d-lg-block"><div class=td-search><div class=td-search__icon></div><input type=search class="td-search__input form-control td-search-input" placeholder="Search this site…" aria-label="Search this site…" autocomplete=off></div></div></div></nav></header><div class="container-fluid td-outer"><div class=td-main><div class="row flex-xl-nowrap"><main class="col-12 col-md-9 col-xl-8 ps-md-5" role=main><div class=td-content><div class="pageinfo pageinfo-primary d-print-none"><p>This is the multi-page printable view of this section.
<a href=# onclick="return print(),!1">Click here to print</a>.</p><p><a href=/prometheus-docs/docs/>Return to the regular view of this page</a>.</p></div><h1 class=title>Prometheus Cluster Documentation</h1><div class=lead>Complete guide to using the Prometheus deep learning cluster at CYENS</div><ul><li>1: <a href=#pg-a39c9f7aa728d5fa3b973bc6ba49228a>Getting Started</a></li><li>2: <a href=#pg-40d50ee23f0cd222719c80af36385d7c>Hardware Specifications</a></li><li>3: <a href=#pg-fb509efb2332ee775ac6c60e40c1e63d>Environment Setup</a></li><li>4: <a href=#pg-e5df203f83389102e86e5c137a248eed>Job Submission</a></li><li>5: <a href=#pg-95f6b25b1a0c56fb7099d5ae22ef3d4d>Partitions & Queues</a></li><li>6: <a href=#pg-23ce6f3804b08b263e06e0c152bf4ee5>Storage Systems</a></li><li>7: <a href=#pg-ac02ffa0d48b518cf3420c0f071e98f8>Environment Modules</a></li><li>8: <a href=#pg-24286fd09742bbbd6cf8bf2a78c047d4>VS Code Remote Development</a></li><li>9: <a href=#pg-1fe159c1b0c0d7939ab765dc5b120cda>Software Installation</a></li></ul><div class=content><div class="pageinfo pageinfo-primary"><p>This section contains comprehensive documentation for the Prometheus cluster - a high-performance computing environment for deep learning research at CYENS.</p></div><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>The Prometheus cluster is a state-of-the-art deep learning computing facility featuring:</p><ul><li><strong>64 NVIDIA A5000 GPUs</strong> (24GB each) across 8 compute nodes</li><li><strong>4 NVIDIA A6000 Ada GPUs</strong> (48GB each) on a dedicated node</li><li><strong>4.6TB total GPU memory</strong> for large-scale model training</li><li><strong>High-performance Lustre storage</strong> with 305TB capacity</li><li><strong>SLURM job scheduler</strong> for efficient resource management</li></ul><p>This documentation will guide you through:</p><ul><li><strong><a href=/prometheus-docs/docs/getting-started/>Getting Started</a></strong> - SSH access and account setup</li><li><strong><a href=/prometheus-docs/docs/hardware/>Hardware Specifications</a></strong> - Detailed cluster architecture</li><li><strong><a href=/prometheus-docs/docs/job-submission/>Job Submission</a></strong> - SLURM batch and interactive jobs</li><li><strong><a href=/prometheus-docs/docs/partitions/>Partitions & Queues</a></strong> - Resource allocation policies</li><li><strong><a href=/prometheus-docs/docs/storage/>Storage Systems</a></strong> - File systems and quotas</li><li><strong><a href=/prometheus-docs/docs/modules/>Environment Modules</a></strong> - Software stack management</li><li><strong><a href=/prometheus-docs/docs/vscode/>VS Code Setup</a></strong> - Remote development environment</li><li><strong><a href=/prometheus-docs/docs/software/>Software Installation</a></strong> - Third-party libraries and tools</li></ul><h2 id=quick-start>Quick Start<a class=td-heading-self-link href=#quick-start aria-label="Heading self-link"></a></h2><ol><li><strong>Generate SSH keys</strong> and request cluster access</li><li><strong>Connect via SSH</strong> to <code>prometheus.cyens.org.cy</code></li><li><strong>Submit your first job</strong> using SLURM</li><li><strong>Set up development environment</strong> with modules or containers</li></ol><h2 id=cluster-specifications>Cluster Specifications<a class=td-heading-self-link href=#cluster-specifications aria-label="Heading self-link"></a></h2><h3 id=compute-resources>Compute Resources<a class=td-heading-self-link href=#compute-resources aria-label="Heading self-link"></a></h3><ul><li><strong>9 compute nodes</strong> total</li><li><strong>GPU nodes <code>gpu[01-08]</code></strong>: 8×A5000 GPUs each (64 total GPUs)</li><li><strong>GPU node <code>gpu09</code></strong>: 4×A6000 Ada GPUs (48GB VRAM each)</li><li><strong>512GB RAM</strong> per compute node</li><li><strong>32 CPU cores</strong> per node (AMD EPYC 7313)</li></ul><h3 id=storage>Storage<a class=td-heading-self-link href=#storage aria-label="Heading self-link"></a></h3><ul><li><strong>Home directories</strong>: 20GB SSD per user</li><li><strong>Shared storage</strong>: 30TB Lustre filesystem per group</li><li><strong>Local storage</strong>: 1TB NVMe SSD per compute node</li></ul><h3 id=networking-infrastructure>Networking Infrastructure<a class=td-heading-self-link href=#networking-infrastructure aria-label="Heading self-link"></a></h3><ul><li><strong>Management Network</strong>: Netgear M4300-52G switch with 48×1G ports plus 2×10GBASE-T and 2×SFP+</li><li><strong>High-Performance Interconnect</strong>: Mellanox HDR InfiniBand switch with 40×QSFP56 ports</li><li><strong>InfiniBand Speed</strong>: 200Gb/s HDR connectivity with hybrid copper cables</li><li><strong>Low Latency</strong>: Sub-microsecond messaging for distributed computing workloads</li></ul><h3 id=software-environment>Software Environment<a class=td-heading-self-link href=#software-environment aria-label="Heading self-link"></a></h3><ul><li><strong>Rocky Linux 8.5</strong> operating system</li><li><strong>SLURM</strong> workload manager</li><li><strong>Lmod</strong> environment modules</li><li><strong>CUDA 11.3+</strong> with deep learning frameworks</li></ul><h2 id=support--resources>Support & Resources<a class=td-heading-self-link href=#support--resources aria-label="Heading self-link"></a></h2><ul><li><strong>System administrators</strong>: Contact your MRG leader</li><li><strong>Documentation</strong>: This site and <code>/opt/cluster/docs/</code></li><li><strong>Cluster status</strong>: Monitor with <code>sinfo</code> and <code>squeue</code></li></ul><p>For detailed instructions, start with the <a href=/prometheus-docs/docs/getting-started/>Getting Started</a> guide.</p></div></div><div class=td-content style=page-break-before:always><h1 id=pg-a39c9f7aa728d5fa3b973bc6ba49228a>1 - Getting Started</h1><div class=lead>SSH access and initial setup for the Prometheus cluster</div><h2 id=prerequisites>Prerequisites<a class=td-heading-self-link href=#prerequisites aria-label="Heading self-link"></a></h2><p>Before accessing the Prometheus cluster, you need:</p><ul><li>A valid cluster account (contact your MRG leader)</li><li>SSH client installed on your local machine</li><li>Basic familiarity with Linux command line</li></ul><h2 id=generate-ssh-keys>Generate SSH Keys<a class=td-heading-self-link href=#generate-ssh-keys aria-label="Heading self-link"></a></h2><p>The Prometheus cluster uses <strong>RSA key authentication</strong> for secure access. You need to generate a public/private key pair:</p><h3 id=step-1-create-ssh-key-pair>Step 1: Create SSH Key Pair<a class=td-heading-self-link href=#step-1-create-ssh-key-pair aria-label="Heading self-link"></a></h3><p>Open your terminal and run:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ssh-keygen
</span></span></code></pre></div><p>Follow the on-screen instructions. This creates:</p><ul><li><strong>Private key</strong>: <code>~/.ssh/id_rsa</code> (keep this secure!)</li><li><strong>Public key</strong>: <code>~/.ssh/id_rsa.pub</code> (share this with administrators)</li></ul><h3 id=step-2-secure-your-private-key>Step 2: Secure Your Private Key<a class=td-heading-self-link href=#step-2-secure-your-private-key aria-label="Heading self-link"></a></h3><p>For Linux/Mac users, set proper permissions:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>chmod <span style=color:#0000cf;font-weight:700>600</span> ~/.ssh/id_rsa
</span></span></code></pre></div><h3 id=step-3-request-cluster-access>Step 3: Request Cluster Access<a class=td-heading-self-link href=#step-3-request-cluster-access aria-label="Heading self-link"></a></h3><ol><li><strong>Send your public key</strong> to your MRG leader</li><li><strong>Request a Prometheus account</strong></li><li><strong>Wait for account confirmation</strong></li></ol><h3 id=step-4-add-passphrase-optional-but-recommended>Step 4: Add Passphrase (Optional but Recommended)<a class=td-heading-self-link href=#step-4-add-passphrase-optional-but-recommended aria-label="Heading self-link"></a></h3><p>For additional security, add a passphrase to your key:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ssh-keygen -p -f ~/.ssh/id_rsa
</span></span></code></pre></div><h2 id=connect-to-prometheus>Connect to Prometheus<a class=td-heading-self-link href=#connect-to-prometheus aria-label="Heading self-link"></a></h2><h3 id=configure-ssh-client>Configure SSH Client<a class=td-heading-self-link href=#configure-ssh-client aria-label="Heading self-link"></a></h3><p>Create or edit <code>~/.ssh/config</code> file with the following content:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Host prometheus
</span></span><span style=display:flex><span>  Hostname prometheus.cyens.org.cy
</span></span><span style=display:flex><span>  User &lt;your-username&gt;
</span></span><span style=display:flex><span>  IdentityFile ~/.ssh/id_rsa
</span></span></code></pre></div><p>Replace <code>&lt;your-username></code> with your actual cluster username.</p><h3 id=connect-via-ssh>Connect via SSH<a class=td-heading-self-link href=#connect-via-ssh aria-label="Heading self-link"></a></h3><p>Once your account is activated, connect using:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ssh prometheus
</span></span></code></pre></div><p>You should now be logged into the Prometheus head node!</p><h2 id=first-login-setup>First Login Setup<a class=td-heading-self-link href=#first-login-setup aria-label="Heading self-link"></a></h2><h3 id=check-your-environment>Check Your Environment<a class=td-heading-self-link href=#check-your-environment aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check current directory</span>
</span></span><span style=display:flex><span><span style=color:#204a87>pwd</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># List available partitions</span>
</span></span><span style=display:flex><span>sinfo
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check your groups</span>
</span></span><span style=display:flex><span>groups
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># View your home directory quota</span>
</span></span><span style=display:flex><span>quota -us
</span></span></code></pre></div><h3 id=understand-the-file-system>Understand the File System<a class=td-heading-self-link href=#understand-the-file-system aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Your home directory (20GB limit)</span>
</span></span><span style=display:flex><span>ls -la /trinity/home/<span style=color:#000>$USER</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Shared group storage (30TB per group)</span>
</span></span><span style=display:flex><span>ls -la /lustreFS/data/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check group quota</span>
</span></span><span style=display:flex><span>lfs quota -gh &lt;group-name&gt; /lustreFS/
</span></span></code></pre></div><h2 id=cluster-architecture-overview>Cluster Architecture Overview<a class=td-heading-self-link href=#cluster-architecture-overview aria-label="Heading self-link"></a></h2><p>The Prometheus cluster consists of:</p><h3 id=head-node>Head Node<a class=td-heading-self-link href=#head-node aria-label="Heading self-link"></a></h3><ul><li><strong>Login and job submission</strong> point</li><li><strong>DO NOT run compute jobs here</strong></li><li>Used for file management and job scheduling</li></ul><h3 id=compute-nodes>Compute Nodes<a class=td-heading-self-link href=#compute-nodes aria-label="Heading self-link"></a></h3><ul><li><strong><code>gpu[01-08]</code></strong>: 8 nodes with A5000 GPUs (8 GPUs each)</li><li><strong><code>gpu09</code></strong>: 1 node with A6000 Ada GPUs (4 GPUs)</li><li><strong>512GB RAM</strong> and <strong>32 CPU cores</strong> per node</li></ul><h3 id=storage-systems>Storage Systems<a class=td-heading-self-link href=#storage-systems aria-label="Heading self-link"></a></h3><ul><li><strong><code>/trinity/home/</code></strong>: Personal home directories (SSD, 20GB limit)</li><li><strong><code>/lustreFS/data/</code></strong>: Shared group storage (305TB Lustre filesystem)</li><li><strong>Local storage</strong>: 1TB NVMe on each compute node</li></ul><h2 id=important-usage-rules>Important Usage Rules<a class=td-heading-self-link href=#important-usage-rules aria-label="Heading self-link"></a></h2><div class="alert alert-warning" role=alert><h4 class=alert-heading>Critical</h4><strong>NEVER run compute jobs directly on the head node!</strong> Always use SLURM to submit jobs to compute nodes.</div><h3 id=checking-cluster-status>Checking Cluster Status<a class=td-heading-self-link href=#checking-cluster-status aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># View partition information</span>
</span></span><span style=display:flex><span>sinfo
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check job queue</span>
</span></span><span style=display:flex><span>squeue
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check your running jobs</span>
</span></span><span style=display:flex><span>squeue -u <span style=color:#000>$USER</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># View detailed node information</span>
</span></span><span style=display:flex><span>scontrol show nodes
</span></span></code></pre></div><p>Example <code>sinfo</code> output:</p><pre tabindex=0><code>PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST
defq*        up   infinite      6  idle~ gpu[03-08]
defq*        up   infinite      1    mix gpu02
defq*        up   infinite      1   idle gpu01
a6000        up   infinite      1    mix gpu09
</code></pre><h2 id=next-steps>Next Steps<a class=td-heading-self-link href=#next-steps aria-label="Heading self-link"></a></h2><p>Now that you&rsquo;re connected to Prometheus:</p><ol><li><strong>Set up your development environment</strong> - See <a href=../modules/>Environment Modules</a></li><li><strong>Learn about partitions and queues</strong> - Read <a href=../partitions/>Partitions & Queues</a></li><li><strong>Submit your first job</strong> - Follow <a href=../job-submission/>Job Submission</a></li><li><strong>Configure VS Code</strong> (optional) - See <a href=../vscode/>VS Code Setup</a></li></ol><h2 id=getting-help>Getting Help<a class=td-heading-self-link href=#getting-help aria-label="Heading self-link"></a></h2><ul><li><strong>Cluster status</strong>: Use <code>sinfo</code> and <code>squeue</code> commands</li><li><strong>Documentation</strong>: Check <code>/opt/cluster/docs/</code> on the cluster</li><li><strong>Support</strong>: Contact your MRG leader</li><li><strong>System issues</strong>: Report to cluster administrators</li></ul><h2 id=common-first-time-issues>Common First-Time Issues<a class=td-heading-self-link href=#common-first-time-issues aria-label="Heading self-link"></a></h2><h3 id=permission-denied-publickey>&ldquo;Permission denied (publickey)&rdquo;<a class=td-heading-self-link href=#permission-denied-publickey aria-label="Heading self-link"></a></h3><ul><li>Verify your public key was added to the cluster</li><li>Check your SSH config file syntax</li><li>Ensure private key permissions are correct (<code>chmod 600</code>)</li></ul><h3 id=connection-refused>&ldquo;Connection refused&rdquo;<a class=td-heading-self-link href=#connection-refused aria-label="Heading self-link"></a></h3><ul><li>Verify the hostname: <code>prometheus.cyens.org.cy</code></li><li>Check if you&rsquo;re connected to the internet</li><li>Confirm your account is activated</li></ul><h3 id=quota-exceeded>&ldquo;Quota exceeded&rdquo;<a class=td-heading-self-link href=#quota-exceeded aria-label="Heading self-link"></a></h3><ul><li>Home directory has a 20GB limit</li><li>Use group storage in <code>/lustreFS/data/</code> for large files</li><li>Check usage with <code>quota -us</code></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-40d50ee23f0cd222719c80af36385d7c>2 - Hardware Specifications</h1><div class=lead>Detailed hardware specifications of the Prometheus cluster</div><h2 id=cluster-overview>Cluster Overview<a class=td-heading-self-link href=#cluster-overview aria-label="Heading self-link"></a></h2><p>The Prometheus cluster features a modern architecture optimized for deep learning workloads with high-performance GPUs, abundant memory, and fast storage systems.</p><h2 id=head-node>Head Node<a class=td-heading-self-link href=#head-node aria-label="Heading self-link"></a></h2><p><strong>Management and login node for the cluster</strong></p><h3 id=hardware-configuration>Hardware Configuration<a class=td-heading-self-link href=#hardware-configuration aria-label="Heading self-link"></a></h3><ul><li><strong>Chassis</strong>: GIGABYTE R182-Z90-00</li><li><strong>Motherboard</strong>: GIGABYTE MZ92-FS0-00</li><li><strong>CPU</strong>: 2× AMD EPYC 7313 (16 cores/32 threads each)</li><li><strong>Total CPU Cores</strong>: 32 cores / 64 threads</li><li><strong>RAM</strong>: 16× 32GB Samsung M393A4K40EB3-CWE</li><li><strong>Total RAM</strong>: 512GB DDR4</li><li><strong>Storage</strong>: 2× 1.92TB Intel SSDSC2KB019T8 SSD</li><li><strong>File System</strong>: <code>/trinity/home</code> (400GB allocated)</li></ul><h3 id=purpose>Purpose<a class=td-heading-self-link href=#purpose aria-label="Heading self-link"></a></h3><ul><li>SSH login and job submission</li><li>File management and transfers</li><li>SLURM job scheduling</li><li><strong>Not for compute workloads</strong></li></ul><h2 id=compute-nodes>Compute Nodes<a class=td-heading-self-link href=#compute-nodes aria-label="Heading self-link"></a></h2><h3 id=gpu-nodes-gpu01-08-8-nodes>GPU Nodes <code>gpu[01-08]</code> (8 nodes)<a class=td-heading-self-link href=#gpu-nodes-gpu01-08-8-nodes aria-label="Heading self-link"></a></h3><p><strong>Primary compute nodes with NVIDIA A5000 GPUs</strong></p><h4 id=hardware-configuration-1>Hardware Configuration<a class=td-heading-self-link href=#hardware-configuration-1 aria-label="Heading self-link"></a></h4><ul><li><strong>Chassis</strong>: Supermicro AS-4124GS-TNR</li><li><strong>Motherboard</strong>: Supermicro H12DSG-O-CPU</li><li><strong>CPU</strong>: 2× AMD EPYC 7313 (16 cores/32 threads each)</li><li><strong>Total CPU Cores</strong>: 32 cores / 64 threads per node</li><li><strong>RAM</strong>: 16× 32GB SK Hynix HMAA4GR7AJR8N-XN</li><li><strong>Total RAM</strong>: 512GB DDR4 per node</li><li><strong>Local Storage</strong>: 1× 1TB Samsung SSD 980 NVMe</li></ul><h4 id=gpu-specifications>GPU Specifications<a class=td-heading-self-link href=#gpu-specifications aria-label="Heading self-link"></a></h4><ul><li><strong>GPU Model</strong>: NVIDIA A5000</li><li><strong>GPU Count</strong>: 8 GPUs per node (64 total across all nodes)</li><li><strong>GPU Memory</strong>: 24GB GDDR6 per GPU</li><li><strong>CUDA Cores</strong>: 8,192 per GPU</li><li><strong>Tensor Cores</strong>: 256 RT Cores (2nd gen)</li><li><strong>Peak Performance</strong>: 27.8 TFLOPS FP32 per GPU</li><li><strong>Memory Bandwidth</strong>: 768 GB/s per GPU</li></ul><h4 id=total-resources-gpu01-08>Total Resources (gpu[01-08])<a class=td-heading-self-link href=#total-resources-gpu01-08 aria-label="Heading self-link"></a></h4><ul><li><strong>Total GPUs</strong>: 64× NVIDIA A5000</li><li><strong>Total GPU Memory</strong>: 1,536GB (1.5TB)</li><li><strong>Total CPU Cores</strong>: 256 cores / 512 threads</li><li><strong>Total System RAM</strong>: 4TB DDR4</li></ul><h3 id=gpu-node-gpu09-1-node>GPU Node <code>gpu09</code> (1 node)<a class=td-heading-self-link href=#gpu-node-gpu09-1-node aria-label="Heading self-link"></a></h3><p><strong>High-memory GPU node with NVIDIA A6000 Ada</strong></p><h4 id=hardware-configuration-2>Hardware Configuration<a class=td-heading-self-link href=#hardware-configuration-2 aria-label="Heading self-link"></a></h4><ul><li><strong>Chassis</strong>: ASUS RS720A-E11-RS12</li><li><strong>Motherboard</strong>: ASUS KMPP-D32</li><li><strong>CPU</strong>: 2× AMD EPYC 7313 (16 cores/32 threads each)</li><li><strong>Total CPU Cores</strong>: 32 cores / 64 threads</li><li><strong>RAM</strong>: 16× 32GB SK Hynix HMAA4GR7AJR8N-XN</li><li><strong>Total RAM</strong>: 512GB DDR4</li><li><strong>Local Storage</strong>: 1× 1TB Samsung SSD 980 NVMe</li></ul><h4 id=gpu-specifications-1>GPU Specifications<a class=td-heading-self-link href=#gpu-specifications-1 aria-label="Heading self-link"></a></h4><ul><li><strong>GPU Model</strong>: NVIDIA RTX A6000 Ada Generation</li><li><strong>GPU Count</strong>: 4 GPUs</li><li><strong>GPU Memory</strong>: 48GB GDDR6 per GPU</li><li><strong>CUDA Cores</strong>: 18,176 per GPU</li><li><strong>Tensor Cores</strong>: 568 (4th gen)</li><li><strong>Peak Performance</strong>: 91.06 TFLOPS FP32 per GPU</li><li><strong>Memory Bandwidth</strong>: 960 GB/s per GPU</li></ul><h4 id=total-resources-gpu09>Total Resources (gpu09)<a class=td-heading-self-link href=#total-resources-gpu09 aria-label="Heading self-link"></a></h4><ul><li><strong>Total GPUs</strong>: 4× NVIDIA A6000 Ada</li><li><strong>Total GPU Memory</strong>: 192GB</li><li><strong>Total CPU Cores</strong>: 32 cores / 64 threads</li><li><strong>Total System RAM</strong>: 512GB DDR4</li></ul><h2 id=storage-nodes>Storage Nodes<a class=td-heading-self-link href=#storage-nodes aria-label="Heading self-link"></a></h2><h3 id=storage-architecture>Storage Architecture<a class=td-heading-self-link href=#storage-architecture aria-label="Heading self-link"></a></h3><p><strong>High-performance parallel file system</strong></p><h4 id=hardware-configuration-2-storage-nodes>Hardware Configuration (2 storage nodes)<a class=td-heading-self-link href=#hardware-configuration-2-storage-nodes aria-label="Heading self-link"></a></h4><ul><li><strong>Chassis</strong>: Supermicro Super Server</li><li><strong>Motherboard</strong>: Supermicro H12SSL-i</li><li><strong>CPU</strong>: 1× AMD EPYC 7302P (16 cores/32 threads each)</li><li><strong>RAM</strong>: 8× 16GB Samsung M393A2K40DB3-CWE</li><li><strong>Total RAM</strong>: 256GB DDR4 per node</li><li><strong>OS Storage</strong>: 2× 240GB Intel SSDSC2KB240G7 SSD</li><li><strong>Data Storage</strong>: 24× 7.68TB Samsung MZILT7T6HALA/007 NVMe SSD</li></ul><h4 id=storage-specifications>Storage Specifications<a class=td-heading-self-link href=#storage-specifications aria-label="Heading self-link"></a></h4><ul><li><strong>File System</strong>: Lustre parallel file system</li><li><strong>Mount Point</strong>: <code>/lustreFS</code></li><li><strong>Raw Capacity</strong>: 368TB per storage node</li><li><strong>Total Raw Capacity</strong>: 736TB across both nodes</li><li><strong>Usable Capacity</strong>: ~305TB (after RAID and file system overhead)</li><li><strong>Performance</strong>: High-throughput parallel I/O</li></ul><h2 id=software-environment>Software Environment<a class=td-heading-self-link href=#software-environment aria-label="Heading self-link"></a></h2><h3 id=operating-system>Operating System<a class=td-heading-self-link href=#operating-system aria-label="Heading self-link"></a></h3><ul><li><strong>Distribution</strong>: Rocky Linux 8.5 (Green Obsidian)</li><li><strong>Kernel Version</strong>: 4.18.0-348.23.1.el8_5.x86_64</li><li><strong>Architecture</strong>: x86_64</li></ul><h3 id=management-software>Management Software<a class=td-heading-self-link href=#management-software aria-label="Heading self-link"></a></h3><ul><li><strong>Job Scheduler</strong>: SLURM Workload Manager</li><li><strong>Module System</strong>: Lmod (Lua-based Environment Modules)</li><li><strong>File System</strong>: Lustre for parallel storage</li></ul><h3 id=development-tools>Development Tools<a class=td-heading-self-link href=#development-tools aria-label="Heading self-link"></a></h3><ul><li><strong>CUDA Toolkit</strong>: 11.3+ with cuDNN</li><li><strong>Compilers</strong>: GCC, Intel, NVCC</li><li><strong>MPI</strong>: OpenMPI, MPICH</li><li><strong>Python</strong>: Multiple versions with conda/pip</li><li><strong>Deep Learning</strong>: PyTorch, TensorFlow, JAX</li><li><strong>Containers</strong>: Singularity/Apptainer support</li></ul><h2 id=network-architecture>Network Architecture<a class=td-heading-self-link href=#network-architecture aria-label="Heading self-link"></a></h2><h3 id=interconnect>Interconnect<a class=td-heading-self-link href=#interconnect aria-label="Heading self-link"></a></h3><ul><li><strong>Compute Network</strong>: High-speed Ethernet</li><li><strong>Storage Network</strong>: Dedicated Lustre network</li><li><strong>Management Network</strong>: Separate administrative network</li></ul><h3 id=bandwidth>Bandwidth<a class=td-heading-self-link href=#bandwidth aria-label="Heading self-link"></a></h3><ul><li><strong>Node-to-Node</strong>: High-bandwidth for distributed training</li><li><strong>Storage Access</strong>: Optimized for parallel I/O workloads</li><li><strong>External Access</strong>: Internet connectivity for downloads</li></ul><h2 id=performance-characteristics>Performance Characteristics<a class=td-heading-self-link href=#performance-characteristics aria-label="Heading self-link"></a></h2><h3 id=compute-performance>Compute Performance<a class=td-heading-self-link href=#compute-performance aria-label="Heading self-link"></a></h3><ul><li><strong>Total GPU Performance</strong>:<ul><li>A5000 nodes: 1,779 TFLOPS FP32 (64 × 27.8)</li><li>A6000 node: 364 TFLOPS FP32 (4 × 91.06)</li><li><strong>Combined</strong>: ~2,143 TFLOPS FP32</li></ul></li><li><strong>Memory Bandwidth</strong>:<ul><li>A5000 total: 49,152 GB/s</li><li>A6000 total: 3,840 GB/s</li><li><strong>Combined</strong>: ~53TB/s GPU memory bandwidth</li></ul></li></ul><h3 id=storage-performance>Storage Performance<a class=td-heading-self-link href=#storage-performance aria-label="Heading self-link"></a></h3><ul><li><strong>Lustre File System</strong>: High-throughput parallel I/O</li><li><strong>Local NVMe</strong>: High IOPS for temporary data</li><li><strong>Home Directories</strong>: SSD-backed for fast access</li></ul><h2 id=resource-allocation>Resource Allocation<a class=td-heading-self-link href=#resource-allocation aria-label="Heading self-link"></a></h2><h3 id=per-node-resources>Per-Node Resources<a class=td-heading-self-link href=#per-node-resources aria-label="Heading self-link"></a></h3><ul><li><strong>CPU Cores</strong>: 32 physical / 64 logical per node</li><li><strong>System Memory</strong>: 512GB DDR4 per node</li><li><strong>GPU Memory</strong>:<ul><li>A5000 nodes: 192GB per node (8 × 24GB)</li><li>A6000 node: 192GB (4 × 48GB)</li></ul></li><li><strong>Local Storage</strong>: 1TB NVMe SSD per compute node</li></ul><h3 id=total-cluster-resources>Total Cluster Resources<a class=td-heading-self-link href=#total-cluster-resources aria-label="Heading self-link"></a></h3><ul><li><strong>Compute Nodes</strong>: 9 total</li><li><strong>CPU Cores</strong>: 288 physical / 576 logical</li><li><strong>System Memory</strong>: 4.5TB DDR4</li><li><strong>GPUs</strong>: 68 total (64 A5000 + 4 A6000 Ada)</li><li><strong>GPU Memory</strong>: 1.728TB total</li><li><strong>Shared Storage</strong>: 305TB Lustre + local NVMe</li></ul><h2 id=use-cases-and-workloads>Use Cases and Workloads<a class=td-heading-self-link href=#use-cases-and-workloads aria-label="Heading self-link"></a></h2><h3 id=optimized-for>Optimized For<a class=td-heading-self-link href=#optimized-for aria-label="Heading self-link"></a></h3><ul><li><strong>Large Language Models</strong>: High GPU memory for transformer models</li><li><strong>Computer Vision</strong>: Parallel training on multiple GPUs</li><li><strong>Distributed Training</strong>: Multi-node deep learning</li><li><strong>High-throughput Computing</strong>: Batch processing workflows</li><li><strong>Interactive Development</strong>: Jupyter notebooks and VS Code</li></ul><h3 id=performance-considerations>Performance Considerations<a class=td-heading-self-link href=#performance-considerations aria-label="Heading self-link"></a></h3><ul><li><strong>Memory-bound workloads</strong>: Benefit from A6000&rsquo;s 48GB VRAM</li><li><strong>Compute-intensive tasks</strong>: Leverage A5000&rsquo;s efficiency</li><li><strong>Data-intensive jobs</strong>: Utilize high-performance Lustre storage</li><li><strong>Multi-GPU training</strong>: Scale across nodes with SLURM</li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-fb509efb2332ee775ac6c60e40c1e63d>3 - Environment Setup</h1><div class=lead>Configure your development environment on the Prometheus cluster</div><h2 id=development-environment-options>Development Environment Options<a class=td-heading-self-link href=#development-environment-options aria-label="Heading self-link"></a></h2><p>The Prometheus cluster supports multiple development environments:</p><ol><li><strong>Container-based</strong> (Recommended)</li><li><strong>Module-based</strong> (Traditional HPC)</li><li><strong>Custom Python environments</strong></li></ol><h2 id=container-based-setup>Container-Based Setup<a class=td-heading-self-link href=#container-based-setup aria-label="Heading self-link"></a></h2><h3 id=using-pre-built-containers>Using Pre-built Containers<a class=td-heading-self-link href=#using-pre-built-containers aria-label="Heading self-link"></a></h3><p>The cluster provides optimized containers for common deep learning frameworks:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># List available containers</span>
</span></span><span style=display:flex><span>ls /shared/containers/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Use PyTorch container</span>
</span></span><span style=display:flex><span>singularity shell --nv /shared/containers/pytorch-gpu.sif
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Use TensorFlow container</span>
</span></span><span style=display:flex><span>singularity shell --nv /shared/containers/tensorflow-gpu.sif
</span></span></code></pre></div><h3 id=building-custom-containers>Building Custom Containers<a class=td-heading-self-link href=#building-custom-containers aria-label="Heading self-link"></a></h3><p>Create a definition file (<code>pytorch-custom.def</code>):</p><pre tabindex=0><code class=language-singularity data-lang=singularity>Bootstrap: docker
From: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel

%post
    apt-get update &amp;&amp; apt-get install -y \
        git \
        vim \
        htop \
        tmux
    
    pip install \
        transformers \
        datasets \
        wandb \
        jupyter \
        matplotlib \
        seaborn

%environment
    export CUDA_VISIBLE_DEVICES=0,1,2,3
    export PYTHONPATH=/opt/code:$PYTHONPATH

%runscript
    exec &#34;$@&#34;
</code></pre><p>Build the container:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>singularity build pytorch-custom.sif pytorch-custom.def
</span></span></code></pre></div><h2 id=python-environment-setup>Python Environment Setup<a class=td-heading-self-link href=#python-environment-setup aria-label="Heading self-link"></a></h2><h3 id=using-conda>Using Conda<a class=td-heading-self-link href=#using-conda aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load conda module</span>
</span></span><span style=display:flex><span>module load conda
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Create environment</span>
</span></span><span style=display:flex><span>conda create -n myenv <span style=color:#000>python</span><span style=color:#ce5c00;font-weight:700>=</span>3.9
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Activate environment</span>
</span></span><span style=display:flex><span>conda activate myenv
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install packages</span>
</span></span><span style=display:flex><span>conda install pytorch torchvision torchaudio pytorch-cuda<span style=color:#ce5c00;font-weight:700>=</span>11.8 -c pytorch -c nvidia
</span></span><span style=display:flex><span>conda install -c conda-forge jupyter matplotlib pandas
</span></span></code></pre></div><h3 id=using-pip-with-virtual-environments>Using pip with virtual environments<a class=td-heading-self-link href=#using-pip-with-virtual-environments aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load Python module</span>
</span></span><span style=display:flex><span>module load python/3.9
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Create virtual environment</span>
</span></span><span style=display:flex><span>python -m venv ~/venvs/deeplearning
</span></span><span style=display:flex><span><span style=color:#204a87>source</span> ~/venvs/deeplearning/bin/activate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install packages</span>
</span></span><span style=display:flex><span>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
</span></span><span style=display:flex><span>pip install jupyter notebook jupyterlab
</span></span><span style=display:flex><span>pip install transformers datasets wandb
</span></span></code></pre></div><h2 id=gpu-environment-configuration>GPU Environment Configuration<a class=td-heading-self-link href=#gpu-environment-configuration aria-label="Heading self-link"></a></h2><h3 id=checking-gpu-availability>Checking GPU Availability<a class=td-heading-self-link href=#checking-gpu-availability aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check available GPUs</span>
</span></span><span style=display:flex><span>nvidia-smi
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check CUDA version</span>
</span></span><span style=display:flex><span>nvcc --version
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Test PyTorch GPU access</span>
</span></span><span style=display:flex><span>python -c <span style=color:#4e9a06>&#34;import torch; print(f&#39;CUDA available: {torch.cuda.is_available()}&#39;)&#34;</span>
</span></span></code></pre></div><h3 id=setting-gpu-visibility>Setting GPU Visibility<a class=td-heading-self-link href=#setting-gpu-visibility aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Use specific GPUs</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>CUDA_VISIBLE_DEVICES</span><span style=color:#ce5c00;font-weight:700>=</span>0,1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Use all available GPUs</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>CUDA_VISIBLE_DEVICES</span><span style=color:#ce5c00;font-weight:700>=</span>0,1,2,3,4,5,6,7
</span></span></code></pre></div><h2 id=jupyter-notebook-setup>Jupyter Notebook Setup<a class=td-heading-self-link href=#jupyter-notebook-setup aria-label="Heading self-link"></a></h2><h3 id=local-jupyter-on-compute-node>Local Jupyter on Compute Node<a class=td-heading-self-link href=#local-jupyter-on-compute-node aria-label="Heading self-link"></a></h3><ol><li><p><strong>Request an interactive session</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>srun --partition<span style=color:#ce5c00;font-weight:700>=</span>gpu --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:1 --time<span style=color:#ce5c00;font-weight:700>=</span>4:00:00 --pty bash
</span></span></code></pre></div></li><li><p><strong>Start Jupyter</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>module load python/3.9
</span></span><span style=display:flex><span><span style=color:#204a87>source</span> ~/venvs/deeplearning/bin/activate
</span></span><span style=display:flex><span>jupyter notebook --no-browser --port<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>8888</span> --ip<span style=color:#ce5c00;font-weight:700>=</span>0.0.0.0
</span></span></code></pre></div></li><li><p><strong>Set up SSH tunnel</strong> (from your local machine):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ssh -L 8888:compute-node:8888 username@prometheus-cluster.example.com
</span></span></code></pre></div></li></ol><h3 id=jupyterhub-access>JupyterHub Access<a class=td-heading-self-link href=#jupyterhub-access aria-label="Heading self-link"></a></h3><p>If available, access JupyterHub directly:</p><pre tabindex=0><code>https://jupyter.prometheus-cluster.example.com
</code></pre><h2 id=development-tools>Development Tools<a class=td-heading-self-link href=#development-tools aria-label="Heading self-link"></a></h2><h3 id=vs-code-remote-development>VS Code Remote Development<a class=td-heading-self-link href=#vs-code-remote-development aria-label="Heading self-link"></a></h3><ol><li><strong>Install VS Code</strong> with Remote-SSH extension</li><li><strong>Configure SSH connection</strong> in VS Code</li><li><strong>Connect to cluster</strong> and open your project folder</li></ol><h3 id=tmux-for-session-management>tmux for Session Management<a class=td-heading-self-link href=#tmux-for-session-management aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Start new session</span>
</span></span><span style=display:flex><span>tmux new-session -s training
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Detach session (Ctrl+b, then d)</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Reattach session</span>
</span></span><span style=display:flex><span>tmux attach-session -t training
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># List sessions</span>
</span></span><span style=display:flex><span>tmux list-sessions
</span></span></code></pre></div><h2 id=storage-and-data-access>Storage and Data Access<a class=td-heading-self-link href=#storage-and-data-access aria-label="Heading self-link"></a></h2><h3 id=home-directory-setup>Home Directory Setup<a class=td-heading-self-link href=#home-directory-setup aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Create project structure</span>
</span></span><span style=display:flex><span>mkdir -p ~/projects/<span style=color:#ce5c00;font-weight:700>{</span>experiments,datasets,models,scripts<span style=color:#ce5c00;font-weight:700>}</span>
</span></span><span style=display:flex><span>mkdir -p ~/logs
</span></span></code></pre></div><h3 id=using-shared-storage>Using Shared Storage<a class=td-heading-self-link href=#using-shared-storage aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Link shared datasets</span>
</span></span><span style=display:flex><span>ln -s /shared/datasets ~/datasets
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Copy models to your space</span>
</span></span><span style=display:flex><span>cp -r /shared/models/pretrained ~/models/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Use scratch space for temporary files</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>TMPDIR</span><span style=color:#ce5c00;font-weight:700>=</span>/scratch/<span style=color:#000>$USER</span>
</span></span><span style=display:flex><span>mkdir -p <span style=color:#000>$TMPDIR</span>
</span></span></code></pre></div><h2 id=environment-variables>Environment Variables<a class=td-heading-self-link href=#environment-variables aria-label="Heading self-link"></a></h2><p>Create <code>~/.cluster_env</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># CUDA settings</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>CUDA_VISIBLE_DEVICES</span><span style=color:#ce5c00;font-weight:700>=</span>0,1,2,3
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>CUDA_CACHE_PATH</span><span style=color:#ce5c00;font-weight:700>=</span>/scratch/<span style=color:#000>$USER</span>/cuda_cache
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Python settings</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>PYTHONPATH</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>$HOME</span>/projects:<span style=color:#000>$PYTHONPATH</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>JUPYTER_CONFIG_DIR</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>$HOME</span>/.jupyter
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Weights &amp; Biases</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>WANDB_DIR</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#000>$HOME</span>/logs/wandb
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>WANDB_CACHE_DIR</span><span style=color:#ce5c00;font-weight:700>=</span>/scratch/<span style=color:#000>$USER</span>/wandb_cache
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Hugging Face</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>HF_DATASETS_CACHE</span><span style=color:#ce5c00;font-weight:700>=</span>/scratch/<span style=color:#000>$USER</span>/hf_cache
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>TRANSFORMERS_CACHE</span><span style=color:#ce5c00;font-weight:700>=</span>/scratch/<span style=color:#000>$USER</span>/transformers_cache
</span></span></code></pre></div><p>Source it in your <code>.bashrc</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#39;source ~/.cluster_env&#39;</span> &gt;&gt; ~/.bashrc
</span></span></code></pre></div><h2 id=troubleshooting>Troubleshooting<a class=td-heading-self-link href=#troubleshooting aria-label="Heading self-link"></a></h2><h3 id=common-issues>Common Issues<a class=td-heading-self-link href=#common-issues aria-label="Heading self-link"></a></h3><p><strong>CUDA out of memory</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Clear GPU memory</span>
</span></span><span style=display:flex><span>nvidia-smi --gpu-reset
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Monitor GPU usage</span>
</span></span><span style=display:flex><span>watch -n <span style=color:#0000cf;font-weight:700>1</span> nvidia-smi
</span></span></code></pre></div><p><strong>Module not found</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check loaded modules</span>
</span></span><span style=display:flex><span>module list
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Reload environment</span>
</span></span><span style=display:flex><span><span style=color:#204a87>source</span> ~/.bashrc
</span></span></code></pre></div><p><strong>Permission denied</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check file permissions</span>
</span></span><span style=display:flex><span>ls -la
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Fix permissions</span>
</span></span><span style=display:flex><span>chmod <span style=color:#0000cf;font-weight:700>755</span> script.py
</span></span></code></pre></div><h2 id=next-steps>Next Steps<a class=td-heading-self-link href=#next-steps aria-label="Heading self-link"></a></h2><ul><li><strong>Submit your first job</strong>: <a href=../job-submission/>Job Submission Guide</a></li><li><strong>Monitor your work</strong>: <a href=../monitoring/>Monitoring Guide</a></li><li><strong>Manage data</strong>: <a href=../storage/>Storage Guide</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-e5df203f83389102e86e5c137a248eed>4 - Job Submission</h1><div class=lead>Submit and manage jobs using SLURM on the Prometheus cluster</div><h2 id=slurm-job-scheduler>SLURM Job Scheduler<a class=td-heading-self-link href=#slurm-job-scheduler aria-label="Heading self-link"></a></h2><p>The Prometheus cluster uses <strong>SLURM</strong> (Simple Linux Utility for Resource Management) for job scheduling and resource allocation. SLURM ensures fair resource sharing and efficient cluster utilization.</p><div class="alert alert-warning" role=alert><h4 class=alert-heading>Important</h4><strong>NEVER run compute jobs directly on the head node!</strong> Always use SLURM to submit jobs to compute nodes.</div><h2 id=interactive-jobs>Interactive Jobs<a class=td-heading-self-link href=#interactive-jobs aria-label="Heading self-link"></a></h2><p>Interactive jobs are perfect for development, testing, and debugging. Use <code>srun</code> to request resources immediately.</p><h3 id=basic-interactive-session>Basic Interactive Session<a class=td-heading-self-link href=#basic-interactive-session aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Request 1 GPU, 2 CPUs, 1GB RAM for 1 hour</span>
</span></span><span style=display:flex><span>srun -c <span style=color:#0000cf;font-weight:700>1</span> -n <span style=color:#0000cf;font-weight:700>1</span> -p defq --qos<span style=color:#ce5c00;font-weight:700>=</span>normal --mem<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>100</span> --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:1 -t 01:00 --pty /bin/bash
</span></span></code></pre></div><h3 id=interactive-session-parameters>Interactive Session Parameters<a class=td-heading-self-link href=#interactive-session-parameters aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Request specific resources</span>
</span></span><span style=display:flex><span>srun --partition<span style=color:#ce5c00;font-weight:700>=</span>defq <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>     --qos<span style=color:#ce5c00;font-weight:700>=</span>normal <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>     --cpus-per-task<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>4</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>     --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:2 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>     --mem<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>20000</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>     --time<span style=color:#ce5c00;font-weight:700>=</span>04:00:00 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>     --pty /bin/bash
</span></span></code></pre></div><h3 id=a6000-interactive-session>A6000 Interactive Session<a class=td-heading-self-link href=#a6000-interactive-session aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Request A6000 GPU with high memory</span>
</span></span><span style=display:flex><span>srun --partition<span style=color:#ce5c00;font-weight:700>=</span>a6000 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>     --qos<span style=color:#ce5c00;font-weight:700>=</span>normal-a6000 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>     --cpus-per-task<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>8</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>     --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:1 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>     --mem<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>50000</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>     --time<span style=color:#ce5c00;font-weight:700>=</span>02:00:00 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>     --pty /bin/bash
</span></span></code></pre></div><h2 id=batch-jobs>Batch Jobs<a class=td-heading-self-link href=#batch-jobs aria-label="Heading self-link"></a></h2><p>Batch jobs run unattended and are ideal for training models, parameter sweeps, and long-running experiments.</p><h3 id=basic-batch-script>Basic Batch Script<a class=td-heading-self-link href=#basic-batch-script aria-label="Heading self-link"></a></h3><p>Create a file <code>train_model.slurm</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span><span style=color:#8f5902;font-style:italic>#SBATCH -o res_%j.txt      # output file</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH -e res_%j.err      # error file</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH -J my_training     # job name</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --partition=defq   # partition</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --qos=normal       # priority queue</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --ntasks=1         # number of tasks</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --cpus-per-task=4  # CPU cores per task</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:2       # number of GPUs</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --mem=32000        # memory in MB</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=1-12:00     # time limit (1 day, 12 hours)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load required modules</span>
</span></span><span style=display:flex><span>module load CUDA/11.3.1
</span></span><span style=display:flex><span>module load Python/3.9.5
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Activate your environment</span>
</span></span><span style=display:flex><span><span style=color:#204a87>source</span> ~/anaconda3/bin/activate
</span></span><span style=display:flex><span>conda activate myenv
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Set CUDA devices</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>CUDA_VISIBLE_DEVICES</span><span style=color:#ce5c00;font-weight:700>=</span>0,1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Run your training script</span>
</span></span><span style=display:flex><span><span style=color:#204a87>cd</span> /lustreFS/data/mygroup/myproject
</span></span><span style=display:flex><span>python train_model.py --epochs <span style=color:#0000cf;font-weight:700>100</span> --batch-size <span style=color:#0000cf;font-weight:700>64</span>
</span></span></code></pre></div><h3 id=submit-the-batch-job>Submit the Batch Job<a class=td-heading-self-link href=#submit-the-batch-job aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sbatch train_model.slurm
</span></span></code></pre></div><h2 id=slurm-parameters-reference>SLURM Parameters Reference<a class=td-heading-self-link href=#slurm-parameters-reference aria-label="Heading self-link"></a></h2><h3 id=common-sbatch-directives>Common SBATCH Directives<a class=td-heading-self-link href=#common-sbatch-directives aria-label="Heading self-link"></a></h3><table><thead><tr><th>Parameter</th><th>Description</th><th>Example</th></tr></thead><tbody><tr><td><code>-J, --job-name</code></td><td>Job name</td><td><code>#SBATCH -J training_job</code></td></tr><tr><td><code>-o, --output</code></td><td>Output file</td><td><code>#SBATCH -o output_%j.txt</code></td></tr><tr><td><code>-e, --error</code></td><td>Error file</td><td><code>#SBATCH -e error_%j.err</code></td></tr><tr><td><code>-p, --partition</code></td><td>Partition</td><td><code>#SBATCH --partition=defq</code></td></tr><tr><td><code>--qos</code></td><td>Priority queue</td><td><code>#SBATCH --qos=normal</code></td></tr><tr><td><code>-n, --ntasks</code></td><td>Number of tasks</td><td><code>#SBATCH --ntasks=1</code></td></tr><tr><td><code>-c, --cpus-per-task</code></td><td>CPUs per task</td><td><code>#SBATCH --cpus-per-task=8</code></td></tr><tr><td><code>--gres</code></td><td>Generic resources</td><td><code>#SBATCH --gres=gpu:4</code></td></tr><tr><td><code>--mem</code></td><td>Memory (MB)</td><td><code>#SBATCH --mem=64000</code></td></tr><tr><td><code>-t, --time</code></td><td>Time limit</td><td><code>#SBATCH --time=2-00:00</code></td></tr></tbody></table><h3 id=time-format-examples>Time Format Examples<a class=td-heading-self-link href=#time-format-examples aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 30 minutes</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=00:30:00</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 4 hours</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=04:00:00</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 1 day, 12 hours</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=1-12:00:00</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 7 days (maximum for long queue)</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=7-00:00:00</span>
</span></span></code></pre></div><h3 id=gpu-allocation-examples>GPU Allocation Examples<a class=td-heading-self-link href=#gpu-allocation-examples aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Request any available GPU</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Request multiple GPUs</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:4</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># All GPUs on A5000 node (8 GPUs)</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:8</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># A6000 GPU specifically</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --partition=a6000 --gres=gpu:1</span>
</span></span></code></pre></div><h2 id=advanced-job-examples>Advanced Job Examples<a class=td-heading-self-link href=#advanced-job-examples aria-label="Heading self-link"></a></h2><h3 id=multi-gpu-training-script>Multi-GPU Training Script<a class=td-heading-self-link href=#multi-gpu-training-script aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span><span style=color:#8f5902;font-style:italic>#SBATCH -J multi_gpu_training</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --partition=defq</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --qos=long</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --ntasks=1</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --cpus-per-task=16</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:4</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --mem=128000</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=2-00:00:00</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH -o logs/multi_gpu_%j.out</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH -e logs/multi_gpu_%j.err</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Ensure logs directory exists</span>
</span></span><span style=display:flex><span>mkdir -p logs
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load modules</span>
</span></span><span style=display:flex><span>module load CUDA/11.3.1 Python/3.9.5
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Activate environment</span>
</span></span><span style=display:flex><span>conda activate pytorch-env
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Set environment variables</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>CUDA_VISIBLE_DEVICES</span><span style=color:#ce5c00;font-weight:700>=</span>0,1,2,3
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>OMP_NUM_THREADS</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>4</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Run distributed training</span>
</span></span><span style=display:flex><span>python -m torch.distributed.launch <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    --nproc_per_node<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>4</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    --master_port<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>12355</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    train_distributed.py <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    --config config.yaml <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    --output-dir /lustreFS/data/mygroup/results
</span></span></code></pre></div><h3 id=parameter-sweep-with-job-arrays>Parameter Sweep with Job Arrays<a class=td-heading-self-link href=#parameter-sweep-with-job-arrays aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span><span style=color:#8f5902;font-style:italic>#SBATCH -J param_sweep</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --partition=defq</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --qos=normal</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --array=1-20%5      # 20 jobs, max 5 running</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --cpus-per-task=2</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:1</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --mem=16000</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=08:00:00</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH -o logs/sweep_%A_%a.out</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH -e logs/sweep_%A_%a.err</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Parameter arrays</span>
</span></span><span style=display:flex><span><span style=color:#000>learning_rates</span><span style=color:#ce5c00;font-weight:700>=(</span>0.001 0.01 0.1 0.2<span style=color:#ce5c00;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#000>batch_sizes</span><span style=color:#ce5c00;font-weight:700>=(</span><span style=color:#0000cf;font-weight:700>16</span> <span style=color:#0000cf;font-weight:700>32</span> <span style=color:#0000cf;font-weight:700>64</span> <span style=color:#0000cf;font-weight:700>128</span> 256<span style=color:#ce5c00;font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Calculate parameters for this task</span>
</span></span><span style=display:flex><span><span style=color:#000>lr_idx</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>$((</span> <span style=color:#ce5c00;font-weight:700>(</span><span style=color:#000>$SLURM_ARRAY_TASK_ID</span> <span style=color:#ce5c00;font-weight:700>-</span> <span style=color:#0000cf;font-weight:700>1</span><span style=color:#ce5c00;font-weight:700>)</span> <span style=color:#ce5c00;font-weight:700>/</span> <span style=color:#4e9a06>${#</span><span style=color:#000>batch_sizes</span><span style=color:#000;font-weight:700>[@]</span><span style=color:#4e9a06>}</span> <span style=color:#204a87;font-weight:700>))</span>
</span></span><span style=display:flex><span><span style=color:#000>bs_idx</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>$((</span> <span style=color:#ce5c00;font-weight:700>(</span><span style=color:#000>$SLURM_ARRAY_TASK_ID</span> <span style=color:#ce5c00;font-weight:700>-</span> <span style=color:#0000cf;font-weight:700>1</span><span style=color:#ce5c00;font-weight:700>)</span> <span style=color:#ce5c00;font-weight:700>%</span> <span style=color:#4e9a06>${#</span><span style=color:#000>batch_sizes</span><span style=color:#000;font-weight:700>[@]</span><span style=color:#4e9a06>}</span> <span style=color:#204a87;font-weight:700>))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#000>lr</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>${</span><span style=color:#000>learning_rates</span><span style=color:#000;font-weight:700>[</span><span style=color:#000>$lr_idx</span><span style=color:#000;font-weight:700>]</span><span style=color:#4e9a06>}</span>
</span></span><span style=display:flex><span><span style=color:#000>bs</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>${</span><span style=color:#000>batch_sizes</span><span style=color:#000;font-weight:700>[</span><span style=color:#000>$bs_idx</span><span style=color:#000;font-weight:700>]</span><span style=color:#4e9a06>}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load environment</span>
</span></span><span style=display:flex><span>module load Python/3.9.5
</span></span><span style=display:flex><span>conda activate myenv
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Run experiment</span>
</span></span><span style=display:flex><span>python train.py <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    --learning-rate <span style=color:#000>$lr</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    --batch-size <span style=color:#000>$bs</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    --experiment-name <span style=color:#4e9a06>&#34;sweep_</span><span style=color:#4e9a06>${</span><span style=color:#000>SLURM_ARRAY_TASK_ID</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>&#34;</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    --output-dir /lustreFS/data/mygroup/sweep_results
</span></span></code></pre></div><h3 id=a6000-high-memory-job>A6000 High-Memory Job<a class=td-heading-self-link href=#a6000-high-memory-job aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span><span style=color:#8f5902;font-style:italic>#SBATCH -J large_model_training</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --partition=a6000</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --qos=long-a6000</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --ntasks=1</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --cpus-per-task=16</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:3        # Use 3 of 4 A6000 GPUs</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --mem=256000        # 256GB RAM</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=5-00:00:00   # 5 days</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH -o logs/large_model_%j.out</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH -e logs/large_model_%j.err</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load modules</span>
</span></span><span style=display:flex><span>module load CUDA/11.3.1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Activate environment with large model libraries</span>
</span></span><span style=display:flex><span>conda activate large-models
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Set memory optimization</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>CUDA_VISIBLE_DEVICES</span><span style=color:#ce5c00;font-weight:700>=</span>0,1,2
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>PYTORCH_CUDA_ALLOC_CONF</span><span style=color:#ce5c00;font-weight:700>=</span>max_split_size_mb:512
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Train large language model</span>
</span></span><span style=display:flex><span>python train_llm.py <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    --model-size 70B <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    --gradient-checkpointing <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    --fp16 <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    --data-dir /lustreFS/data/mygroup/datasets <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    --output-dir /lustreFS/data/mygroup/llm_checkpoints
</span></span></code></pre></div><h2 id=job-management-commands>Job Management Commands<a class=td-heading-self-link href=#job-management-commands aria-label="Heading self-link"></a></h2><h3 id=monitoring-jobs>Monitoring Jobs<a class=td-heading-self-link href=#monitoring-jobs aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check job queue</span>
</span></span><span style=display:flex><span>squeue
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check your jobs only</span>
</span></span><span style=display:flex><span>squeue -u <span style=color:#000>$USER</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Detailed job information</span>
</span></span><span style=display:flex><span>scontrol show job &lt;job_id&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Job history</span>
</span></span><span style=display:flex><span>sacct -u <span style=color:#000>$USER</span> --starttime<span style=color:#ce5c00;font-weight:700>=</span>2024-01-01
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Job efficiency statistics</span>
</span></span><span style=display:flex><span>seff &lt;job_id&gt;
</span></span></code></pre></div><h3 id=job-control>Job Control<a class=td-heading-self-link href=#job-control aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Cancel a job</span>
</span></span><span style=display:flex><span>scancel &lt;job_id&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Cancel all your jobs</span>
</span></span><span style=display:flex><span>scancel -u <span style=color:#000>$USER</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Cancel jobs by name</span>
</span></span><span style=display:flex><span>scancel --name<span style=color:#ce5c00;font-weight:700>=</span>training_job
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Hold a job (prevent it from running)</span>
</span></span><span style=display:flex><span>scontrol hold &lt;job_id&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Release a held job</span>
</span></span><span style=display:flex><span>scontrol release &lt;job_id&gt;
</span></span></code></pre></div><h3 id=job-information>Job Information<a class=td-heading-self-link href=#job-information aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Show partition information</span>
</span></span><span style=display:flex><span>sinfo
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Show detailed node information</span>
</span></span><span style=display:flex><span>scontrol show nodes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Show QoS information</span>
</span></span><span style=display:flex><span>sacctmgr show qos
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Show your job priorities</span>
</span></span><span style=display:flex><span>sprio -u <span style=color:#000>$USER</span>
</span></span></code></pre></div><h2 id=resource-monitoring>Resource Monitoring<a class=td-heading-self-link href=#resource-monitoring aria-label="Heading self-link"></a></h2><h3 id=during-job-execution>During Job Execution<a class=td-heading-self-link href=#during-job-execution aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Monitor GPU usage on your job</span>
</span></span><span style=display:flex><span>srun --jobid<span style=color:#ce5c00;font-weight:700>=</span>&lt;job_id&gt; nvidia-smi
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check memory usage</span>
</span></span><span style=display:flex><span>srun --jobid<span style=color:#ce5c00;font-weight:700>=</span>&lt;job_id&gt; free -h
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Monitor CPU usage</span>
</span></span><span style=display:flex><span>srun --jobid<span style=color:#ce5c00;font-weight:700>=</span>&lt;job_id&gt; htop
</span></span></code></pre></div><h3 id=job-performance-analysis>Job Performance Analysis<a class=td-heading-self-link href=#job-performance-analysis aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Job efficiency report</span>
</span></span><span style=display:flex><span>seff &lt;job_id&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Detailed job accounting</span>
</span></span><span style=display:flex><span>sacct -j &lt;job_id&gt; --format<span style=color:#ce5c00;font-weight:700>=</span>JobID,JobName,Partition,Account,AllocCPUS,State,ExitCode,Start,End,Elapsed,MaxRSS,MaxVMSize
</span></span></code></pre></div><h2 id=best-practices>Best Practices<a class=td-heading-self-link href=#best-practices aria-label="Heading self-link"></a></h2><h3 id=resource-allocation>Resource Allocation<a class=td-heading-self-link href=#resource-allocation aria-label="Heading self-link"></a></h3><ol><li><p><strong>Request appropriate resources</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Don&#39;t over-allocate</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --cpus-per-task=4   # Not 32 if you only use 4</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --mem=16000         # Not 500000 if you only need 16GB</span>
</span></span></code></pre></div></li><li><p><strong>Use job arrays for parameter sweeps</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --array=1-100%10    # Limit concurrent jobs</span>
</span></span></code></pre></div></li><li><p><strong>Choose appropriate partitions</strong>:</p><ul><li>Use <code>defq</code> for most workloads</li><li>Use <code>a6000</code> only when you need >24GB GPU memory</li></ul></li></ol><h3 id=data-management>Data Management<a class=td-heading-self-link href=#data-management aria-label="Heading self-link"></a></h3><ol><li><p><strong>Use appropriate storage</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Large datasets and results</span>
</span></span><span style=display:flex><span><span style=color:#204a87>cd</span> /lustreFS/data/mygroup
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Temporary files during job</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>TMPDIR</span><span style=color:#ce5c00;font-weight:700>=</span>/tmp/<span style=color:#000>$SLURM_JOB_ID</span>
</span></span><span style=display:flex><span>mkdir -p <span style=color:#000>$TMPDIR</span>
</span></span></code></pre></div></li><li><p><strong>Clean up after jobs</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Add to end of script</span>
</span></span><span style=display:flex><span>rm -rf /tmp/<span style=color:#000>$SLURM_JOB_ID</span>
</span></span></code></pre></div></li></ol><h3 id=debugging>Debugging<a class=td-heading-self-link href=#debugging aria-label="Heading self-link"></a></h3><ol><li><p><strong>Test interactively first</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>srun -p defq --qos<span style=color:#ce5c00;font-weight:700>=</span>normal --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:1 --time<span style=color:#ce5c00;font-weight:700>=</span>1:00:00 --pty /bin/bash
</span></span></code></pre></div></li><li><p><strong>Use smaller datasets for debugging</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>python train.py --debug --max-samples <span style=color:#0000cf;font-weight:700>1000</span>
</span></span></code></pre></div></li><li><p><strong>Check logs regularly</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>tail -f logs/training_12345.out
</span></span></code></pre></div></li></ol><h2 id=troubleshooting>Troubleshooting<a class=td-heading-self-link href=#troubleshooting aria-label="Heading self-link"></a></h2><h3 id=common-issues>Common Issues<a class=td-heading-self-link href=#common-issues aria-label="Heading self-link"></a></h3><p><strong>Job pending forever</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check why job is pending</span>
</span></span><span style=display:flex><span>squeue -u <span style=color:#000>$USER</span> -t PENDING
</span></span><span style=display:flex><span>scontrol show job &lt;job_id&gt; <span style=color:#000;font-weight:700>|</span> grep Reason
</span></span></code></pre></div><p><strong>Out of memory errors</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Reduce batch size or request more memory</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --mem=64000  # Increase memory</span>
</span></span></code></pre></div><p><strong>CUDA out of memory</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># In your script, add:</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>PYTORCH_CUDA_ALLOC_CONF</span><span style=color:#ce5c00;font-weight:700>=</span>max_split_size_mb:128
</span></span></code></pre></div><p><strong>Job killed by time limit</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Request more time or use checkpointing</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=3-00:00:00</span>
</span></span></code></pre></div><p><strong>Cannot access files</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check file permissions and paths</span>
</span></span><span style=display:flex><span>ls -la /lustreFS/data/mygroup/
</span></span></code></pre></div><h2 id=next-steps>Next Steps<a class=td-heading-self-link href=#next-steps aria-label="Heading self-link"></a></h2><ul><li><strong>Learn about partitions and queues</strong>: <a href=../partitions/>Partitions & Queues</a></li><li><strong>Understand storage systems</strong>: <a href=../storage/>Storage Guide</a></li><li><strong>Set up development environment</strong>: <a href=../modules/>Environment Modules</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-95f6b25b1a0c56fb7099d5ae22ef3d4d>5 - Partitions & Queues</h1><div class=lead>Understanding SLURM partitions and priority queues on Prometheus</div><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>The Prometheus cluster has <strong>two partitions</strong> with different priority queues (QoS) that control resource limits and scheduling priority. All limits are applied <strong>per group</strong>, and the default time limit is <strong>4 hours</strong> for all partitions.</p><h2 id=partition-architecture>Partition Architecture<a class=td-heading-self-link href=#partition-architecture aria-label="Heading self-link"></a></h2><h3 id=defq-partition-default><code>defq</code> Partition (Default)<a class=td-heading-self-link href=#defq-partition-default aria-label="Heading self-link"></a></h3><ul><li><strong>Nodes</strong>: 8 compute nodes (<code>gpu[01-08]</code>)</li><li><strong>GPU Type</strong>: NVIDIA A5000 (24GB VRAM each)</li><li><strong>Total GPUs</strong>: 64 (8 GPUs per node)</li><li><strong>Default partition</strong>: Jobs submitted without specifying partition go here</li></ul><h3 id=a6000-partition><code>a6000</code> Partition<a class=td-heading-self-link href=#a6000-partition aria-label="Heading self-link"></a></h3><ul><li><strong>Nodes</strong>: 1 compute node (<code>gpu09</code>)</li><li><strong>GPU Type</strong>: NVIDIA RTX A6000 Ada Generation (48GB VRAM each)</li><li><strong>Total GPUs</strong>: 4</li><li><strong>Use case</strong>: High-memory GPU workloads</li></ul><h2 id=priority-queues-qos>Priority Queues (QoS)<a class=td-heading-self-link href=#priority-queues-qos aria-label="Heading self-link"></a></h2><div class="alert alert-info" role=alert><h4 class=alert-heading>Resource Limits</h4>All resource limits are applied <strong>per group</strong>, not per user. Coordinate with your group members to avoid conflicts.</div><h3 id=defq-partition-queues><code>defq</code> Partition Queues<a class=td-heading-self-link href=#defq-partition-queues aria-label="Heading self-link"></a></h3><table class="table table-striped"><thead><tr><th>Priority Queue</th><th>Time Limit</th><th>Max CPUs</th><th>Max GPUs</th><th>Max RAM</th><th>Max Jobs</th><th>Priority</th></tr></thead><tbody><tr><td><code>normal</code></td><td>1 day</td><td>384</td><td>48</td><td>3TB</td><td>30</td><td>High</td></tr><tr><td><code>long</code></td><td>7 days</td><td>384</td><td>48</td><td>3TB</td><td>20</td><td>Medium</td></tr><tr><td><code>preemptive</code></td><td>Infinite</td><td>All*</td><td>All*</td><td>All*</td><td>10</td><td>Low</td></tr></tbody></table><h3 id=a6000-partition-queues><code>a6000</code> Partition Queues<a class=td-heading-self-link href=#a6000-partition-queues aria-label="Heading self-link"></a></h3><table class="table table-striped"><thead><tr><th>Priority Queue</th><th>Time Limit</th><th>Max CPUs</th><th>Max GPUs</th><th>Max RAM</th><th>Max Jobs</th><th>Priority</th></tr></thead><tbody><tr><td><code>normal-a6000</code></td><td>1 day</td><td>48</td><td>3</td><td>384GB</td><td>6</td><td>High</td></tr><tr><td><code>long-a6000</code></td><td>7 days</td><td>48</td><td>3</td><td>384GB</td><td>4</td><td>Medium</td></tr><tr><td><code>preemptive-a6000</code></td><td>Infinite</td><td>All*</td><td>All*</td><td>All*</td><td>2</td><td>Low</td></tr></tbody></table><p>* <strong>Preemptive queues</strong> can use all available resources but jobs may be automatically terminated when higher-priority jobs need resources.</p><h2 id=queue-selection-guidelines>Queue Selection Guidelines<a class=td-heading-self-link href=#queue-selection-guidelines aria-label="Heading self-link"></a></h2><h3 id=use-normal-or-normal-a6000-for>Use <code>normal</code> or <code>normal-a6000</code> for:<a class=td-heading-self-link href=#use-normal-or-normal-a6000-for aria-label="Heading self-link"></a></h3><ul><li><strong>Interactive development</strong> and testing</li><li><strong>Short training runs</strong> (&lt; 24 hours)</li><li><strong>Production jobs</strong> that need guaranteed completion</li><li><strong>Debugging</strong> and experimentation</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --partition=defq</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --qos=normal</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=12:00:00</span>
</span></span></code></pre></div><h3 id=use-long-or-long-a6000-for>Use <code>long</code> or <code>long-a6000</code> for:<a class=td-heading-self-link href=#use-long-or-long-a6000-for aria-label="Heading self-link"></a></h3><ul><li><strong>Extended training</strong> (1-7 days)</li><li><strong>Large model training</strong> requiring multiple days</li><li><strong>Parameter sweeps</strong> with many iterations</li><li><strong>Production workloads</strong> with longer time requirements</li></ul><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --partition=defq</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --qos=long</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=3-00:00:00  # 3 days</span>
</span></span></code></pre></div><h3 id=use-preemptive-queues-sparingly-for>Use <code>preemptive</code> queues sparingly for:<a class=td-heading-self-link href=#use-preemptive-queues-sparingly-for aria-label="Heading self-link"></a></h3><ul><li><strong>Low-priority background jobs</strong></li><li><strong>Opportunistic computing</strong> when cluster is idle</li><li><strong>Jobs that can handle interruption</strong> (with checkpointing)</li><li><strong>Testing with unlimited time</strong></li></ul><div class="alert alert-warning" role=alert><h4 class=alert-heading>Warning</h4>Preemptive jobs can be <strong>automatically terminated</strong> at any time! Use the <code>requeue</code> option and implement checkpointing.</div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --partition=defq</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --qos=preemptive</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --requeue          # Automatically resubmit if preempted</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=7-00:00:00</span>
</span></span></code></pre></div><h2 id=choosing-the-right-partition>Choosing the Right Partition<a class=td-heading-self-link href=#choosing-the-right-partition aria-label="Heading self-link"></a></h2><h3 id=use-defq-partition-when>Use <code>defq</code> partition when:<a class=td-heading-self-link href=#use-defq-partition-when aria-label="Heading self-link"></a></h3><ul><li>Your models fit in <strong>24GB GPU memory</strong></li><li>You need <strong>multiple GPUs</strong> (up to 8 per node)</li><li>Running <strong>distributed training</strong> across nodes</li><li>Working with <strong>standard deep learning models</strong></li></ul><h3 id=use-a6000-partition-when>Use <code>a6000</code> partition when:<a class=td-heading-self-link href=#use-a6000-partition-when aria-label="Heading self-link"></a></h3><ul><li>Your models require <strong>> 24GB GPU memory</strong></li><li>Training <strong>large language models</strong> (70B+ parameters)</li><li>Working with <strong>high-resolution images</strong> or <strong>long sequences</strong></li><li>Need <strong>maximum GPU memory</strong> per device</li></ul><h2 id=example-job-submissions>Example Job Submissions<a class=td-heading-self-link href=#example-job-submissions aria-label="Heading self-link"></a></h2><h3 id=standard-training-job-defqnormal>Standard Training Job (defq/normal)<a class=td-heading-self-link href=#standard-training-job-defqnormal aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span><span style=color:#8f5902;font-style:italic>#SBATCH -J standard_training</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --partition=defq</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --qos=normal</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --cpus-per-task=8</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:2</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --mem=64000</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=18:00:00</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Your training code here</span>
</span></span><span style=display:flex><span>python train_model.py --gpus <span style=color:#0000cf;font-weight:700>2</span>
</span></span></code></pre></div><h3 id=long-training-job-defqlong>Long Training Job (defq/long)<a class=td-heading-self-link href=#long-training-job-defqlong aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span><span style=color:#8f5902;font-style:italic>#SBATCH -J long_training</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --partition=defq</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --qos=long</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --cpus-per-task=16</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:4</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --mem=128000</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=5-00:00:00  # 5 days</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Long-running training with checkpointing</span>
</span></span><span style=display:flex><span>python train_model.py --gpus <span style=color:#0000cf;font-weight:700>4</span> --checkpoint-freq <span style=color:#0000cf;font-weight:700>1000</span>
</span></span></code></pre></div><h3 id=large-model-training-a6000normal-a6000>Large Model Training (a6000/normal-a6000)<a class=td-heading-self-link href=#large-model-training-a6000normal-a6000 aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span><span style=color:#8f5902;font-style:italic>#SBATCH -J large_model</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --partition=a6000</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --qos=normal-a6000</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --cpus-per-task=16</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:2</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --mem=256000</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=20:00:00</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Large model requiring high GPU memory</span>
</span></span><span style=display:flex><span>python train_llm.py --model-size 70B --gpus <span style=color:#0000cf;font-weight:700>2</span>
</span></span></code></pre></div><h3 id=preemptive-job-with-checkpointing>Preemptive Job with Checkpointing<a class=td-heading-self-link href=#preemptive-job-with-checkpointing aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span><span style=color:#8f5902;font-style:italic>#SBATCH -J preemptive_job</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --partition=defq</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --qos=preemptive</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --cpus-per-task=4</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:1</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --mem=32000</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=7-00:00:00</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --requeue</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --signal=SIGUSR1@90  # Signal 90 seconds before termination</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Handle preemption gracefully</span>
</span></span><span style=display:flex><span><span style=color:#204a87>trap</span> <span style=color:#4e9a06>&#39;echo &#34;Job preempted, saving checkpoint...&#34;; python save_checkpoint.py&#39;</span> SIGUSR1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>python train_model.py --resume-from-checkpoint
</span></span></code></pre></div><h2 id=monitoring-queue-status>Monitoring Queue Status<a class=td-heading-self-link href=#monitoring-queue-status aria-label="Heading self-link"></a></h2><h3 id=check-partition-information>Check Partition Information<a class=td-heading-self-link href=#check-partition-information aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># View all partitions and their status</span>
</span></span><span style=display:flex><span>sinfo
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Example output:</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># PARTITION AVAIL  TIMELIMIT  NODES  STATE NODELIST</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># defq*        up   infinite      6  idle~ gpu[03-08]</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># defq*        up   infinite      1    mix gpu02</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># defq*        up   infinite      1   idle gpu01</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># a6000        up   infinite      1    mix gpu09</span>
</span></span></code></pre></div><h3 id=check-queue-status>Check Queue Status<a class=td-heading-self-link href=#check-queue-status aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># View current job queue</span>
</span></span><span style=display:flex><span>squeue
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># View jobs by partition</span>
</span></span><span style=display:flex><span>squeue -p defq
</span></span><span style=display:flex><span>squeue -p a6000
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># View jobs by QoS</span>
</span></span><span style=display:flex><span>squeue --qos<span style=color:#ce5c00;font-weight:700>=</span>normal
</span></span><span style=display:flex><span>squeue --qos<span style=color:#ce5c00;font-weight:700>=</span>long
</span></span></code></pre></div><h3 id=check-your-resource-usage>Check Your Resource Usage<a class=td-heading-self-link href=#check-your-resource-usage aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># View your running jobs</span>
</span></span><span style=display:flex><span>squeue -u <span style=color:#000>$USER</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check job priorities</span>
</span></span><span style=display:flex><span>sprio -u <span style=color:#000>$USER</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># View resource limits</span>
</span></span><span style=display:flex><span>sacctmgr show qos <span style=color:#000>format</span><span style=color:#ce5c00;font-weight:700>=</span>Name,MaxWall,MaxTRES,MaxJobs
</span></span></code></pre></div><h2 id=resource-planning>Resource Planning<a class=td-heading-self-link href=#resource-planning aria-label="Heading self-link"></a></h2><h3 id=calculate-resource-needs>Calculate Resource Needs<a class=td-heading-self-link href=#calculate-resource-needs aria-label="Heading self-link"></a></h3><p>Before submitting jobs, consider:</p><ol><li><p><strong>GPU Memory Requirements</strong>:</p><ul><li>Small models (&lt; 1B params): A5000 (24GB)</li><li>Large models (> 10B params): A6000 (48GB)</li></ul></li><li><p><strong>Training Time Estimates</strong>:</p><ul><li>Quick experiments: <code>normal</code> queue (&lt; 1 day)</li><li>Full training: <code>long</code> queue (1-7 days)</li></ul></li><li><p><strong>Number of GPUs</strong>:</p><ul><li>Single GPU: Any node</li><li>Multi-GPU: Consider node topology</li><li>Distributed: Multiple nodes in <code>defq</code></li></ul></li></ol><h3 id=group-coordination>Group Coordination<a class=td-heading-self-link href=#group-coordination aria-label="Heading self-link"></a></h3><p>Since limits are per group:</p><ol><li><strong>Communicate with group members</strong></li><li><strong>Check current group usage</strong>:<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>squeue -A your-group-name
</span></span></code></pre></div></li><li><strong>Plan resource allocation</strong> to avoid conflicts</li></ol><h2 id=best-practices>Best Practices<a class=td-heading-self-link href=#best-practices aria-label="Heading self-link"></a></h2><h3 id=queue-selection-strategy>Queue Selection Strategy<a class=td-heading-self-link href=#queue-selection-strategy aria-label="Heading self-link"></a></h3><ol><li><strong>Start with <code>normal</code> queues</strong> for development</li><li><strong>Use <code>long</code> queues</strong> only when necessary</li><li><strong>Avoid preemptive queues</strong> unless jobs can handle interruption</li><li><strong>Test on smaller resources</strong> before scaling up</li></ol><h3 id=resource-efficiency>Resource Efficiency<a class=td-heading-self-link href=#resource-efficiency aria-label="Heading self-link"></a></h3><ol><li><p><strong>Don&rsquo;t over-allocate resources</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Bad: Requesting 8 GPUs for single-GPU code</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:8</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Good: Request what you actually use</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:1</span>
</span></span></code></pre></div></li><li><p><strong>Use appropriate memory</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Calculate actual memory needs</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --mem=32000  # 32GB, not 500GB</span>
</span></span></code></pre></div></li><li><p><strong>Estimate time accurately</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Add buffer but don&#39;t overestimate</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=18:00:00  # 18 hours, not 7 days</span>
</span></span></code></pre></div></li></ol><h2 id=troubleshooting>Troubleshooting<a class=td-heading-self-link href=#troubleshooting aria-label="Heading self-link"></a></h2><h3 id=job-stuck-in-queue>Job Stuck in Queue<a class=td-heading-self-link href=#job-stuck-in-queue aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check why job is pending</span>
</span></span><span style=display:flex><span>scontrol show job &lt;job_id&gt; <span style=color:#000;font-weight:700>|</span> grep Reason
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Common reasons:</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># - Resources: Requesting more than available</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># - Priority: Lower priority than other jobs</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># - QoSMaxJobsPerUser: Too many jobs running</span>
</span></span></code></pre></div><h3 id=resource-limit-exceeded>Resource Limit Exceeded<a class=td-heading-self-link href=#resource-limit-exceeded aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check current group usage</span>
</span></span><span style=display:flex><span>squeue -A your-group
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Reduce resource requests or wait for jobs to complete</span>
</span></span></code></pre></div><h3 id=wrong-partition-choice>Wrong Partition Choice<a class=td-heading-self-link href=#wrong-partition-choice aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Cancel and resubmit with correct partition</span>
</span></span><span style=display:flex><span>scancel &lt;job_id&gt;
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Edit script and resubmit</span>
</span></span><span style=display:flex><span>sbatch corrected_script.slurm
</span></span></code></pre></div><h2 id=next-steps>Next Steps<a class=td-heading-self-link href=#next-steps aria-label="Heading self-link"></a></h2><ul><li><strong>Learn about storage systems</strong>: <a href=../storage/>Storage Guide</a></li><li><strong>Set up your environment</strong>: <a href=../modules/>Environment Modules</a></li><li><strong>Configure VS Code</strong>: <a href=../vscode/>VS Code Setup</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-23ce6f3804b08b263e06e0c152bf4ee5>6 - Storage Systems</h1><div class=lead>File systems, quotas, and data management on the Prometheus cluster</div><h2 id=storage-overview>Storage Overview<a class=td-heading-self-link href=#storage-overview aria-label="Heading self-link"></a></h2><p>The Prometheus cluster provides multiple storage systems optimized for different use cases, from personal files to high-performance parallel computing workloads.</p><h2 id=storage-architecture>Storage Architecture<a class=td-heading-self-link href=#storage-architecture aria-label="Heading self-link"></a></h2><h3 id=home-directories-trinityhome>Home Directories (<code>/trinity/home/</code>)<a class=td-heading-self-link href=#home-directories-trinityhome aria-label="Heading self-link"></a></h3><ul><li><strong>Type</strong>: SSD-backed storage</li><li><strong>Mount point</strong>: <code>/trinity/home/&lt;username></code></li><li><strong>Quota</strong>: 20GB per user</li><li><strong>Purpose</strong>: Personal configuration files, small scripts</li><li><strong>Backup</strong>: Regular backups maintained</li><li><strong>Performance</strong>: Fast random I/O, moderate capacity</li></ul><h3 id=shared-group-storage-lustrefsdata>Shared Group Storage (<code>/lustreFS/data/</code>)<a class=td-heading-self-link href=#shared-group-storage-lustrefsdata aria-label="Heading self-link"></a></h3><ul><li><strong>Type</strong>: Lustre parallel file system</li><li><strong>Mount point</strong>: <code>/lustreFS/data/&lt;group-name></code></li><li><strong>Quota</strong>: 30TB per group (or 20,971,520 files)</li><li><strong>Purpose</strong>: Primary workspace for research data and results</li><li><strong>Performance</strong>: High-throughput parallel I/O</li><li><strong>Shared</strong>: All group members have access</li></ul><h3 id=local-node-storage>Local Node Storage<a class=td-heading-self-link href=#local-node-storage aria-label="Heading self-link"></a></h3><ul><li><strong>Type</strong>: NVMe SSD (1TB per compute node)</li><li><strong>Purpose</strong>: Temporary files during job execution</li><li><strong>Access</strong>: Only available during allocated jobs</li><li><strong>Performance</strong>: Highest IOPS for temporary data</li></ul><h2 id=file-system-details>File System Details<a class=td-heading-self-link href=#file-system-details aria-label="Heading self-link"></a></h2><h3 id=home-directory-usage>Home Directory Usage<a class=td-heading-self-link href=#home-directory-usage aria-label="Heading self-link"></a></h3><div class="alert alert-warning" role=alert><h4 class=alert-heading>Quota Limit</h4>Home directories have a strict <strong>20GB limit</strong>. Use them only for configuration files, not data or models.</div><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check your home directory quota</span>
</span></span><span style=display:flex><span>quota -us
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># View home directory contents</span>
</span></span><span style=display:flex><span>ls -la /trinity/home/<span style=color:#000>$USER</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Typical home directory structure</span>
</span></span><span style=display:flex><span>/trinity/home/username/
</span></span><span style=display:flex><span>├── .bashrc                 <span style=color:#8f5902;font-style:italic># Shell configuration</span>
</span></span><span style=display:flex><span>├── .ssh/                   <span style=color:#8f5902;font-style:italic># SSH keys and config</span>
</span></span><span style=display:flex><span>├── .jupyter/               <span style=color:#8f5902;font-style:italic># Jupyter configuration</span>
</span></span><span style=display:flex><span>├── .conda/                 <span style=color:#8f5902;font-style:italic># Conda configuration</span>
</span></span><span style=display:flex><span>├── scripts/                <span style=color:#8f5902;font-style:italic># Small utility scripts</span>
</span></span><span style=display:flex><span>└── .local/                 <span style=color:#8f5902;font-style:italic># Local Python packages</span>
</span></span></code></pre></div><p><strong>Best practices for home directories</strong>:</p><ul><li>Store only configuration files and small scripts</li><li>Link to shared storage for data access</li><li>Use symbolic links to avoid quota issues</li></ul><h3 id=shared-group-storage>Shared Group Storage<a class=td-heading-self-link href=#shared-group-storage aria-label="Heading self-link"></a></h3><p>The <code>/lustreFS/data/</code> directory provides high-performance storage for your research work:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Access your group&#39;s shared storage</span>
</span></span><span style=display:flex><span><span style=color:#204a87>cd</span> /lustreFS/data/&lt;group-name&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check group quota</span>
</span></span><span style=display:flex><span>lfs quota -gh &lt;group-name&gt; /lustreFS/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Example group directory structure</span>
</span></span><span style=display:flex><span>/lustreFS/data/mygroup/
</span></span><span style=display:flex><span>├── datasets/               <span style=color:#8f5902;font-style:italic># Shared datasets</span>
</span></span><span style=display:flex><span>├── models/                 <span style=color:#8f5902;font-style:italic># Pre-trained and trained models</span>
</span></span><span style=display:flex><span>├── experiments/            <span style=color:#8f5902;font-style:italic># Individual user experiments</span>
</span></span><span style=display:flex><span>│   ├── user1/
</span></span><span style=display:flex><span>│   ├── user2/
</span></span><span style=display:flex><span>│   └── shared/
</span></span><span style=display:flex><span>├── code/                   <span style=color:#8f5902;font-style:italic># Shared code repositories</span>
</span></span><span style=display:flex><span>├── results/                <span style=color:#8f5902;font-style:italic># Experiment results</span>
</span></span><span style=display:flex><span>└── tmp/                    <span style=color:#8f5902;font-style:italic># Temporary files</span>
</span></span></code></pre></div><p><strong>Quota information</strong>:</p><ul><li><strong>Space limit</strong>: 30TB per group</li><li><strong>File limit</strong>: 20,971,520 files per group</li><li><strong>Shared</strong>: All group members can read/write</li></ul><h3 id=local-node-storage-1>Local Node Storage<a class=td-heading-self-link href=#local-node-storage-1 aria-label="Heading self-link"></a></h3><p>Each compute node has local NVMe storage for temporary files:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># During a SLURM job, use local storage for temporary files</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>TMPDIR</span><span style=color:#ce5c00;font-weight:700>=</span>/tmp/<span style=color:#000>$SLURM_JOB_ID</span>
</span></span><span style=display:flex><span>mkdir -p <span style=color:#000>$TMPDIR</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Example usage in job script</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --job-name=training</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Create temporary directory</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>TMPDIR</span><span style=color:#ce5c00;font-weight:700>=</span>/tmp/<span style=color:#000>$SLURM_JOB_ID</span>
</span></span><span style=display:flex><span>mkdir -p <span style=color:#000>$TMPDIR</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Copy data to local storage for faster I/O</span>
</span></span><span style=display:flex><span>cp /lustreFS/data/mygroup/dataset.tar.gz <span style=color:#000>$TMPDIR</span>/
</span></span><span style=display:flex><span><span style=color:#204a87>cd</span> <span style=color:#000>$TMPDIR</span>
</span></span><span style=display:flex><span>tar -xzf dataset.tar.gz
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Run training with local data</span>
</span></span><span style=display:flex><span>python train.py --data-dir <span style=color:#000>$TMPDIR</span>/dataset
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Copy results back to shared storage</span>
</span></span><span style=display:flex><span>cp -r <span style=color:#000>$TMPDIR</span>/results /lustreFS/data/mygroup/experiments/
</span></span></code></pre></div><h2 id=quota-management>Quota Management<a class=td-heading-self-link href=#quota-management aria-label="Heading self-link"></a></h2><h3 id=checking-quotas>Checking Quotas<a class=td-heading-self-link href=#checking-quotas aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check your home directory quota</span>
</span></span><span style=display:flex><span>quota -us
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check group quota on Lustre filesystem</span>
</span></span><span style=display:flex><span>lfs quota -gh &lt;group-name&gt; /lustreFS/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Example quota output:</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Disk quotas for group mygroup (gid 1001):</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#      Filesystem    used   quota   limit   grace   files   quota   limit   grace</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#        /lustreFS   15.2T     30T     30T       -  1234567  20971520 20971520   -</span>
</span></span></code></pre></div><h3 id=understanding-quota-output>Understanding Quota Output<a class=td-heading-self-link href=#understanding-quota-output aria-label="Heading self-link"></a></h3><ul><li><strong>used</strong>: Current usage</li><li><strong>quota</strong>: Soft limit (warning threshold)</li><li><strong>limit</strong>: Hard limit (cannot exceed)</li><li><strong>grace</strong>: Time allowed to exceed soft quota</li><li><strong>files</strong>: Number of files/inodes used</li></ul><h3 id=managing-quota-issues>Managing Quota Issues<a class=td-heading-self-link href=#managing-quota-issues aria-label="Heading self-link"></a></h3><p>When approaching quota limits:</p><ol><li><p><strong>Clean up temporary files</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Find large files</span>
</span></span><span style=display:flex><span>find /lustreFS/data/mygroup -type f -size +1G -ls
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Find old temporary files</span>
</span></span><span style=display:flex><span>find /lustreFS/data/mygroup -name <span style=color:#4e9a06>&#34;*.tmp&#34;</span> -mtime +7 -delete
</span></span></code></pre></div></li><li><p><strong>Archive old data</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Compress old experiments</span>
</span></span><span style=display:flex><span>tar -czf old_experiments.tar.gz experiments/2023/
</span></span><span style=display:flex><span>rm -rf experiments/2023/
</span></span></code></pre></div></li><li><p><strong>Use efficient storage</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Use compressed formats for datasets</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Store checkpoints selectively</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Remove duplicate files</span>
</span></span></code></pre></div></li></ol><h2 id=data-management-best-practices>Data Management Best Practices<a class=td-heading-self-link href=#data-management-best-practices aria-label="Heading self-link"></a></h2><h3 id=directory-organization>Directory Organization<a class=td-heading-self-link href=#directory-organization aria-label="Heading self-link"></a></h3><p>Organize your group&rsquo;s shared storage efficiently:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Recommended structure</span>
</span></span><span style=display:flex><span>/lustreFS/data/mygroup/
</span></span><span style=display:flex><span>├── datasets/
</span></span><span style=display:flex><span>│   ├── imagenet/           <span style=color:#8f5902;font-style:italic># Large shared datasets</span>
</span></span><span style=display:flex><span>│   ├── coco/
</span></span><span style=display:flex><span>│   └── custom/
</span></span><span style=display:flex><span>├── models/
</span></span><span style=display:flex><span>│   ├── pretrained/         <span style=color:#8f5902;font-style:italic># Downloaded pre-trained models</span>
</span></span><span style=display:flex><span>│   └── checkpoints/        <span style=color:#8f5902;font-style:italic># Training checkpoints</span>
</span></span><span style=display:flex><span>├── experiments/
</span></span><span style=display:flex><span>│   ├── user1/
</span></span><span style=display:flex><span>│   │   ├── project_a/
</span></span><span style=display:flex><span>│   │   └── project_b/
</span></span><span style=display:flex><span>│   └── user2/
</span></span><span style=display:flex><span>├── code/
</span></span><span style=display:flex><span>│   ├── shared_utils/       <span style=color:#8f5902;font-style:italic># Shared code libraries</span>
</span></span><span style=display:flex><span>│   └── experiments/        <span style=color:#8f5902;font-style:italic># Experiment code</span>
</span></span><span style=display:flex><span>└── results/
</span></span><span style=display:flex><span>    ├── papers/             <span style=color:#8f5902;font-style:italic># Results for publications</span>
</span></span><span style=display:flex><span>    └── ongoing/            <span style=color:#8f5902;font-style:italic># Current experiment results</span>
</span></span></code></pre></div><h3 id=file-permissions>File Permissions<a class=td-heading-self-link href=#file-permissions aria-label="Heading self-link"></a></h3><p>Set appropriate permissions for shared access:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Make directories group-writable</span>
</span></span><span style=display:flex><span>chmod g+w /lustreFS/data/mygroup/datasets/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Set default permissions for new files</span>
</span></span><span style=display:flex><span><span style=color:#204a87>umask</span> <span style=color:#0000cf;font-weight:700>002</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Change group ownership if needed</span>
</span></span><span style=display:flex><span>chgrp -R mygroup /lustreFS/data/mygroup/shared/
</span></span></code></pre></div><h3 id=data-transfer>Data Transfer<a class=td-heading-self-link href=#data-transfer aria-label="Heading self-link"></a></h3><h4 id=small-files--1gb>Small Files (&lt; 1GB)<a class=td-heading-self-link href=#small-files--1gb aria-label="Heading self-link"></a></h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Copy from local machine using scp</span>
</span></span><span style=display:flex><span>scp large_dataset.tar.gz prometheus:/lustreFS/data/mygroup/datasets/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Copy between directories on cluster</span>
</span></span><span style=display:flex><span>cp -r /lustreFS/data/mygroup/datasets/source /lustreFS/data/mygroup/experiments/
</span></span></code></pre></div><h4 id=large-files--1gb>Large Files (> 1GB)<a class=td-heading-self-link href=#large-files--1gb aria-label="Heading self-link"></a></h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Use rsync for large transfers with progress</span>
</span></span><span style=display:flex><span>rsync -avP large_dataset/ prometheus:/lustreFS/data/mygroup/datasets/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Parallel compression for large datasets</span>
</span></span><span style=display:flex><span>tar -cf - dataset/ <span style=color:#000;font-weight:700>|</span> pigz &gt; dataset.tar.gz
</span></span></code></pre></div><h4 id=download-datasets>Download Datasets<a class=td-heading-self-link href=#download-datasets aria-label="Heading self-link"></a></h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Download directly to shared storage</span>
</span></span><span style=display:flex><span><span style=color:#204a87>cd</span> /lustreFS/data/mygroup/datasets/
</span></span><span style=display:flex><span>wget https://example.com/large_dataset.tar.gz
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Use aria2 for faster parallel downloads</span>
</span></span><span style=display:flex><span>aria2c -x <span style=color:#0000cf;font-weight:700>8</span> -s <span style=color:#0000cf;font-weight:700>8</span> https://example.com/dataset.tar.gz
</span></span></code></pre></div><h3 id=backup-strategies>Backup Strategies<a class=td-heading-self-link href=#backup-strategies aria-label="Heading self-link"></a></h3><p>While the cluster provides reliable storage, implement your own backup strategy:</p><ol><li><strong>Important results</strong>: Copy to external storage</li><li><strong>Code</strong>: Use git repositories</li><li><strong>Large datasets</strong>: Document download sources for re-acquisition</li><li><strong>Models</strong>: Keep important checkpoints on external storage</li></ol><h2 id=performance-optimization>Performance Optimization<a class=td-heading-self-link href=#performance-optimization aria-label="Heading self-link"></a></h2><h3 id=lustre-file-system-tips>Lustre File System Tips<a class=td-heading-self-link href=#lustre-file-system-tips aria-label="Heading self-link"></a></h3><ol><li><p><strong>Use parallel I/O</strong> for large files:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#8f5902;font-style:italic># PyTorch DataLoader with multiple workers</span>
</span></span><span style=display:flex><span><span style=color:#000>dataloader</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>DataLoader</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>dataset</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>batch_size</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>64</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>num_workers</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>8</span><span style=color:#000;font-weight:700>)</span>
</span></span></code></pre></div></li><li><p><strong>Avoid small random writes</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Bad: Many small writes</span>
</span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>for</span> i in <span style=color:#ce5c00;font-weight:700>{</span>1..1000<span style=color:#ce5c00;font-weight:700>}</span><span style=color:#000;font-weight:700>;</span> <span style=color:#204a87;font-weight:700>do</span> <span style=color:#204a87>echo</span> <span style=color:#000>$i</span> &gt;&gt; file.txt<span style=color:#000;font-weight:700>;</span> <span style=color:#204a87;font-weight:700>done</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Good: Batch writes</span>
</span></span><span style=display:flex><span>seq <span style=color:#0000cf;font-weight:700>1</span> <span style=color:#0000cf;font-weight:700>1000</span> &gt; file.txt
</span></span></code></pre></div></li><li><p><strong>Use appropriate stripe settings</strong> for large files:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Set stripe count for large files (&gt; 1GB)</span>
</span></span><span style=display:flex><span>lfs setstripe -c <span style=color:#0000cf;font-weight:700>4</span> /lustreFS/data/mygroup/large_dataset/
</span></span></code></pre></div></li></ol><h3 id=local-storage-performance>Local Storage Performance<a class=td-heading-self-link href=#local-storage-performance aria-label="Heading self-link"></a></h3><ol><li><strong>Copy frequently accessed data</strong> to local storage during jobs</li><li><strong>Use local storage for temporary files</strong> and intermediate results</li><li><strong>Copy final results back</strong> to shared storage</li></ol><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Example job with local storage optimization</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#!/bin/bash</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:1</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=4:00:00</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Set up local temporary directory</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>TMPDIR</span><span style=color:#ce5c00;font-weight:700>=</span>/tmp/<span style=color:#000>$SLURM_JOB_ID</span>
</span></span><span style=display:flex><span>mkdir -p <span style=color:#000>$TMPDIR</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Copy dataset to local storage</span>
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#34;Copying dataset to local storage...&#34;</span>
</span></span><span style=display:flex><span>cp /lustreFS/data/mygroup/dataset.tar.gz <span style=color:#000>$TMPDIR</span>/
</span></span><span style=display:flex><span><span style=color:#204a87>cd</span> <span style=color:#000>$TMPDIR</span>
</span></span><span style=display:flex><span>tar -xzf dataset.tar.gz
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Run training with local data (much faster I/O)</span>
</span></span><span style=display:flex><span>python train.py --data-dir <span style=color:#000>$TMPDIR</span>/dataset --output-dir <span style=color:#000>$TMPDIR</span>/results
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Copy results back to shared storage</span>
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#34;Copying results back...&#34;</span>
</span></span><span style=display:flex><span>cp -r <span style=color:#000>$TMPDIR</span>/results /lustreFS/data/mygroup/experiments/
</span></span></code></pre></div><h2 id=environment-variables>Environment Variables<a class=td-heading-self-link href=#environment-variables aria-label="Heading self-link"></a></h2><p>Set up useful environment variables for data management:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Add to your ~/.bashrc</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>GROUP_DATA</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;/lustreFS/data/mygroup&#34;</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>DATASETS</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;</span><span style=color:#000>$GROUP_DATA</span><span style=color:#4e9a06>/datasets&#34;</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>MODELS</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;</span><span style=color:#000>$GROUP_DATA</span><span style=color:#4e9a06>/models&#34;</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>EXPERIMENTS</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;</span><span style=color:#000>$GROUP_DATA</span><span style=color:#4e9a06>/experiments/</span><span style=color:#000>$USER</span><span style=color:#4e9a06>&#34;</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>RESULTS</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;</span><span style=color:#000>$GROUP_DATA</span><span style=color:#4e9a06>/results&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Create your experiment directory</span>
</span></span><span style=display:flex><span>mkdir -p <span style=color:#000>$EXPERIMENTS</span>
</span></span></code></pre></div><h2 id=common-storage-issues>Common Storage Issues<a class=td-heading-self-link href=#common-storage-issues aria-label="Heading self-link"></a></h2><h3 id=quota-exceeded>Quota Exceeded<a class=td-heading-self-link href=#quota-exceeded aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Error: &#34;Disk quota exceeded&#34;</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Solution: Check and clean up usage</span>
</span></span><span style=display:flex><span>lfs quota -gh mygroup /lustreFS/
</span></span><span style=display:flex><span>find <span style=color:#000>$GROUP_DATA</span> -type f -size +1G -ls
</span></span></code></pre></div><h3 id=permission-denied>Permission Denied<a class=td-heading-self-link href=#permission-denied aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Error: &#34;Permission denied&#34;</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Solution: Check file permissions and group membership</span>
</span></span><span style=display:flex><span>ls -la /lustreFS/data/mygroup/
</span></span><span style=display:flex><span>groups  <span style=color:#8f5902;font-style:italic># Check your group membership</span>
</span></span></code></pre></div><h3 id=slow-io-performance>Slow I/O Performance<a class=td-heading-self-link href=#slow-io-performance aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Solutions:</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 1. Use local storage for temporary files</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 2. Reduce number of small files</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 3. Use parallel I/O libraries</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 4. Check stripe settings for large files</span>
</span></span><span style=display:flex><span>lfs getstripe /lustreFS/data/mygroup/large_file
</span></span></code></pre></div><h3 id=file-system-full>File System Full<a class=td-heading-self-link href=#file-system-full aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check available space</span>
</span></span><span style=display:flex><span>df -h /lustreFS
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># If file system is full, clean up:</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 1. Remove temporary files</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 2. Compress old data</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># 3. Archive completed experiments</span>
</span></span></code></pre></div><h2 id=next-steps>Next Steps<a class=td-heading-self-link href=#next-steps aria-label="Heading self-link"></a></h2><ul><li><strong>Set up your development environment</strong>: <a href=../modules/>Environment Modules</a></li><li><strong>Configure VS Code for remote development</strong>: <a href=../vscode/>VS Code Setup</a></li><li><strong>Learn about software installation</strong>: <a href=../software/>Software Installation</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-ac02ffa0d48b518cf3420c0f071e98f8>7 - Environment Modules</h1><div class=lead>Using Lmod environment modules to manage software on Prometheus</div><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>The Prometheus cluster uses <strong>Lmod</strong> (Lua-based Environment Modules) to manage software packages and their dependencies. This system allows you to easily load and unload different software versions without conflicts.</p><h2 id=module-system-basics>Module System Basics<a class=td-heading-self-link href=#module-system-basics aria-label="Heading self-link"></a></h2><p>Environment modules modify your shell environment to provide access to specific software packages. When you load a module, it typically:</p><ul><li>Adds software to your <code>PATH</code></li><li>Sets environment variables</li><li>Loads required dependencies</li><li>Configures library paths</li></ul><h2 id=basic-module-commands>Basic Module Commands<a class=td-heading-self-link href=#basic-module-commands aria-label="Heading self-link"></a></h2><h3 id=list-available-modules>List Available Modules<a class=td-heading-self-link href=#list-available-modules aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Show all available modules</span>
</span></span><span style=display:flex><span>module available
</span></span><span style=display:flex><span>module avail
</span></span><span style=display:flex><span>module av
</span></span><span style=display:flex><span>ml av
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Search for specific modules</span>
</span></span><span style=display:flex><span>module avail gcc
</span></span><span style=display:flex><span>module avail python
</span></span><span style=display:flex><span>module avail cuda
</span></span></code></pre></div><h3 id=load-modules>Load Modules<a class=td-heading-self-link href=#load-modules aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load a specific module</span>
</span></span><span style=display:flex><span>module load GCC/10.3.0
</span></span><span style=display:flex><span>module load CUDA/11.3.1
</span></span><span style=display:flex><span>module load Python/3.9.5
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Short form using &#39;ml&#39;</span>
</span></span><span style=display:flex><span>ml GCC/10.3.0 CUDA/11.3.1 Python/3.9.5
</span></span></code></pre></div><h3 id=check-loaded-modules>Check Loaded Modules<a class=td-heading-self-link href=#check-loaded-modules aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># List currently loaded modules</span>
</span></span><span style=display:flex><span>module list
</span></span><span style=display:flex><span>ml list
</span></span></code></pre></div><h3 id=unload-modules>Unload Modules<a class=td-heading-self-link href=#unload-modules aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Unload a specific module</span>
</span></span><span style=display:flex><span>module unload GCC/10.3.0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Unload all modules</span>
</span></span><span style=display:flex><span>module purge
</span></span><span style=display:flex><span>ml purge
</span></span></code></pre></div><h2 id=common-software-modules>Common Software Modules<a class=td-heading-self-link href=#common-software-modules aria-label="Heading self-link"></a></h2><h3 id=compilers>Compilers<a class=td-heading-self-link href=#compilers aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># GNU Compiler Collection</span>
</span></span><span style=display:flex><span>module load GCC/10.3.0
</span></span><span style=display:flex><span>module load GCC/11.2.0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Intel Compilers (if available)</span>
</span></span><span style=display:flex><span>module load intel/2021.4.0
</span></span></code></pre></div><h3 id=cuda-and-gpu-development>CUDA and GPU Development<a class=td-heading-self-link href=#cuda-and-gpu-development aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># CUDA Toolkit</span>
</span></span><span style=display:flex><span>module load CUDA/11.3.1
</span></span><span style=display:flex><span>module load CUDA/11.7.0
</span></span><span style=display:flex><span>module load CUDA/12.0.0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check CUDA after loading</span>
</span></span><span style=display:flex><span>nvcc --version
</span></span><span style=display:flex><span>nvidia-smi
</span></span></code></pre></div><h3 id=python-environments>Python Environments<a class=td-heading-self-link href=#python-environments aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Python interpreter</span>
</span></span><span style=display:flex><span>module load Python/3.9.5
</span></span><span style=display:flex><span>module load Python/3.10.8
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Python with scientific libraries</span>
</span></span><span style=display:flex><span>module load Python/3.9.5-GCCcore-10.3.0
</span></span></code></pre></div><h3 id=deep-learning-frameworks>Deep Learning Frameworks<a class=td-heading-self-link href=#deep-learning-frameworks aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># PyTorch (if pre-installed as module)</span>
</span></span><span style=display:flex><span>module load PyTorch/1.12.1-foss-2022a-CUDA-11.7.0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># TensorFlow (if pre-installed as module)</span>
</span></span><span style=display:flex><span>module load TensorFlow/2.11.0-foss-2022a-CUDA-11.7.0
</span></span></code></pre></div><h3 id=development-tools>Development Tools<a class=td-heading-self-link href=#development-tools aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Git (usually available by default)</span>
</span></span><span style=display:flex><span>module load git/2.36.0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># CMake</span>
</span></span><span style=display:flex><span>module load CMake/3.24.3
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># HDF5 for data storage</span>
</span></span><span style=display:flex><span>module load HDF5/1.12.2
</span></span></code></pre></div><h2 id=module-dependencies>Module Dependencies<a class=td-heading-self-link href=#module-dependencies aria-label="Heading self-link"></a></h2><p>Lmod automatically handles dependencies. When you load a module, it loads required dependencies:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Loading Python might automatically load GCC</span>
</span></span><span style=display:flex><span>module load Python/3.9.5
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check what was loaded</span>
</span></span><span style=display:flex><span>module list
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Might show:</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># GCCcore/10.3.0</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Python/3.9.5-GCCcore-10.3.0</span>
</span></span></code></pre></div><h2 id=setting-up-your-environment>Setting Up Your Environment<a class=td-heading-self-link href=#setting-up-your-environment aria-label="Heading self-link"></a></h2><h3 id=create-a-module-loading-script>Create a Module Loading Script<a class=td-heading-self-link href=#create-a-module-loading-script aria-label="Heading self-link"></a></h3><p>Create <code>~/load_modules.sh</code> for consistent environment setup:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span><span style=color:#8f5902;font-style:italic># ~/load_modules.sh - Load standard development environment</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Clear any existing modules</span>
</span></span><span style=display:flex><span>module purge
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load core development tools</span>
</span></span><span style=display:flex><span>module load GCC/10.3.0
</span></span><span style=display:flex><span>module load CUDA/11.3.1
</span></span><span style=display:flex><span>module load Python/3.9.5
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Optional: Load additional tools</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># module load git/2.36.0</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># module load CMake/3.24.3</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#34;Development environment loaded:&#34;</span>
</span></span><span style=display:flex><span>module list
</span></span></code></pre></div><p>Make it executable and use it:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>chmod +x ~/load_modules.sh
</span></span><span style=display:flex><span><span style=color:#204a87>source</span> ~/load_modules.sh
</span></span></code></pre></div><h3 id=add-to-your-shell-configuration>Add to Your Shell Configuration<a class=td-heading-self-link href=#add-to-your-shell-configuration aria-label="Heading self-link"></a></h3><p>Add common modules to your <code>~/.bashrc</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Add to ~/.bashrc</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load standard modules at login</span>
</span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>if</span> <span style=color:#ce5c00;font-weight:700>[</span> -f ~/load_modules.sh <span style=color:#ce5c00;font-weight:700>]</span><span style=color:#000;font-weight:700>;</span> <span style=color:#204a87;font-weight:700>then</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87>source</span> ~/load_modules.sh
</span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>fi</span>
</span></span></code></pre></div><h2 id=slurm-job-scripts-with-modules>SLURM Job Scripts with Modules<a class=td-heading-self-link href=#slurm-job-scripts-with-modules aria-label="Heading self-link"></a></h2><h3 id=basic-job-with-modules>Basic Job with Modules<a class=td-heading-self-link href=#basic-job-with-modules aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span><span style=color:#8f5902;font-style:italic>#SBATCH -J module_job</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --partition=defq</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --qos=normal</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:1</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=2:00:00</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load required modules</span>
</span></span><span style=display:flex><span>module purge
</span></span><span style=display:flex><span>module load CUDA/11.3.1
</span></span><span style=display:flex><span>module load Python/3.9.5
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Verify modules are loaded</span>
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#34;Loaded modules:&#34;</span>
</span></span><span style=display:flex><span>module list
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check CUDA availability</span>
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#34;CUDA version:&#34;</span>
</span></span><span style=display:flex><span>nvcc --version
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Activate your conda environment</span>
</span></span><span style=display:flex><span><span style=color:#204a87>source</span> ~/anaconda3/bin/activate
</span></span><span style=display:flex><span>conda activate myenv
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Run your script</span>
</span></span><span style=display:flex><span>python train.py
</span></span></code></pre></div><h3 id=multiple-gpu-job-with-modules>Multiple GPU Job with Modules<a class=td-heading-self-link href=#multiple-gpu-job-with-modules aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span><span style=color:#8f5902;font-style:italic>#SBATCH -J multi_gpu_training</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --partition=defq</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --qos=long</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:4</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=1-00:00:00</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load modules for CUDA development</span>
</span></span><span style=display:flex><span>module load GCC/10.3.0
</span></span><span style=display:flex><span>module load CUDA/11.3.1
</span></span><span style=display:flex><span>module load Python/3.9.5
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load MPI for distributed computing (if available)</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># module load OpenMPI/4.1.4-GCC-10.3.0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Set CUDA environment</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>CUDA_VISIBLE_DEVICES</span><span style=color:#ce5c00;font-weight:700>=</span>0,1,2,3
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Activate environment</span>
</span></span><span style=display:flex><span>conda activate pytorch-gpu
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Run distributed training</span>
</span></span><span style=display:flex><span>python -m torch.distributed.launch <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    --nproc_per_node<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>4</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    train_distributed.py
</span></span></code></pre></div><h2 id=python-package-management>Python Package Management<a class=td-heading-self-link href=#python-package-management aria-label="Heading self-link"></a></h2><h3 id=using-conda-with-modules>Using Conda with Modules<a class=td-heading-self-link href=#using-conda-with-modules aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load Python module</span>
</span></span><span style=display:flex><span>module load Python/3.9.5
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install conda (if not already available)</span>
</span></span><span style=display:flex><span>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
</span></span><span style=display:flex><span>bash Miniconda3-latest-Linux-x86_64.sh -b -p ~/miniconda3
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Add conda to path</span>
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#39;export PATH=&#34;$HOME/miniconda3/bin:$PATH&#34;&#39;</span> &gt;&gt; ~/.bashrc
</span></span><span style=display:flex><span><span style=color:#204a87>source</span> ~/.bashrc
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Create environment</span>
</span></span><span style=display:flex><span>conda create -n myenv <span style=color:#000>python</span><span style=color:#ce5c00;font-weight:700>=</span>3.9
</span></span><span style=display:flex><span>conda activate myenv
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install packages</span>
</span></span><span style=display:flex><span>conda install pytorch torchvision torchaudio pytorch-cuda<span style=color:#ce5c00;font-weight:700>=</span>11.7 -c pytorch -c nvidia
</span></span></code></pre></div><h3 id=using-pip-with-modules>Using pip with Modules<a class=td-heading-self-link href=#using-pip-with-modules aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load Python module</span>
</span></span><span style=display:flex><span>module load Python/3.9.5
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Create virtual environment</span>
</span></span><span style=display:flex><span>python -m venv ~/venvs/myproject
</span></span><span style=display:flex><span><span style=color:#204a87>source</span> ~/venvs/myproject/bin/activate
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Upgrade pip</span>
</span></span><span style=display:flex><span>pip install --upgrade pip
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install packages</span>
</span></span><span style=display:flex><span>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117
</span></span><span style=display:flex><span>pip install jupyter numpy pandas matplotlib
</span></span></code></pre></div><h2 id=module-collections>Module Collections<a class=td-heading-self-link href=#module-collections aria-label="Heading self-link"></a></h2><p>Save frequently used module combinations:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Save current modules as a collection</span>
</span></span><span style=display:flex><span>module save my_collection
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># List saved collections</span>
</span></span><span style=display:flex><span>module savelist
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Restore a collection</span>
</span></span><span style=display:flex><span>module restore my_collection
</span></span></code></pre></div><h2 id=custom-module-paths>Custom Module Paths<a class=td-heading-self-link href=#custom-module-paths aria-label="Heading self-link"></a></h2><p>If your group has custom modules:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Add custom module path</span>
</span></span><span style=display:flex><span>module use /lustreFS/data/mygroup/modules
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check module paths</span>
</span></span><span style=display:flex><span>module show MODULEPATH
</span></span></code></pre></div><h2 id=troubleshooting-modules>Troubleshooting Modules<a class=td-heading-self-link href=#troubleshooting-modules aria-label="Heading self-link"></a></h2><h3 id=common-issues>Common Issues<a class=td-heading-self-link href=#common-issues aria-label="Heading self-link"></a></h3><p><strong>Module not found</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check available modules</span>
</span></span><span style=display:flex><span>module avail <span style=color:#000;font-weight:700>|</span> grep -i package_name
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check if you have access to the module path</span>
</span></span><span style=display:flex><span>ls -la /opt/modules/
</span></span></code></pre></div><p><strong>Conflicting modules</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Clear all modules and start fresh</span>
</span></span><span style=display:flex><span>module purge
</span></span><span style=display:flex><span>module load GCC/10.3.0 CUDA/11.3.1
</span></span></code></pre></div><p><strong>CUDA not found after loading</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Verify CUDA module is loaded</span>
</span></span><span style=display:flex><span>module list <span style=color:#000;font-weight:700>|</span> grep -i cuda
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check CUDA environment</span>
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#000>$CUDA_HOME</span>
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#000>$CUDA_PATH</span>
</span></span><span style=display:flex><span>which nvcc
</span></span></code></pre></div><p><strong>Python packages not found</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Ensure Python module is loaded before using pip/conda</span>
</span></span><span style=display:flex><span>module load Python/3.9.5
</span></span><span style=display:flex><span>which python
</span></span><span style=display:flex><span>python --version
</span></span></code></pre></div><h3 id=module-information>Module Information<a class=td-heading-self-link href=#module-information aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Show detailed module information</span>
</span></span><span style=display:flex><span>module show CUDA/11.3.1
</span></span><span style=display:flex><span>module <span style=color:#204a87>help</span> CUDA/11.3.1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># See what a module does before loading</span>
</span></span><span style=display:flex><span>module display CUDA/11.3.1
</span></span></code></pre></div><h2 id=best-practices>Best Practices<a class=td-heading-self-link href=#best-practices aria-label="Heading self-link"></a></h2><h3 id=for-interactive-development>For Interactive Development<a class=td-heading-self-link href=#for-interactive-development aria-label="Heading self-link"></a></h3><ol><li><strong>Create a standard environment script</strong></li><li><strong>Use module collections</strong> for frequently used combinations</li><li><strong>Load modules before activating conda/venv</strong></li></ol><h3 id=for-slurm-jobs>For SLURM Jobs<a class=td-heading-self-link href=#for-slurm-jobs aria-label="Heading self-link"></a></h3><ol><li><strong>Always start with <code>module purge</code></strong></li><li><strong>Load modules explicitly in job scripts</strong></li><li><strong>Verify modules are loaded</strong> with <code>module list</code></li><li><strong>Document module requirements</strong> in your scripts</li></ol><h3 id=for-reproducibility>For Reproducibility<a class=td-heading-self-link href=#for-reproducibility aria-label="Heading self-link"></a></h3><ol><li><p><strong>Pin module versions</strong> in scripts:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>module load CUDA/11.3.1  <span style=color:#8f5902;font-style:italic># Not just &#39;CUDA&#39;</span>
</span></span></code></pre></div></li><li><p><strong>Document module requirements</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Required modules:</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># - GCC/10.3.0</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># - CUDA/11.3.1</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># - Python/3.9.5</span>
</span></span></code></pre></div></li><li><p><strong>Use environment files</strong> for complex setups</p></li></ol><h2 id=example-workflows>Example Workflows<a class=td-heading-self-link href=#example-workflows aria-label="Heading self-link"></a></h2><h3 id=deep-learning-setup>Deep Learning Setup<a class=td-heading-self-link href=#deep-learning-setup aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Standard deep learning environment</span>
</span></span><span style=display:flex><span>module purge
</span></span><span style=display:flex><span>module load GCC/10.3.0
</span></span><span style=display:flex><span>module load CUDA/11.3.1
</span></span><span style=display:flex><span>module load Python/3.9.5
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Activate conda environment</span>
</span></span><span style=display:flex><span>conda activate pytorch-env
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Verify setup</span>
</span></span><span style=display:flex><span>python -c <span style=color:#4e9a06>&#34;import torch; print(f&#39;PyTorch: {torch.__version__}, CUDA: {torch.cuda.is_available()}&#39;)&#34;</span>
</span></span></code></pre></div><h3 id=development-environment>Development Environment<a class=td-heading-self-link href=#development-environment aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Development tools</span>
</span></span><span style=display:flex><span>module purge
</span></span><span style=display:flex><span>module load GCC/10.3.0
</span></span><span style=display:flex><span>module load CUDA/11.3.1
</span></span><span style=display:flex><span>module load Python/3.9.5
</span></span><span style=display:flex><span>module load git/2.36.0
</span></span><span style=display:flex><span>module load CMake/3.24.3
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Save as collection</span>
</span></span><span style=display:flex><span>module save development
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Later, restore quickly</span>
</span></span><span style=display:flex><span>module restore development
</span></span></code></pre></div><h3 id=compilation-environment>Compilation Environment<a class=td-heading-self-link href=#compilation-environment aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># For compiling CUDA code</span>
</span></span><span style=display:flex><span>module purge
</span></span><span style=display:flex><span>module load GCC/10.3.0
</span></span><span style=display:flex><span>module load CUDA/11.3.1
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Compile CUDA program</span>
</span></span><span style=display:flex><span>nvcc -o program program.cu
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># For C++ with GPU support</span>
</span></span><span style=display:flex><span>g++ -I<span style=color:#000>$CUDA_HOME</span>/include -L<span style=color:#000>$CUDA_HOME</span>/lib64 -lcudart program.cpp -o program
</span></span></code></pre></div><h2 id=next-steps>Next Steps<a class=td-heading-self-link href=#next-steps aria-label="Heading self-link"></a></h2><ul><li><strong>Configure VS Code for remote development</strong>: <a href=../vscode/>VS Code Setup</a></li><li><strong>Learn about software installation</strong>: <a href=../software/>Software Installation</a></li><li><strong>Submit your first job</strong>: <a href=../job-submission/>Job Submission</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-24286fd09742bbbd6cf8bf2a78c047d4>8 - VS Code Remote Development</h1><div class=lead>Set up Visual Studio Code for remote development on the Prometheus cluster</div><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>Visual Studio Code provides excellent remote development capabilities for the Prometheus cluster. You can edit code, run Jupyter notebooks, and debug applications directly on the cluster while using your local VS Code interface.</p><h2 id=prerequisites>Prerequisites<a class=td-heading-self-link href=#prerequisites aria-label="Heading self-link"></a></h2><ul><li><strong>VS Code Desktop</strong> installed on your local machine</li><li><strong>Remote-SSH extension</strong> for VS Code</li><li><strong>SSH access</strong> to Prometheus cluster (see <a href=../getting-started/>Getting Started</a>)</li><li><strong>Valid cluster account</strong> with SSH keys configured</li></ul><h2 id=install-required-extensions>Install Required Extensions<a class=td-heading-self-link href=#install-required-extensions aria-label="Heading self-link"></a></h2><p>Install these essential VS Code extensions:</p><ol><li><strong>Remote - SSH</strong> (<code>ms-vscode-remote.remote-ssh</code>)</li><li><strong>Python</strong> (<code>ms-python.python</code>)</li><li><strong>Jupyter</strong> (<code>ms-toolsai.jupyter</code>)</li><li><strong>Git</strong> integration (built-in)</li></ol><p>Optional but recommended:</p><ul><li><strong>Remote - SSH: Editing Configuration Files</strong> (<code>ms-vscode-remote.remote-ssh-edit</code>)</li><li><strong>GitLens</strong> (<code>eamodio.gitlens</code>)</li><li><strong>Thunder Client</strong> for API testing (<code>rangav.vscode-thunder-client</code>)</li></ul><h2 id=ssh-configuration>SSH Configuration<a class=td-heading-self-link href=#ssh-configuration aria-label="Heading self-link"></a></h2><h3 id=basic-ssh-setup>Basic SSH Setup<a class=td-heading-self-link href=#basic-ssh-setup aria-label="Heading self-link"></a></h3><p>First, ensure your <code>~/.ssh/config</code> file is properly configured:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># ~/.ssh/config</span>
</span></span><span style=display:flex><span>Host prometheus
</span></span><span style=display:flex><span>  Hostname prometheus.cyens.org.cy
</span></span><span style=display:flex><span>  User &lt;your-username&gt;
</span></span><span style=display:flex><span>  IdentityFile ~/.ssh/id_rsa
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Host *.cluster
</span></span><span style=display:flex><span>  User &lt;your-username&gt;
</span></span><span style=display:flex><span>  IdentityFile ~/.ssh/prometheus_user_sshd
</span></span><span style=display:flex><span>  ProxyJump prometheus
</span></span></code></pre></div><p>Replace <code>&lt;your-username></code> with your actual cluster username.</p><h2 id=user-sshd-process-setup>User SSHD Process Setup<a class=td-heading-self-link href=#user-sshd-process-setup aria-label="Heading self-link"></a></h2><p>To connect VS Code directly to compute nodes, you need to set up a user SSHD process through SLURM.</p><h3 id=step-1-generate-ssh-keys-for-user-sshd>Step 1: Generate SSH Keys for User SSHD<a class=td-heading-self-link href=#step-1-generate-ssh-keys-for-user-sshd aria-label="Heading self-link"></a></h3><p>Connect to Prometheus and create SSH keys for the user SSHD process:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>ssh prometheus
</span></span><span style=display:flex><span>ssh-keygen -t rsa -f ~/.ssh/prometheus_user_sshd
</span></span></code></pre></div><p>This creates:</p><ul><li><strong>Private key</strong>: <code>~/.ssh/prometheus_user_sshd</code></li><li><strong>Public key</strong>: <code>~/.ssh/prometheus_user_sshd.pub</code></li></ul><h3 id=step-2-create-sshd-job-script>Step 2: Create SSHD Job Script<a class=td-heading-self-link href=#step-2-create-sshd-job-script aria-label="Heading self-link"></a></h3><p>Create <code>~/sshd.sh</code> script for launching the user SSHD process:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span><span style=color:#8f5902;font-style:italic>#SBATCH -o res_%j.txt      # output file</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH -e res_%j.err      # error file</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH -J sshd            # job name</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --partition=defq   # partition</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --qos=normal       # priority queue</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --ntasks=1         # number of tasks</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --cpus-per-task=2  # CPU cores</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:1       # number of GPUs</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --mem=1000         # memory in MB</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=0-04:00     # 4 hours maximum</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Find an available port</span>
</span></span><span style=display:flex><span><span style=color:#000>PORT</span><span style=color:#ce5c00;font-weight:700>=</span><span style=color:#204a87;font-weight:700>$(</span>python -c <span style=color:#4e9a06>&#39;import socket; s=socket.socket(); s.bind((&#34;&#34;, 0)); print(s.getsockname()[1]); s.close()&#39;</span><span style=color:#204a87;font-weight:700>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#34;********************************************************************&#34;</span>
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#34;Starting sshd in Slurm as user&#34;</span>
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#34;Environment information:&#34;</span>
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#34;Date:&#34;</span> <span style=color:#204a87;font-weight:700>$(</span>date<span style=color:#204a87;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#34;Allocated node:&#34;</span> <span style=color:#204a87;font-weight:700>$(</span>hostname<span style=color:#204a87;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#34;Path:&#34;</span> <span style=color:#204a87;font-weight:700>$(</span><span style=color:#204a87>pwd</span><span style=color:#204a87;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#34;Listening on:&#34;</span> <span style=color:#000>$PORT</span>
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#34;********************************************************************&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Start user SSHD process</span>
</span></span><span style=display:flex><span>/usr/sbin/sshd -D -p <span style=color:#4e9a06>${</span><span style=color:#000>PORT</span><span style=color:#4e9a06>}</span> -f /dev/null -h <span style=color:#4e9a06>${</span><span style=color:#000>HOME</span><span style=color:#4e9a06>}</span>/.ssh/prometheus_user_sshd
</span></span></code></pre></div><p>Make the script executable:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>chmod +x ~/sshd.sh
</span></span></code></pre></div><h3 id=step-3-submit-sshd-job>Step 3: Submit SSHD Job<a class=td-heading-self-link href=#step-3-submit-sshd-job aria-label="Heading self-link"></a></h3><p>Submit the SSHD job to get a compute node:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>sbatch sshd.sh
</span></span></code></pre></div><p>Check the job status and get the allocated node and port:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check job status</span>
</span></span><span style=display:flex><span>squeue -u <span style=color:#000>$USER</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># View the output file to get connection details</span>
</span></span><span style=display:flex><span>cat res_&lt;job_id&gt;.txt
</span></span></code></pre></div><p>The output will show something like:</p><pre tabindex=0><code>Starting sshd in Slurm as user
Date: Thu Jun 5 10:30:00 UTC 2025
Allocated node: gpu02
Listening on: 45672
</code></pre><h2 id=connecting-vs-code>Connecting VS Code<a class=td-heading-self-link href=#connecting-vs-code aria-label="Heading self-link"></a></h2><h3 id=method-1-direct-connection>Method 1: Direct Connection<a class=td-heading-self-link href=#method-1-direct-connection aria-label="Heading self-link"></a></h3><ol><li><strong>Open VS Code</strong> on your local machine</li><li><strong>Press F1</strong> or Ctrl/Cmd+Shift+P to open command palette</li><li><strong>Type</strong>: &ldquo;Remote-SSH: Connect to Host&rdquo;</li><li><strong>Enter</strong>: <code>ssh -p 45672 gpu02.cluster</code> (use your actual port and node)</li></ol><p>VS Code will automatically update your SSH config file.</p><h3 id=method-2-manual-ssh-config>Method 2: Manual SSH Config<a class=td-heading-self-link href=#method-2-manual-ssh-config aria-label="Heading self-link"></a></h3><p>Add the connection details to your <code>~/.ssh/config</code>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>Host gpu02.cluster
</span></span><span style=display:flex><span>    HostName gpu02.cluster
</span></span><span style=display:flex><span>    Port <span style=color:#0000cf;font-weight:700>45672</span>
</span></span><span style=display:flex><span>    User &lt;your-username&gt;
</span></span><span style=display:flex><span>    IdentityFile ~/.ssh/prometheus_user_sshd
</span></span><span style=display:flex><span>    ProxyJump prometheus
</span></span></code></pre></div><p>Then connect using &ldquo;Remote-SSH: Connect to Host&rdquo; → <code>gpu02.cluster</code></p><h2 id=development-workflow>Development Workflow<a class=td-heading-self-link href=#development-workflow aria-label="Heading self-link"></a></h2><h3 id=1-connect-to-compute-node>1. Connect to Compute Node<a class=td-heading-self-link href=#1-connect-to-compute-node aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Submit SSHD job</span>
</span></span><span style=display:flex><span>sbatch sshd.sh
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Wait for job to start (check with squeue)</span>
</span></span><span style=display:flex><span>squeue -u <span style=color:#000>$USER</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Get connection details</span>
</span></span><span style=display:flex><span>cat res_&lt;job_id&gt;.txt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Connect VS Code to the allocated node</span>
</span></span></code></pre></div><h3 id=2-open-your-project>2. Open Your Project<a class=td-heading-self-link href=#2-open-your-project aria-label="Heading self-link"></a></h3><p>Once connected to the compute node:</p><ol><li><strong>File → Open Folder</strong></li><li><strong>Navigate to</strong>: <code>/lustreFS/data/mygroup/experiments/myproject</code></li><li><strong>Open the folder</strong></li></ol><h3 id=3-set-up-python-environment>3. Set Up Python Environment<a class=td-heading-self-link href=#3-set-up-python-environment aria-label="Heading self-link"></a></h3><p>In the VS Code terminal on the remote machine:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load required modules</span>
</span></span><span style=display:flex><span>module load CUDA/11.3.1 Python/3.9.5
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Activate your conda environment</span>
</span></span><span style=display:flex><span>conda activate myenv
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Verify GPU access</span>
</span></span><span style=display:flex><span>python -c <span style=color:#4e9a06>&#34;import torch; print(f&#39;CUDA available: {torch.cuda.is_available()}&#39;)&#34;</span>
</span></span></code></pre></div><h3 id=4-configure-python-interpreter>4. Configure Python Interpreter<a class=td-heading-self-link href=#4-configure-python-interpreter aria-label="Heading self-link"></a></h3><ol><li><strong>Press Ctrl/Cmd+Shift+P</strong></li><li><strong>Type</strong>: &ldquo;Python: Select Interpreter&rdquo;</li><li><strong>Choose</strong>: Your conda environment interpreter<ul><li>Usually: <code>~/miniconda3/envs/myenv/bin/python</code></li></ul></li></ol><h2 id=working-with-jupyter-notebooks>Working with Jupyter Notebooks<a class=td-heading-self-link href=#working-with-jupyter-notebooks aria-label="Heading self-link"></a></h2><h3 id=start-jupyter-server>Start Jupyter Server<a class=td-heading-self-link href=#start-jupyter-server aria-label="Heading self-link"></a></h3><p>In the VS Code terminal on the remote machine:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load modules and activate environment</span>
</span></span><span style=display:flex><span>module load Python/3.9.5
</span></span><span style=display:flex><span>conda activate myenv
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Start Jupyter (no browser needed)</span>
</span></span><span style=display:flex><span>jupyter notebook --no-browser --port<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>8888</span> --ip<span style=color:#ce5c00;font-weight:700>=</span>0.0.0.0
</span></span></code></pre></div><h3 id=connect-vs-code-to-jupyter>Connect VS Code to Jupyter<a class=td-heading-self-link href=#connect-vs-code-to-jupyter aria-label="Heading self-link"></a></h3><ol><li><strong>Open a <code>.ipynb</code> file</strong> in VS Code</li><li><strong>Click &ldquo;Select Kernel&rdquo;</strong> in the top-right</li><li><strong>Choose &ldquo;Existing Jupyter Server&rdquo;</strong></li><li><strong>Enter</strong>: <code>http://localhost:8888</code></li><li><strong>Enter the token</strong> from the Jupyter output</li></ol><h2 id=development-best-practices>Development Best Practices<a class=td-heading-self-link href=#development-best-practices aria-label="Heading self-link"></a></h2><h3 id=resource-management>Resource Management<a class=td-heading-self-link href=#resource-management aria-label="Heading self-link"></a></h3><ol><li><p><strong>Request appropriate resources</strong> for development:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --cpus-per-task=4  # Not 32 for development</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:1       # Usually sufficient for development</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --mem=8000         # 8GB for most development tasks</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=0-04:00     # 4 hours for development session</span>
</span></span></code></pre></div></li><li><p><strong>Use longer sessions for intensive work</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=0-08:00     # 8 hours for longer development</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --qos=long         # If you need more than 1 day</span>
</span></span></code></pre></div></li></ol><h3 id=file-organization>File Organization<a class=td-heading-self-link href=#file-organization aria-label="Heading self-link"></a></h3><p>Set up a consistent workspace structure:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Recommended project structure</span>
</span></span><span style=display:flex><span>/lustreFS/data/mygroup/experiments/myproject/
</span></span><span style=display:flex><span>├── data/                   <span style=color:#8f5902;font-style:italic># Datasets and data files</span>
</span></span><span style=display:flex><span>├── notebooks/              <span style=color:#8f5902;font-style:italic># Jupyter notebooks</span>
</span></span><span style=display:flex><span>├── src/                    <span style=color:#8f5902;font-style:italic># Source code</span>
</span></span><span style=display:flex><span>├── configs/                <span style=color:#8f5902;font-style:italic># Configuration files</span>
</span></span><span style=display:flex><span>├── scripts/                <span style=color:#8f5902;font-style:italic># Training and utility scripts</span>
</span></span><span style=display:flex><span>├── results/                <span style=color:#8f5902;font-style:italic># Experiment results</span>
</span></span><span style=display:flex><span>└── README.md               <span style=color:#8f5902;font-style:italic># Project documentation</span>
</span></span></code></pre></div><h3 id=environment-configuration>Environment Configuration<a class=td-heading-self-link href=#environment-configuration aria-label="Heading self-link"></a></h3><p>Create a workspace settings file (<code>.vscode/settings.json</code>):</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-json data-lang=json><span style=display:flex><span><span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;python.defaultInterpreterPath&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;~/miniconda3/envs/myenv/bin/python&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;python.terminal.activateEnvironment&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#204a87;font-weight:700>true</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;jupyter.jupyterServerType&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#4e9a06>&#34;local&#34;</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>    <span style=color:#204a87;font-weight:700>&#34;files.watcherExclude&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#000;font-weight:700>{</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>&#34;**/data/**&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#204a87;font-weight:700>true</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>&#34;**/results/**&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#204a87;font-weight:700>true</span><span style=color:#000;font-weight:700>,</span>
</span></span><span style=display:flex><span>        <span style=color:#204a87;font-weight:700>&#34;**/.git/**&#34;</span><span style=color:#000;font-weight:700>:</span> <span style=color:#204a87;font-weight:700>true</span>
</span></span><span style=display:flex><span>    <span style=color:#000;font-weight:700>}</span>
</span></span><span style=display:flex><span><span style=color:#000;font-weight:700>}</span>
</span></span></code></pre></div><h3 id=git-integration>Git Integration<a class=td-heading-self-link href=#git-integration aria-label="Heading self-link"></a></h3><p>Configure Git for your project:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Set up Git credentials</span>
</span></span><span style=display:flex><span>git config --global user.name <span style=color:#4e9a06>&#34;Your Name&#34;</span>
</span></span><span style=display:flex><span>git config --global user.email <span style=color:#4e9a06>&#34;your.email@example.com&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Initialize repository (if new project)</span>
</span></span><span style=display:flex><span><span style=color:#204a87>cd</span> /lustreFS/data/mygroup/experiments/myproject
</span></span><span style=display:flex><span>git init
</span></span><span style=display:flex><span>git add .
</span></span><span style=display:flex><span>git commit -m <span style=color:#4e9a06>&#34;Initial commit&#34;</span>
</span></span></code></pre></div><h2 id=troubleshooting>Troubleshooting<a class=td-heading-self-link href=#troubleshooting aria-label="Heading self-link"></a></h2><h3 id=connection-issues>Connection Issues<a class=td-heading-self-link href=#connection-issues aria-label="Heading self-link"></a></h3><p><strong>&ldquo;Could not establish connection&rdquo;</strong>:</p><ol><li>Check if SSHD job is running: <code>squeue -u $USER</code></li><li>Verify node name and port from job output</li><li>Ensure SSH keys are properly configured</li></ol><p><strong>&ldquo;Permission denied&rdquo;</strong>:</p><ol><li>Check SSH key permissions: <code>chmod 600 ~/.ssh/prometheus_user_sshd</code></li><li>Verify ProxyJump configuration in SSH config</li><li>Test SSH connection manually: <code>ssh -p &lt;port> &lt;node>.cluster</code></li></ol><h3 id=performance-issues>Performance Issues<a class=td-heading-self-link href=#performance-issues aria-label="Heading self-link"></a></h3><p><strong>Slow file operations</strong>:</p><ol><li>Exclude large directories from VS Code watcher</li><li>Use local storage for frequently accessed files</li><li>Consider using VS Code on the head node for file browsing only</li></ol><p><strong>High memory usage</strong>:</p><ol><li>Close unused notebooks and files</li><li>Restart VS Code Python extension if needed</li><li>Request more memory in SSHD job if necessary</li></ol><h3 id=sshd-job-management>SSHD Job Management<a class=td-heading-self-link href=#sshd-job-management aria-label="Heading self-link"></a></h3><p><strong>Job terminated unexpectedly</strong>:</p><ol><li>Check job logs: <code>cat res_&lt;job_id>.err</code></li><li>Resubmit SSHD job: <code>sbatch sshd.sh</code></li><li>Update VS Code connection with new port/node</li></ol><p><strong>Need longer development time</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Modify sshd.sh for longer sessions</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=0-08:00     # 8 hours</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --qos=long         # For &gt;24 hours</span>
</span></span></code></pre></div><h2 id=cleanup-and-best-practices>Cleanup and Best Practices<a class=td-heading-self-link href=#cleanup-and-best-practices aria-label="Heading self-link"></a></h2><h3 id=end-development-session>End Development Session<a class=td-heading-self-link href=#end-development-session aria-label="Heading self-link"></a></h3><p>When finishing your work:</p><ol><li><strong>Save all files</strong> in VS Code</li><li><strong>Close VS Code connection</strong></li><li><strong>Cancel the SSHD job</strong>:<div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>scancel &lt;job_id&gt;
</span></span></code></pre></div></li><li><strong>Remove SSH config entries</strong> added by VS Code (optional)</li></ol><h3 id=security-considerations>Security Considerations<a class=td-heading-self-link href=#security-considerations aria-label="Heading self-link"></a></h3><ol><li><strong>Don&rsquo;t leave SSHD jobs running</strong> when not needed</li><li><strong>Use strong passphrases</strong> for SSH keys</li><li><strong>Regularly rotate SSH keys</strong> if required by policy</li><li><strong>Monitor your running jobs</strong>: <code>squeue -u $USER</code></li></ol><h2 id=advanced-configuration>Advanced Configuration<a class=td-heading-self-link href=#advanced-configuration aria-label="Heading self-link"></a></h2><h3 id=multiple-concurrent-sessions>Multiple Concurrent Sessions<a class=td-heading-self-link href=#multiple-concurrent-sessions aria-label="Heading self-link"></a></h3><p>You can run multiple SSHD jobs for different projects:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Submit multiple jobs</span>
</span></span><span style=display:flex><span>sbatch sshd.sh
</span></span><span style=display:flex><span>sbatch sshd.sh
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Connect VS Code to different nodes</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Node 1: gpu01.cluster:45672</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Node 2: gpu03.cluster:45673</span>
</span></span></code></pre></div><h3 id=custom-sshd-configuration>Custom SSHD Configuration<a class=td-heading-self-link href=#custom-sshd-configuration aria-label="Heading self-link"></a></h3><p>Create specialized SSHD scripts for different use cases:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># sshd_gpu4.sh - For multi-GPU development</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:4</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --cpus-per-task=16</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --mem=64000</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># sshd_a6000.sh - For high-memory development</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --partition=a6000</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --qos=normal-a6000</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --gres=gpu:1</span>
</span></span></code></pre></div><h2 id=next-steps>Next Steps<a class=td-heading-self-link href=#next-steps aria-label="Heading self-link"></a></h2><ul><li><strong>Learn about software installation</strong>: <a href=../software/>Software Installation</a></li><li><strong>Explore advanced job submission</strong>: <a href=../job-submission/>Job Submission</a></li><li><strong>Understand storage optimization</strong>: <a href=../storage/>Storage Systems</a></li></ul></div><div class=td-content style=page-break-before:always><h1 id=pg-1fe159c1b0c0d7939ab765dc5b120cda>9 - Software Installation</h1><div class=lead>Installing and managing software packages on the Prometheus cluster</div><h2 id=overview>Overview<a class=td-heading-self-link href=#overview aria-label="Heading self-link"></a></h2><p>The Prometheus cluster provides several methods for installing and managing software packages. This guide covers both system-wide modules and user-specific installations.</p><h2 id=installation-methods>Installation Methods<a class=td-heading-self-link href=#installation-methods aria-label="Heading self-link"></a></h2><h3 id=1-environment-modules-recommended>1. Environment Modules (Recommended)<a class=td-heading-self-link href=#1-environment-modules-recommended aria-label="Heading self-link"></a></h3><p>Use pre-installed software via the module system when available:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>module avail python
</span></span><span style=display:flex><span>module load Python/3.9.5
</span></span></code></pre></div><h3 id=2-condamamba-package-manager>2. Conda/Mamba Package Manager<a class=td-heading-self-link href=#2-condamamba-package-manager aria-label="Heading self-link"></a></h3><p>Install packages in isolated environments:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda install pytorch torchvision torchaudio pytorch-cuda<span style=color:#ce5c00;font-weight:700>=</span>11.7 -c pytorch -c nvidia
</span></span></code></pre></div><h3 id=3-pip-package-manager>3. pip Package Manager<a class=td-heading-self-link href=#3-pip-package-manager aria-label="Heading self-link"></a></h3><p>Install Python packages via pip:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu117
</span></span></code></pre></div><h3 id=4-source-installation>4. Source Installation<a class=td-heading-self-link href=#4-source-installation aria-label="Heading self-link"></a></h3><p>Compile software from source when needed:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>git clone https://github.com/project/repo.git
</span></span><span style=display:flex><span><span style=color:#204a87>cd</span> repo <span style=color:#ce5c00;font-weight:700>&amp;&amp;</span> python setup.py install
</span></span></code></pre></div><h2 id=setting-up-python-environments>Setting Up Python Environments<a class=td-heading-self-link href=#setting-up-python-environments aria-label="Heading self-link"></a></h2><h3 id=conda-installation>Conda Installation<a class=td-heading-self-link href=#conda-installation aria-label="Heading self-link"></a></h3><p>If conda is not available, install Miniconda:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Download Miniconda</span>
</span></span><span style=display:flex><span><span style=color:#204a87>cd</span> /lustreFS/data/mygroup/<span style=color:#000>$USER</span>
</span></span><span style=display:flex><span>wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install Miniconda</span>
</span></span><span style=display:flex><span>bash Miniconda3-latest-Linux-x86_64.sh -b -p ~/miniconda3
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Initialize conda</span>
</span></span><span style=display:flex><span>~/miniconda3/bin/conda init bash
</span></span><span style=display:flex><span><span style=color:#204a87>source</span> ~/.bashrc
</span></span></code></pre></div><h3 id=create-virtual-environments>Create Virtual Environments<a class=td-heading-self-link href=#create-virtual-environments aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Create a new environment</span>
</span></span><span style=display:flex><span>conda create -n pytorch-env <span style=color:#000>python</span><span style=color:#ce5c00;font-weight:700>=</span>3.9
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Activate environment</span>
</span></span><span style=display:flex><span>conda activate pytorch-env
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install packages</span>
</span></span><span style=display:flex><span>conda install pytorch torchvision torchaudio pytorch-cuda<span style=color:#ce5c00;font-weight:700>=</span>11.7 -c pytorch -c nvidia
</span></span><span style=display:flex><span>conda install jupyter matplotlib pandas scikit-learn
</span></span></code></pre></div><h3 id=environment-management>Environment Management<a class=td-heading-self-link href=#environment-management aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># List environments</span>
</span></span><span style=display:flex><span>conda env list
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Export environment</span>
</span></span><span style=display:flex><span>conda env <span style=color:#204a87>export</span> &gt; environment.yml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Create from file</span>
</span></span><span style=display:flex><span>conda env create -f environment.yml
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Remove environment</span>
</span></span><span style=display:flex><span>conda env remove -n old-env
</span></span></code></pre></div><h2 id=deep-learning-frameworks>Deep Learning Frameworks<a class=td-heading-self-link href=#deep-learning-frameworks aria-label="Heading self-link"></a></h2><h3 id=pytorch-installation>PyTorch Installation<a class=td-heading-self-link href=#pytorch-installation aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Create PyTorch environment</span>
</span></span><span style=display:flex><span>conda create -n pytorch <span style=color:#000>python</span><span style=color:#ce5c00;font-weight:700>=</span>3.9
</span></span><span style=display:flex><span>conda activate pytorch
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install PyTorch with CUDA support</span>
</span></span><span style=display:flex><span>conda install pytorch torchvision torchaudio pytorch-cuda<span style=color:#ce5c00;font-weight:700>=</span>11.7 -c pytorch -c nvidia
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Verify installation</span>
</span></span><span style=display:flex><span>python -c <span style=color:#4e9a06>&#34;import torch; print(f&#39;PyTorch {torch.__version__}, CUDA: {torch.cuda.is_available()}&#39;)&#34;</span>
</span></span></code></pre></div><h3 id=tensorflow-installation>TensorFlow Installation<a class=td-heading-self-link href=#tensorflow-installation aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Create TensorFlow environment</span>
</span></span><span style=display:flex><span>conda create -n tensorflow <span style=color:#000>python</span><span style=color:#ce5c00;font-weight:700>=</span>3.9
</span></span><span style=display:flex><span>conda activate tensorflow
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install TensorFlow</span>
</span></span><span style=display:flex><span>pip install tensorflow<span style=color:#ce5c00;font-weight:700>[</span>and-cuda<span style=color:#ce5c00;font-weight:700>]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Verify GPU support</span>
</span></span><span style=display:flex><span>python -c <span style=color:#4e9a06>&#34;import tensorflow as tf; print(f&#39;TensorFlow {tf.__version__}, GPUs: {len(tf.config.list_physical_devices(&#34;</span>GPU<span style=color:#4e9a06>&#34;))}&#39;)&#34;</span>
</span></span></code></pre></div><h3 id=jax-installation>JAX Installation<a class=td-heading-self-link href=#jax-installation aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Create JAX environment</span>
</span></span><span style=display:flex><span>conda create -n jax <span style=color:#000>python</span><span style=color:#ce5c00;font-weight:700>=</span>3.9
</span></span><span style=display:flex><span>conda activate jax
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install JAX with CUDA support</span>
</span></span><span style=display:flex><span>pip install --upgrade <span style=color:#4e9a06>&#34;jax[cuda11_pip]&#34;</span> -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Verify installation</span>
</span></span><span style=display:flex><span>python -c <span style=color:#4e9a06>&#34;import jax; print(f&#39;JAX devices: {jax.devices()}&#39;)&#34;</span>
</span></span></code></pre></div><h2 id=specialized-libraries>Specialized Libraries<a class=td-heading-self-link href=#specialized-libraries aria-label="Heading self-link"></a></h2><h3 id=minkowskiengine>MinkowskiEngine<a class=td-heading-self-link href=#minkowskiengine aria-label="Heading self-link"></a></h3><p>MinkowskiEngine is an auto-differentiation library for sparse tensors, particularly useful for 3D computer vision tasks.</p><h4 id=installation-steps>Installation Steps<a class=td-heading-self-link href=#installation-steps aria-label="Heading self-link"></a></h4><ol><li><p><strong>Create dedicated environment</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda create -n py3-mink <span style=color:#000>python</span><span style=color:#ce5c00;font-weight:700>=</span>3.8
</span></span><span style=display:flex><span>conda activate py3-mink
</span></span></code></pre></div></li><li><p><strong>Install dependencies</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda install openblas-devel -c anaconda
</span></span><span style=display:flex><span>conda install <span style=color:#000>pytorch</span><span style=color:#ce5c00;font-weight:700>==</span>1.10.1 <span style=color:#000>torchvision</span><span style=color:#ce5c00;font-weight:700>==</span>0.11.2 <span style=color:#000>torchaudio</span><span style=color:#ce5c00;font-weight:700>==</span>0.10.1 <span style=color:#000>cudatoolkit</span><span style=color:#ce5c00;font-weight:700>=</span>11.3 -c pytorch -c conda-forge
</span></span></code></pre></div></li><li><p><strong>Load required modules</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>module load CUDA/11.3.1 gnu9
</span></span></code></pre></div></li><li><p><strong>Submit interactive job for compilation</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>srun -n <span style=color:#0000cf;font-weight:700>1</span> -c <span style=color:#0000cf;font-weight:700>4</span> --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:1 --mem<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>20000</span> --pty /bin/bash
</span></span></code></pre></div></li><li><p><strong>Install MinkowskiEngine</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda activate py3-mink
</span></span><span style=display:flex><span>pip install -U git+https://github.com/NVIDIA/MinkowskiEngine -v --no-deps <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    --install-option<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;--blas_include_dirs=</span><span style=color:#4e9a06>${</span><span style=color:#000>CONDA_PREFIX</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>/include&#34;</span> <span style=color:#4e9a06>\
</span></span></span><span style=display:flex><span><span style=color:#4e9a06></span>    --install-option<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#4e9a06>&#34;--blas=openblas&#34;</span>
</span></span></code></pre></div></li></ol><h4 id=usage-example>Usage Example<a class=td-heading-self-link href=#usage-example aria-label="Heading self-link"></a></h4><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#204a87;font-weight:700>import</span> <span style=color:#000>torch</span>
</span></span><span style=display:flex><span><span style=color:#204a87;font-weight:700>import</span> <span style=color:#000>MinkowskiEngine</span> <span style=color:#204a87;font-weight:700>as</span> <span style=color:#000>ME</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Create sparse tensor</span>
</span></span><span style=display:flex><span><span style=color:#000>coords</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>torch</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>IntTensor</span><span style=color:#000;font-weight:700>([[</span><span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>,</span> <span style=color:#0000cf;font-weight:700>1</span><span style=color:#000;font-weight:700>],</span> <span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>,</span> <span style=color:#0000cf;font-weight:700>1</span><span style=color:#000;font-weight:700>],</span> <span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>,</span> <span style=color:#0000cf;font-weight:700>2</span><span style=color:#000;font-weight:700>],</span> <span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>1</span><span style=color:#000;font-weight:700>,</span> <span style=color:#0000cf;font-weight:700>0</span><span style=color:#000;font-weight:700>],</span> <span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>1</span><span style=color:#000;font-weight:700>,</span> <span style=color:#0000cf;font-weight:700>2</span><span style=color:#000;font-weight:700>]])</span>
</span></span><span style=display:flex><span><span style=color:#000>feats</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>torch</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>FloatTensor</span><span style=color:#000;font-weight:700>([[</span><span style=color:#0000cf;font-weight:700>1</span><span style=color:#000;font-weight:700>],</span> <span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>2</span><span style=color:#000;font-weight:700>],</span> <span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>3</span><span style=color:#000;font-weight:700>],</span> <span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>4</span><span style=color:#000;font-weight:700>],</span> <span style=color:#000;font-weight:700>[</span><span style=color:#0000cf;font-weight:700>5</span><span style=color:#000;font-weight:700>]])</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Create sparse tensor</span>
</span></span><span style=display:flex><span><span style=color:#000>sparse_tensor</span> <span style=color:#ce5c00;font-weight:700>=</span> <span style=color:#000>ME</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>SparseTensor</span><span style=color:#000;font-weight:700>(</span><span style=color:#000>feats</span><span style=color:#000;font-weight:700>,</span> <span style=color:#000>coords</span><span style=color:#000;font-weight:700>)</span>
</span></span><span style=display:flex><span><span style=color:#204a87>print</span><span style=color:#000;font-weight:700>(</span><span style=color:#4e9a06>f</span><span style=color:#4e9a06>&#34;Sparse tensor shape: </span><span style=color:#4e9a06>{</span><span style=color:#000>sparse_tensor</span><span style=color:#ce5c00;font-weight:700>.</span><span style=color:#000>shape</span><span style=color:#4e9a06>}</span><span style=color:#4e9a06>&#34;</span><span style=color:#000;font-weight:700>)</span>
</span></span></code></pre></div><h3 id=pointgpt>PointGPT<a class=td-heading-self-link href=#pointgpt aria-label="Heading self-link"></a></h3><p>PointGPT extends GPT concepts to point clouds for 3D understanding tasks.</p><h4 id=installation-steps-1>Installation Steps<a class=td-heading-self-link href=#installation-steps-1 aria-label="Heading self-link"></a></h4><ol><li><p><strong>Create environment</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda create -n pointgpt <span style=color:#000>python</span><span style=color:#ce5c00;font-weight:700>=</span>3.8
</span></span><span style=display:flex><span>conda activate pointgpt
</span></span></code></pre></div></li><li><p><strong>Install PyTorch and dependencies</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda install <span style=color:#000>pytorch</span><span style=color:#ce5c00;font-weight:700>==</span>1.10.1 <span style=color:#000>torchvision</span><span style=color:#ce5c00;font-weight:700>==</span>0.11.2 <span style=color:#000>torchaudio</span><span style=color:#ce5c00;font-weight:700>==</span>0.10.1 <span style=color:#000>cudatoolkit</span><span style=color:#ce5c00;font-weight:700>=</span>11.3 tensorboard -c pytorch -c conda-forge
</span></span><span style=display:flex><span>pip install easydict h5py matplotlib open3d opencv-python pyyaml timm tqdm transforms3d termcolor scipy ninja plyfile <span style=color:#000>numpy</span><span style=color:#ce5c00;font-weight:700>==</span>1.23.4
</span></span><span style=display:flex><span>pip install <span style=color:#000>setuptools</span><span style=color:#ce5c00;font-weight:700>==</span>59.5.0
</span></span></code></pre></div></li><li><p><strong>Load CUDA module</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>module load CUDA/11.3.1
</span></span></code></pre></div></li><li><p><strong>Clone PointGPT repository</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#204a87>cd</span> /lustreFS/data/mygroup/<span style=color:#000>$USER</span>
</span></span><span style=display:flex><span>git clone https://github.com/CGuangyan-BIT/PointGPT.git
</span></span><span style=display:flex><span><span style=color:#204a87>cd</span> PointGPT
</span></span></code></pre></div></li><li><p><strong>Submit interactive job for compilation</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>srun -n <span style=color:#0000cf;font-weight:700>1</span> -c <span style=color:#0000cf;font-weight:700>4</span> --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:1 --mem<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>20000</span> --pty /bin/bash
</span></span></code></pre></div></li><li><p><strong>Install extensions</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda activate pointgpt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Chamfer Distance &amp; EMD</span>
</span></span><span style=display:flex><span><span style=color:#204a87>cd</span> ./extensions/chamfer_dist
</span></span><span style=display:flex><span>python setup.py install --user
</span></span><span style=display:flex><span><span style=color:#204a87>cd</span> ../emd
</span></span><span style=display:flex><span>python setup.py install --user
</span></span><span style=display:flex><span><span style=color:#204a87>cd</span> ../
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># PointNet++</span>
</span></span><span style=display:flex><span>pip install <span style=color:#4e9a06>&#34;git+https://github.com/erikwijmans/Pointnet2_PyTorch.git#egg=pointnet2_ops&amp;subdirectory=pointnet2_ops_lib&#34;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># GPU kNN</span>
</span></span><span style=display:flex><span>pip install --upgrade https://github.com/unlimblue/KNN_CUDA/releases/download/0.2/KNN_CUDA-0.2-py3-none-any.whl
</span></span></code></pre></div></li></ol><h2 id=computer-vision-libraries>Computer Vision Libraries<a class=td-heading-self-link href=#computer-vision-libraries aria-label="Heading self-link"></a></h2><h3 id=opencv-installation>OpenCV Installation<a class=td-heading-self-link href=#opencv-installation aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda activate myenv
</span></span><span style=display:flex><span>conda install opencv -c conda-forge
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Or install from pip</span>
</span></span><span style=display:flex><span>pip install opencv-python opencv-contrib-python
</span></span></code></pre></div><h3 id=open3d-for-3d-processing>Open3D for 3D Processing<a class=td-heading-self-link href=#open3d-for-3d-processing aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda activate myenv
</span></span><span style=display:flex><span>pip install open3d
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Test installation</span>
</span></span><span style=display:flex><span>python -c <span style=color:#4e9a06>&#34;import open3d as o3d; print(f&#39;Open3D {o3d.__version__}&#39;)&#34;</span>
</span></span></code></pre></div><h3 id=pilpillow-for-image-processing>PIL/Pillow for Image Processing<a class=td-heading-self-link href=#pilpillow-for-image-processing aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda install pillow
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># or</span>
</span></span><span style=display:flex><span>pip install Pillow
</span></span></code></pre></div><h2 id=scientific-computing>Scientific Computing<a class=td-heading-self-link href=#scientific-computing aria-label="Heading self-link"></a></h2><h3 id=numpy-scipy-pandas>NumPy, SciPy, Pandas<a class=td-heading-self-link href=#numpy-scipy-pandas aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda install numpy scipy pandas matplotlib seaborn
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># or</span>
</span></span><span style=display:flex><span>pip install numpy scipy pandas matplotlib seaborn
</span></span></code></pre></div><h3 id=jupyter-and-ipython>Jupyter and IPython<a class=td-heading-self-link href=#jupyter-and-ipython aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda install jupyter ipython ipykernel
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># or</span>
</span></span><span style=display:flex><span>pip install jupyter ipython ipykernel
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Add environment to Jupyter</span>
</span></span><span style=display:flex><span>python -m ipykernel install --user --name myenv --display-name <span style=color:#4e9a06>&#34;Python (myenv)&#34;</span>
</span></span></code></pre></div><h3 id=scikit-learn>Scikit-learn<a class=td-heading-self-link href=#scikit-learn aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda install scikit-learn
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># or</span>
</span></span><span style=display:flex><span>pip install scikit-learn
</span></span></code></pre></div><h2 id=development-tools>Development Tools<a class=td-heading-self-link href=#development-tools aria-label="Heading self-link"></a></h2><h3 id=git-and-version-control>Git and Version Control<a class=td-heading-self-link href=#git-and-version-control aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Git is usually available by default</span>
</span></span><span style=display:flex><span>git --version
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Configure Git</span>
</span></span><span style=display:flex><span>git config --global user.name <span style=color:#4e9a06>&#34;Your Name&#34;</span>
</span></span><span style=display:flex><span>git config --global user.email <span style=color:#4e9a06>&#34;your.email@example.com&#34;</span>
</span></span></code></pre></div><h3 id=build-tools>Build Tools<a class=td-heading-self-link href=#build-tools aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install build essentials</span>
</span></span><span style=display:flex><span>conda install cmake make ninja
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># For C++ development</span>
</span></span><span style=display:flex><span>conda install gxx_linux-64 gcc_linux-64
</span></span></code></pre></div><h3 id=debugging-tools>Debugging Tools<a class=td-heading-self-link href=#debugging-tools aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install debugging tools</span>
</span></span><span style=display:flex><span>pip install pdb++ ipdb
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Memory profiling</span>
</span></span><span style=display:flex><span>pip install memory_profiler
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Line profiling</span>
</span></span><span style=display:flex><span>pip install line_profiler
</span></span></code></pre></div><h2 id=installation-in-slurm-jobs>Installation in SLURM Jobs<a class=td-heading-self-link href=#installation-in-slurm-jobs aria-label="Heading self-link"></a></h2><h3 id=interactive-installation>Interactive Installation<a class=td-heading-self-link href=#interactive-installation aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Submit interactive job for installation</span>
</span></span><span style=display:flex><span>srun --partition<span style=color:#ce5c00;font-weight:700>=</span>defq --qos<span style=color:#ce5c00;font-weight:700>=</span>normal --gres<span style=color:#ce5c00;font-weight:700>=</span>gpu:1 --mem<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>16000</span> --time<span style=color:#ce5c00;font-weight:700>=</span>2:00:00 --pty /bin/bash
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load modules</span>
</span></span><span style=display:flex><span>module load CUDA/11.3.1 Python/3.9.5
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Activate environment</span>
</span></span><span style=display:flex><span>conda activate myenv
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install packages</span>
</span></span><span style=display:flex><span>pip install package-name
</span></span></code></pre></div><h3 id=batch-installation-script>Batch Installation Script<a class=td-heading-self-link href=#batch-installation-script aria-label="Heading self-link"></a></h3><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic></span><span style=color:#8f5902;font-style:italic>#SBATCH -J install_packages</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --partition=defq</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --qos=normal</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --cpus-per-task=4</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --mem=8000</span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic>#SBATCH --time=1:00:00</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load modules</span>
</span></span><span style=display:flex><span>module load Python/3.9.5
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Activate environment</span>
</span></span><span style=display:flex><span>conda activate myenv
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install packages</span>
</span></span><span style=display:flex><span>pip install -r requirements.txt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#34;Installation completed&#34;</span>
</span></span></code></pre></div><h2 id=package-management-best-practices>Package Management Best Practices<a class=td-heading-self-link href=#package-management-best-practices aria-label="Heading self-link"></a></h2><h3 id=requirements-files>Requirements Files<a class=td-heading-self-link href=#requirements-files aria-label="Heading self-link"></a></h3><p>Create <code>requirements.txt</code> for reproducibility:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-txt data-lang=txt><span style=display:flex><span>torch==1.12.1+cu117
</span></span><span style=display:flex><span>torchvision==0.13.1+cu117
</span></span><span style=display:flex><span>torchaudio==0.12.1+cu117
</span></span><span style=display:flex><span>numpy==1.23.4
</span></span><span style=display:flex><span>pandas==1.5.2
</span></span><span style=display:flex><span>matplotlib==3.6.2
</span></span><span style=display:flex><span>jupyter==1.0.0
</span></span></code></pre></div><p>Install from requirements:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>pip install -r requirements.txt
</span></span></code></pre></div><h3 id=environment-files>Environment Files<a class=td-heading-self-link href=#environment-files aria-label="Heading self-link"></a></h3><p>Create <code>environment.yml</code> for conda:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#204a87;font-weight:700>name</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline> </span><span style=color:#000>myproject</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline></span><span style=color:#204a87;font-weight:700>channels</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span>- <span style=color:#000>pytorch</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span>- <span style=color:#000>nvidia</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span>- <span style=color:#000>conda-forge</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline></span><span style=color:#204a87;font-weight:700>dependencies</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span>- <span style=color:#000>python=3.9</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span>- <span style=color:#000>pytorch=1.12.1</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span>- <span style=color:#000>torchvision=0.13.1</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span>- <span style=color:#000>torchaudio=0.12.1</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span>- <span style=color:#000>pytorch-cuda=11.7</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span>- <span style=color:#000>numpy</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span>- <span style=color:#000>pandas</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span>- <span style=color:#000>matplotlib</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span>- <span style=color:#000>jupyter</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span>- <span style=color:#000>pip</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>  </span>- <span style=color:#204a87;font-weight:700>pip</span><span style=color:#000;font-weight:700>:</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span><span style=display:flex><span><span style=color:#f8f8f8;text-decoration:underline>    </span>- <span style=color:#000>some-pip-package</span><span style=color:#f8f8f8;text-decoration:underline>
</span></span></span></code></pre></div><p>Create environment:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>conda env create -f environment.yml
</span></span></code></pre></div><h3 id=storage-considerations>Storage Considerations<a class=td-heading-self-link href=#storage-considerations aria-label="Heading self-link"></a></h3><p>Install packages in shared group storage to avoid quota issues:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Set conda environments path</span>
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#34;envs_dirs:
</span></span></span><span style=display:flex><span><span style=color:#4e9a06>  - /lustreFS/data/mygroup/conda/envs&#34;</span> &gt; ~/.condarc
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Set pip cache directory</span>
</span></span><span style=display:flex><span><span style=color:#204a87>export</span> <span style=color:#000>PIP_CACHE_DIR</span><span style=color:#ce5c00;font-weight:700>=</span>/lustreFS/data/mygroup/pip-cache
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#4e9a06>&#39;export PIP_CACHE_DIR=/lustreFS/data/mygroup/pip-cache&#39;</span> &gt;&gt; ~/.bashrc
</span></span></code></pre></div><h2 id=troubleshooting>Troubleshooting<a class=td-heading-self-link href=#troubleshooting aria-label="Heading self-link"></a></h2><h3 id=common-installation-issues>Common Installation Issues<a class=td-heading-self-link href=#common-installation-issues aria-label="Heading self-link"></a></h3><p><strong>CUDA compatibility errors</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check CUDA version</span>
</span></span><span style=display:flex><span>nvidia-smi
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install matching PyTorch version</span>
</span></span><span style=display:flex><span>conda install pytorch torchvision torchaudio pytorch-cuda<span style=color:#ce5c00;font-weight:700>=</span>11.7 -c pytorch -c nvidia
</span></span></code></pre></div><p><strong>Memory errors during installation</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Request more memory for installation</span>
</span></span><span style=display:flex><span>srun --mem<span style=color:#ce5c00;font-weight:700>=</span><span style=color:#0000cf;font-weight:700>32000</span> --pty /bin/bash
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Or increase pip timeout</span>
</span></span><span style=display:flex><span>pip install --timeout <span style=color:#0000cf;font-weight:700>1000</span> package-name
</span></span></code></pre></div><p><strong>Permission errors</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install in user space</span>
</span></span><span style=display:flex><span>pip install --user package-name
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Or check conda environment ownership</span>
</span></span><span style=display:flex><span>ls -la ~/miniconda3/envs/
</span></span></code></pre></div><p><strong>Network timeouts</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Use conda-forge channel</span>
</span></span><span style=display:flex><span>conda install -c conda-forge package-name
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Or use pip with retries</span>
</span></span><span style=display:flex><span>pip install --retries <span style=color:#0000cf;font-weight:700>10</span> package-name
</span></span></code></pre></div><h3 id=compilation-issues>Compilation Issues<a class=td-heading-self-link href=#compilation-issues aria-label="Heading self-link"></a></h3><p><strong>Missing compilers</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Load compiler modules</span>
</span></span><span style=display:flex><span>module load GCC/10.3.0
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Check compiler availability</span>
</span></span><span style=display:flex><span>gcc --version
</span></span><span style=display:flex><span>nvcc --version
</span></span></code></pre></div><p><strong>Missing headers</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install development packages</span>
</span></span><span style=display:flex><span>conda install gxx_linux-64 gcc_linux-64
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># For CUDA development</span>
</span></span><span style=display:flex><span>module load CUDA/11.3.1
</span></span><span style=display:flex><span><span style=color:#204a87>echo</span> <span style=color:#000>$CUDA_HOME</span>
</span></span></code></pre></div><h3 id=environment-conflicts>Environment Conflicts<a class=td-heading-self-link href=#environment-conflicts aria-label="Heading self-link"></a></h3><p><strong>Package conflicts</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Create fresh environment</span>
</span></span><span style=display:flex><span>conda create -n clean-env <span style=color:#000>python</span><span style=color:#ce5c00;font-weight:700>=</span>3.9
</span></span><span style=display:flex><span>conda activate clean-env
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Install packages one by one</span>
</span></span><span style=display:flex><span>conda install pytorch -c pytorch
</span></span></code></pre></div><p><strong>Module vs conda conflicts</strong>:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Always load modules before activating conda</span>
</span></span><span style=display:flex><span>module load Python/3.9.5
</span></span><span style=display:flex><span>conda activate myenv
</span></span></code></pre></div><h2 id=package-documentation>Package Documentation<a class=td-heading-self-link href=#package-documentation aria-label="Heading self-link"></a></h2><p>Keep track of installed packages:</p><div class=highlight><pre tabindex=0 style=background-color:#f8f8f8;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#8f5902;font-style:italic># List conda packages</span>
</span></span><span style=display:flex><span>conda list &gt; conda_packages.txt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># List pip packages</span>
</span></span><span style=display:flex><span>pip freeze &gt; pip_requirements.txt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#8f5902;font-style:italic># Environment information</span>
</span></span><span style=display:flex><span>conda info --envs &gt; environments.txt
</span></span></code></pre></div><h2 id=next-steps>Next Steps<a class=td-heading-self-link href=#next-steps aria-label="Heading self-link"></a></h2><ul><li><strong>Learn job submission</strong>: <a href=../job-submission/>Job Submission</a></li><li><strong>Explore GPU programming</strong>: <a href=../gpu-computing/>GPU Computing</a></li><li><strong>Set up monitoring</strong>: <a href=../monitoring/>Performance Monitoring</a></li></ul></div></main></div></div><footer class="td-footer row d-print-none"><div class=container-fluid><div class="row mx-md-2"><div class="td-footer__left col-6 col-sm-4 order-sm-1"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=CYENS aria-label=CYENS><a target=_blank rel=noopener href=https://www.cyens.org.cy aria-label=CYENS><i class=/assets/icons/cyens_icon.png></i></a></li></ul></div><div class="td-footer__right col-6 col-sm-4 order-sm-3"><ul class=td-footer__links-list><li class=td-footer__links-item data-bs-toggle=tooltip title=Support aria-label=Support><a target=_blank rel=noopener href=mailto:m.loizou@cyens.org.cy aria-label=Support><i class="fa fa-envelope"></i></a></li><li class=td-footer__links-item data-bs-toggle=tooltip title=GitHub aria-label=GitHub><a target=_blank rel=noopener href=https://github.com/marios2019/prometheus-docs aria-label=GitHub><i class="fab fa-github"></i></a></li></ul></div><div class="td-footer__center col-12 col-sm-4 py-2 order-sm-2"><span class=td-footer__copyright>&copy;
2018&ndash;2025
<span class=td-footer__authors>Docsy Authors | <a href=https://creativecommons.org/licenses/by/4.0>CC BY 4.0</a> |</span></span><span class=td-footer__all_rights_reserved>All Rights Reserved</span></div></div></div></footer></div><script src=/prometheus-docs/js/main.min.c9f028f8b259a0707ef150edb92cddf8778d792bfa6c54b0ab6a04241b378324.js integrity="sha256-yfAo+LJZoHB+8VDtuSzd+HeNeSv6bFSwq2oEJBs3gyQ=" crossorigin=anonymous></script><script defer src=/prometheus-docs/js/click-to-copy.min.73478a7d4807698aed7e355eb23f9890ca18fea3158604c8471746d046702bad.js integrity="sha256-c0eKfUgHaYrtfjVesj+YkMoY/qMVhgTIRxdG0EZwK60=" crossorigin=anonymous></script><script src=/prometheus-docs/js/tabpane-persist.js></script></body></html>